---
title: "OCI HPCポータル"
excerpt: "OCIを活用してHPC/機械学習ワークロードを実行する際の有益な情報を技術面にフォーカスしてお届けする、OCI HPCポータルです。ベアメタルインスタンス、GPUインスタンス、クラスタ・ネットワーク等のリソースをリソース・マネージャ、Terraform、Ansibleを駆使して効率的に構築するチュートリアル集、パフォーマンス関連情報、テクニカルTips集、関連情報リンク集等をお届けします。チュートリアルで構築するHPCクラスタは、NFS、Lustre、LDAP、Slurm、OpenMPI等、システム運用・利用に欠かせないソフトウェアが使えます。またGPUクラスタは、NVIDIA Container Toolkit、Docker、containerdやEnrootのコンテナランタイム等、大規模な分散機械学習ワークロード実行に必須のソフトウェアが使えます。"
permalink: /hpc/
layout: single
tags: "hpc"
toc: true
---
<style>
table, th, td {
    font-size: 80%;
}
</style>

***
# 0. OCI HPCポータルとは

**OCI HPCポータル** は、 **[Oracle Cloud Infrastructure](https://www.oracle.com/cloud/)**（以降 **OCI** と呼称）を活用してHPC/機械学習ワークロードを実行する際の有益な情報を技術面にフォーカスしてお届けすることを目的に、以下のコンテンツを提供します。

1. **[OCI HPCチュートリアル集](#1-oci-hpcチュートリアル集)**   
HPC/機械学習ワークロードの実行に最適なベアメタルインスタンス、GPUインスタンス、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** 等の各種IaaSサービスを組み合わせて、様々な用途のHPC/GPUクラスタを構築する手順を解説するチュートリアル集です。  
提供するチュートリアルは、以下のカテゴリに分かれています。

    1. **[HPCクラスタ](#1-1-hpcクラスタ)**
    2. **[機械学習環境](#1-2-機械学習環境)**
    3. **[ファイル共有ストレージ](#1-3-ファイル共有ストレージ)**
    4. **[チュートリアルを組み合わせた実践的HPCシステム構築](#1-4-チュートリアルを組み合わせた実践的hpcシステム構築)**

2. **[OCI HPCパフォーマンス関連情報](#2-oci-hpcパフォーマンス関連情報)**  
HPC/機械学習ワークロードを実行する際のパフォーマンスに関する情報や、パフォーマンスチューニングを行う際の必須の情報源であるプロファイリングに関する情報を提供します。  
提供するパフォーマンス関連情報は、以下のカテゴリに分かれています。

    1. **[標準ベンチマーク実行方法](#2-1-標準ベンチマーク実行方法)**
    2. **[パフォーマンス関連Tips集](#2-2-パフォーマンス関連tips集)**
    3. **[プロファイリング関連Tips集](#2-3-プロファイリング関連tips集)**

3. **[OCI HPCテクニカルTips集](#3-oci-hpcテクニカルtips集)**  
HPC/機械学習ワークロードを実行する際に有益なテクニカルTipsを集めています。  
提供するテクニカルTipsは、以下のカテゴリに分かれています。

    1. **[クラスタ・ネットワーク](#3-1-クラスタネットワーク)**
    2. **[ストレージ](#3-2-ストレージ)**
    3. **[クラスタ管理](#3-3-クラスタ管理)**
    4. **[機械学習](#3-4-機械学習)**
    5. **[ソフトウェア環境](#3-5-ソフトウェア環境)**
    6. **[その他](#3-6-その他)**

4. **[OCI HPC関連情報リンク集](#4-oci-hpc関連情報リンク集)**  
HPC/機械学習ワークロードを実行する際に有益なテクニカル情報を提供するウェブサイトの情報を集めています。

5. **[OCI HPC用語集](#5-oci-hpc用語集)**  
本ポータルサイトを読み進めるうえで理解が必要なHPC関連の **OCI** 用語を解説しています。

提供するコンテンツは、随時追加・更新されますので、定期的にチェックしてみて下さい。  
また本ポータルサイト内のコンテンツは、作者が誠心誠意作成していますが、間違いや不正確な記述などを見つけた場合は、 **[ここ](https://github.com/oracle-japan/ocitutorials/issues)** からIssue登録でご連絡ください。

***
# 1. OCI HPCチュートリアル集

## 1-0. 概要

本章は、様々な用途のHPCシステムを構築するチュートリアルを集めた、 **OCI HPCチュートリアル集** です。

各チュートリアルは、以下4つのカテゴリに分類されます。

- **[HPCクラスタ](#1-1-hpcクラスタ)**

    このカテゴリは、最新のCPUを搭載するベアメタルインスタンスを **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** で高帯域・低遅延に接続する、CPUワークロード向けHPCクラスタを構築するためのチュートリアルを集めています。  
    チュートリアルで構築するHPCクラスタは、NFS、LDAP、 **Slurm** 、 **OpenMPI** 等、システム運用・利用に欠かせないソフトウェアが使えます。

- **[機械学習環境](#1-2-機械学習環境)**

    このカテゴリは、GPUを搭載する単一のVMインスタンスを使用する小規模機械学習環境から、複数のGPUを搭載するベアメタルインスタンスを **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** で高帯域・低遅延に接続する大規模機械学習ワークロード向けGPUクラスタまで、機械学習環境を構築するためのチュートリアルを集めています。  
    チュートリアルで構築するGPUクラスタは、 **NVIDIA Container Toolkit** 、 **Docker Community Edition** / **containerd** / **Enroot** のコンテナランタイム等、大規模な分散機械学習ワークロード実行に必須のソフトウェアが使えます。

- **[ファイル共有ストレージ](#1-3-ファイル共有ストレージ)**

    このカテゴリは、HPC/GPUクラスタを運用する際に必須となるファイル共有ストレージやそのバックアップ環境を、 **OCI** のサービスを組み合わせて構築するためのチュートリアルを集めています。

- **[チュートリアルを組み合わせた実践的HPCシステム構築](#1-4-チュートリアルを組み合わせた実践的hpcシステム構築)**

    このカテゴリは、前述3つのカテゴリのチュートリアルを組み合わせることで、より実践的なHPCシステムを構築するためのチュートリアルを集めています。

**[HPCクラスタ](#1-1-hpcクラスタ)** と **[機械学習環境](#1-2-機械学習環境)** のカテゴリは、利用目的や構築手法の異なるチュートリアルを複数用意しており、自身の要件に合わせて以下の観点で使用するチュートリアルを選択します。

- 構築手法

    構築手法は、 **[マーケットプレース](#5-5-マーケットプレイス)** 提供の **[スタック](#5-3-スタック)** を使用する自動構築、予め用意された **[Terraform](#5-12-terraform)** スクリプトを使用する自動構築、及び **OCI** コンソールから各リソースを順次構築する手動構築に分かれ、それぞれ以下の利点・欠点があります。  
    なお、ここで紹介する自動構築（**スタック**）に分類されるチュートリアルの多くは、 **[HPCクラスタスタック](#5-10-hpcクラスタスタック)** を活用しています。


    |                     | 利点                                                            | 欠点                                                | 備考                                                                                                              |     |
    | :-----------------: | ------------------------------------------------------------- | ------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | --- |
    | 自動構築<br>（**スタック**）      | 構築作業時間（※1）が短い<br>GUI操作（※2）が可能                            | 構築手順のブラックボックス化<br>　・システム構成の変更が難しい<br>　・問題発生時の原因究明が困難 | ※1）スタックメニュー選択の時間<br>※2） **OCI** コンソール操作                                                                                 |     |
    | 自動構築<br>（**Terraform**） | 構築作業時間（※3）が短い<br>CLI/GUI（※4）を選択可能                           | **Terraform** 実行環境（※5）が必要                              | ※3）スタックメニュー選択の時間<br>or<br>**Terraform** スクリプト内変数修正<br>に要する時間<br>※4）**Terraform** CLI/ **OCI** コンソール<br>※5）**Terraform** CLIを選択した場合 |     |
    | 手動構築<br>（ **OCI** コンソール）  | 構築手順が明確<br>　・システム構成の変更が容易<br>　・問題発生時の原因究明が容易<br>GUI操作（※6）が可能 | 構築作業時間が長い                                       | ※6） **OCI** コンソール操作                                                                                                   |     |



- クラスタ管理機能の有無

    構築するHPC/GPUクラスタは、以下のクラスタ管理機能を利用可能とするか、これらを含まない基礎インフラストラクチャのみを構築するかを選択する事が可能です。

    - ユーザ統合管理（LDAP)
    - ファイル共有ストレージ（NFS）
    - 計算/GPUノードのリソース管理・ジョブ管理（ **Slurm** ） 

- スタティック/オンデマンドクラスタ

    構築するHPC/GPUクラスタは、ワークロードを実行する際に動的に必要な計算リソースを作成するオンデマンドクラスタとして構築するか、ワークロードの有無にかかわらず常に計算リソースを起動しておくスタティッククラスタとして構築するかを選択することが可能です。  
    なおオンデマンドクラスタは、 **[クラスタオートスケーリング](#5-9-クラスタオートスケーリング)** が実現します。

- コンテナランタイム

    **[機械学習環境](#1-2-機械学習環境)** カテゴリに含まれるチュートリアルは、使用するコンテナランタイムを以下から選択することが可能です。

    - **Docker Community Edition**
    - **Enroot** with **Pyxis** integrated in **Slurm**
    - **containerd**

## 1-1. HPCクラスタ

本章は、HPCクラスタを構築するチュートリアルを集めています。自身の要件に合わせてチュートリアルを選んだら、そのチュートリアル名をクリックします。

| チュートリアル名                                                                                      | 構築手法                | クラスタ<br>管理機能 | スタティック/<br>オンデマンド | 計算ノードOS                |
| :-------------------------------------------------------------------------------------------: | :-----------------: | :----------: | :---------------: | :--------------------: |
| **[HPCクラスタを構築する<br>(基礎インフラ手動構築編)](/ocitutorials/hpc/spinup-cluster-network/)**                | 手動構築  | 無し           | スタティック            | **Oracle Linux** 7.9/8 |
| **[HPCクラスタを構築する<br>(基礎インフラ自動構築編)](/ocitutorials/hpc/spinup-hpc-cluster-withterraform/)**      | 自動構築<br>（**Terraform** CLI/**スタック**） | 無し           | スタティック            | **Oracle Linux** 7.9/8 |
| **[HPCクラスタを構築する<br>(スタティッククラスタ自動構築編)](/ocitutorials/hpc/spinup-hpc-cluster)**                 | 自動構築<br>（**スタック**）      | 有り           | スタティック            | **Oracle Linux** 7.9/8 |
| **[HPCクラスタを構築する<br>(オンデマンドクラスタ自動構築編)](/ocitutorials/hpc/spinup-hpc-cluster-withautoscaling)** | 自動構築<br>（**スタック**）      | 有り           | オンデマンド            | **Oracle Linux** 7.9/8 |

## 1-2. 機械学習環境

本章は、機械学習環境を構築するチュートリアルを集めています。自身の要件に合わせてチュートリアルを選んだら、そのチュートリアル名をクリックします。

| チュートリアル名                                                                                       | GPU<br>ノード数 | 構築手法                             | クラスタ<br>管理機能 | スタティック/<br>オンデマンド | コンテナ<br>ランタイム                                                        | GPUノードOS               |
| :--------------------------------------------------------------------------------------------: | :---------: | :------------------------------: | :----------: | :---------------: | :------------------------------------------------------------------: | :--------------------: |
| **[GPUインスタンスで<br>機械学習にトライ](/ocitutorials/hpc/spinup-ml-instance/)**                            | 単一          | 手動構築                             | 無し           | スタティック            | **Docker CE**                                                        | **Oracle Linux** 7.9/8 |
| **[GPUインスタンスで<br>分散機械学習環境を構築する](/ocitutorials/hpc/spinup-ml-instance-cntnd/)**                 | 単一          | 手動構築                             | 無し           | スタティック            | **containerd** | **Oracle Linux** 8     |
| **[GPUクラスタを構築する<br>(基礎インフラ手動構築編)](/ocitutorials/hpc/spinup-gpu-cluster/)**                     | 複数          | 手動構築                             | 無し           | スタティック            | **Docker CE**                                                        | **Oracle Linux** 7.9/8 |
| **[GPUクラスタを構築する<br>(基礎インフラ自動構築編)](/ocitutorials/hpc/spinup-gpu-cluster-withterraform/)**       | 複数          | 自動構築<br>（Terraform CLI<br>/スタック） | 無し           | スタティック            | **Docker CE**                                                        | **Oracle Linux** 7.9/8 |
| **[GPUクラスタを構築する<br>(スタティッククラスタ自動構築編)](/ocitutorials/hpc/spinup-gpu-cluster-withstack/)**       | 複数          | 自動構築<br>（スタック）                   | 有り           | スタティック            | **Enroot**                                                           | **Oracle Linux** 7.9/8 |
| **[GPUクラスタを構築する<br>(オンデマンドクラスタ自動構築編)](/ocitutorials/hpc/spinup-gpu-cluster-withautoscaling/)** | 複数          | 自動構築<br>（スタック）                   | 有り           | オンデマンド            | **Enroot**                                                           | **Oracle Linux** 7.9/8 |
| **[GPUクラスタを構築する<br>(Ubuntu OS編)](/ocitutorials/hpc/spinup-gpu-cluster-withubuntu/)**           | 複数          | 手動構築                             | 無し           | スタティック            | -                                                                    | **Ubuntu** 20.04       |

## 1-3. ファイル共有ストレージ

## 1-3-0. 概要

本章は、HPC/GPUクラスタから利用するファイル共有ストレージやそのバックアップ環境を構築するチュートリアルを集めています。

## 1-3-1. ファイル共有ストレージ

下表は、性能や保存するデータ特性の異なるファイル共有ストレージを、その構築手法毎にまとめています。  
自身の要件に合わせてファイル共有ストレージの構築手法を選択したら、そのチュートリアル名をクリックします。

| 構築手法の呼称                      | チュートリアル名                                                                                              | 保存対象データ<br>（※7） | ターゲット<br>スループットレンジ | 利用可能な<br>バックアップ手法                                                              |
| :--------------------------: | :---------------------------------------------------------------------------------------------------: | :-------------: | :----------------: | :----------------------------------------------------------------------------: |
| 標準FSS                        | **[ファイル・ストレージで<br>ファイル共有ストレージを<br>構築する](/ocitutorials/hpc/spinup-nfs-server-fss/)**                   | 長期保存データ         | 120 MiB/s（※8）      | **ファイル・ストレージ** が<br>提供するバックアップ機能<br>（※11）                                      |
| 高性能FSS                       | **[ファイル・ストレージで<br>ファイル共有ストレージを<br>構築する](/ocitutorials/hpc/spinup-nfs-server-fss/)**                   | 長期保存データ         | 10 GiB/s（※8）       | **ファイル・ストレージ** が<br>提供するバックアップ機能<br>（※11）                                      |
| **File Storage with Lustre** | **[File Storage with Lustre<br>でファイル共有ストレージ<br>を構築する](/ocitutorials/hpc/spinup-lustre-server-fswl/)** | 長期保存データ         | 3.9 - 25 GB/s（※9）  | -<br>（※12）                                                                          |
| 標準ブロック・ボリューム<br>NFSサーバ         | **[ブロック・ボリュームで<br>ファイル共有ストレージを<br>構築する<br>（BM.Optimized3.36編）](/ocitutorials/hpc/spinup-nfs-server/)**                       | 中期保存データ         | 5 GiB/s（※10）       | **[Rclone](https://rclone.org/)** を使用した<br> **オブジェクト・ストレージ** への<br>バックアップ（※13） |
| 高性能ブロック・ボリューム<br>NFSサーバ         | **[ブロック・ボリュームで<br>ファイル共有ストレージを<br>構築する<br>（BM.Standard.E6.256編）](/ocitutorials/hpc/spinup-nfs-server-e6/)**                       | 中期保存データ         | 10 GiB/s（※10）       | **[Rclone](https://rclone.org/)** を使用した<br> **オブジェクト・ストレージ** への<br>バックアップ（※13） |
| DenceIO<br>NFSサーバ            | **[短期保存データ用<br>高速ファイル共有ストレージ<br>を構築する](/ocitutorials/hpc/spinup-nfs-server-nvme/)**                   | 短期保存データ         | 10 GiB/s（※10）      | **[Rclone](https://rclone.org/)** を使用した<br> **オブジェクト・ストレージ** への<br>バックアップ（※13） |

※7）対象の構築手法が提供するデータ可用性レベルに応じて分類しています。（データ可用性が低いほど短期保存データ用ファイル共有ストレージに分類）  
※8）**マウント・ターゲット** の最大スループット値を元に記載しています。  
※9）使用する **パフォーマンス層** （MB/s/TB）と **容量** （TB）を掛け合わせて計算した値で、最大値は **テナンシ** 当たり・ **可用性ドメイン** 当たりの **サービス制限** で決定されています。この **サービス制限** に関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/General/Concepts/servicelimits.htm#File_Storage_w_Lustre_Limits)** を参照してください。  
※10）**IOR** の測定値を元に記載しています。詳細な測定値は、 **[OCI HPCテクニカルTips集](#3-oci-hpcテクニカルtips集)** の **[HPC/GPUクラスタ向けファイル共有ストレージの最適な構築手法](/ocitutorials/hpc/tech-knowhow/howto-configure-sharedstorage/)** の **[2-1. コストパフォーマンスによる比較](/ocitutorials/hpc/tech-knowhow/howto-configure-sharedstorage/#2-1-コストパフォーマンスによる比較)** を参照してください。  
※11）スナップショット、クローン、及びレプリケーションが用意されています。これらサービスの詳細は、 **OCI** 公式ドキュメントの **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/File/home.htm)** を参照ください。  
※12）**オブジェクト・ストレージ** へのバックアップ機能が今後リリースされる予定です。（2025年4月現在）  
※13）このバックアップ環境は、チュートリアル **[ベア・メタル・インスタンスNFSサーバ向けバックアップサーバを構築する](/ocitutorials/hpc/spinup-backup-server)** に従って構築することが出来ます。

どのファイル共有ストレージ構築手法を選択するかは、 **[OCI HPCテクニカルTips集](#3-oci-hpcテクニカルtips集)** の **[HPC/GPUクラスタ向けファイル共有ストレージの最適な構築手法](/ocitutorials/hpc/tech-knowhow/howto-configure-sharedstorage/)** を参照してください。

## 1-3-2. バックアップ

ファイル共有ストレージのバックアップは、チュートリアル **[ベア・メタル・インスタンスNFSサーバ向けバックアップサーバを構築する](/ocitutorials/hpc/spinup-backup-server)** に従って構築するバックアップサーバで、容量単価の安価な **オブジェクト・ストレージ** や **ブロック・ボリューム** の **より低いコスト** にバックアップすることが可能です。

ファイル共有ストレージのバックアップを念頭に置いたバックアップ環境構築手法の比較は、 **[OCI HPCテクニカルTips集](#3-oci-hpcテクニカルtips集)** の **[ファイル共有ストレージ向けバックアップ環境の最適な構築手法](/ocitutorials/hpc/tech-knowhow/howto-choose-osbackuptool/)** を参照してください。

## 1-4. チュートリアルを組み合わせた実践的HPCシステム構築

本章は、異なるカテゴリのチュートリアルを組み合わせてより実践的なHPCシステムを構築する手法を紹介します。自身の要件に合わせてチュートリアルを選んだら、そのチュートリアル名をクリックします。

| No. | チュートリアル名                                                                                          | 組み合わせるチュートリアル                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 構築するシステム概要                                                              |
| :-: | ------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| 1   | **[ブロック・ボリュームNFSサーバと<br>基礎インフラ編HPC/GPUクラスタを<br>組み合わせる](/ocitutorials/hpc/cluster-with-bv-base/)** | **[ブロック・ボリュームで<br>NFSサーバを構築する<br>（BM.Optimized3.36編）](/ocitutorials/hpc/spinup-nfs-server/)**<br>or<br>**[ブロック・ボリュームで<br>NFSサーバを構築する<br>（BM.Standard.E6.256編）](/ocitutorials/hpc/spinup-nfs-server-e6/)**<br><br>**[HPCクラスタを構築する<br>(基礎インフラ手動構築編)](/ocitutorials/hpc/spinup-cluster-network/)**<br>or<br>**[GPUクラスタを構築する<br>(基礎インフラ手動構築編)](/ocitutorials/hpc/spinup-gpu-cluster/)**                                                                                                                                                                                                                             | 基礎インフラ編のHPC/GPUクラスタの<br>ファイル共有ストレージを<br> **ブロック・ボリューム** NFSサーバで<br>サービス |
| 2   | **[ブロック・ボリュームNFSサーバと<br>自動構築編HPC/GPUクラスタを<br>組み合わせる](/ocitutorials/hpc/cluster-with-bv-stack/)**  | **[ブロック・ボリュームで<br>NFSサーバを構築する<br>（BM.Optimized3.36編）](/ocitutorials/hpc/spinup-nfs-server/)**<br>or<br>**[ブロック・ボリュームで<br>NFSサーバを構築する<br>（BM.Standard.E6.256編）](/ocitutorials/hpc/spinup-nfs-server-e6/)**<br><br>**[HPCクラスタを構築する<br>(スタティッククラスタ自動構築編)](/ocitutorials/hpc/spinup-hpc-cluster)**<br>or<br>**[HPCクラスタを構築する<br>(オンデマンドクラスタ自動構築編)](/ocitutorials/hpc/spinup-hpc-cluster-withautoscaling)**<br>or<br>**[GPUクラスタを構築する<br>(スタティッククラスタ自動構築編)](/ocitutorials/hpc/spinup-gpu-cluster-withstack/)**<br>or<br>**[GPUクラスタを構築する<br>(オンデマンドクラスタ自動構築編)](/ocitutorials/hpc/spinup-gpu-cluster-withautoscaling/)** | 自動構築編のHPC/GPUクラスタの<br>ファイル共有ストレージを<br> **ブロック・ボリューム** NFSサーバで<br>サービス   |

下表は、各チュートリアルで構築するシステム仕様を示します。

| No. | 構築手法 | クラスタ管理機能 | スタティック/オンデマンド          | コンテナランタイム（※15） |
| :-: | :--: | :------: | :--------------------: | :-----------: |
| 1   | 手動   | 無し（※14）   | スタティック                 | **Docker CE** |
| 2   | 自動   | 有り       | スタティック<br>or<br>オンデマンド | **Enroot**    |

※14）ファイル共有ストレージは、 **ブロック・ボリューム** NFSサーバが提供します。  
※15）GPUクラスタが対象です。

***
# 2. OCI HPCパフォーマンス関連情報

## 2-0. 概要

本章は、HPC/機械学習ワークロードを実行する際のパフォーマンス関連情報を提供する、 **OCI HPCパフォーマンス関連情報** です。

提供するコンテンツは、以下のカテゴリに分類されます。

- **[標準ベンチマーク実行方法](#2-1-標準ベンチマーク実行方法)**

    このカテゴリは、構築したHPC/GPUクラスタが想定通りの性能となっているかを確認する際に有効な、標準ベンチマークの実行方法を解説しています。

- **[パフォーマンス関連Tips集](#2-2-パフォーマンス関連tips集)**

    このカテゴリは、パフォーマンスに影響するパラメータをどのように設定すればよいかといった、パフォーマンス関連Tipsを集めています。

- **[プロファイリング関連Tips集](#2-3-プロファイリング関連tips集)**

    このカテゴリは、HPCアプリケーションのパフォーマンス解析やチューニングに必要な情報を収集する、プロファイリング関連Tipsを集めています。

## 2-1. 標準ベンチマーク実行方法

本章は、HPC/GPUクラスタの基礎性能を計測するデファクトスタンダードな以下の標準ベンチマークを、HPC/機械学習ワークロード向けシェイプや **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** を使用して実行する方法を解説します。

- **HPL**
- **STREAM**
- **Intel MPI Benchmarks**
- **NCCL Tests**

各ベンチマークの実行方法は、下表の対象シェイプ部分のリンクをクリックして参照してください。

| 名称                      | ベンチマークサイトURL                                                                                               | 対象シェイプ                                                                                                                                  |
| :---------------------: | :--------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------: |
| **HPL**                 | **[Link](https://www.netlib.org/benchmark/hpl/)**                                                          | **[BM.Optimized3.36](/ocitutorials/hpc/benchmark/run-hpl/)**<br>**[BM.Standard.E5.192](/ocitutorials/hpc/benchmark/run-hpl-e5/)**<br>**[BM.Standard.E6.256](/ocitutorials/hpc/benchmark/run-hpl-e6/)**       |
| **STREAM**              | **[Link](https://www.cs.virginia.edu/stream/)**                                                            | **[BM.Optimized3.36](/ocitutorials/hpc/benchmark/run-stream/)**<br>**[BM.Standard.E5.192](/ocitutorials/hpc/benchmark/run-stream-e5/)**<br>**[BM.Standard.E6.256](/ocitutorials/hpc/benchmark/run-stream-e6/)** |
| **Intel MPI Benchmarks** | **[Link](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-mpi-benchmarks.html)** | **[BM.Optimized3.36](/ocitutorials/hpc/benchmark/run-imb/)**                                                                            |
| **NCCL Tests**          | **[Link](https://github.com/NVIDIA/nccl-tests)**                                                           | **[BM.GPU.A100-v2.8/BM.GPU4.8](/ocitutorials/hpc/benchmark/run-nccltests/)**<br>**[BM.GPU.H100.8](/ocitutorials/hpc/benchmark/run-nccltests-h100/)**                                                 |

## 2-2. パフォーマンス関連Tips集

本章は、HPC/機械学習ワークロードの実行時パフォーマンスに関連する、パフォーマンス関連Tipsを提供します。

- **[パフォーマンスに関連するベアメタルインスタンスのBIOS設定方法](/ocitutorials/hpc/benchmark/bios-setting/)**

    ベアメタルインスタンスは、作成する際にBIOSの設定を指定することが可能です。  
    これらの設定は、 **NPS** （NUMA nodes per socket）や **SMT** （Simultanious Multi Threading）といった、当該インスタンスの性能に影響するものが少なくありません。  
    本パフォーマンス関連Tipsは、これらのBIOS設定を指定してHPC/GPUクラスタを構築する方法を解説します。

- **[不要サービス停止によるパフォーマンスチューニング方法](/ocitutorials/hpc/benchmark/stop-unused-service/)**

    計算リソースを極限まで使用するHPCワークロードの実行は、些細な計算リソースを使用するOS常駐サービスがその性能に影響することがあります。  
    特に高並列実行時は、HPCクラスタ内の1ノードでこのようなサービスが稼働していることで、そのスケーラビリティに影響を及ぼします。  
    本パフォーマンス関連Tipsは、OS標準で稼働している常駐サービスの中でリソースを多く消費しているものを特定しこれを停止することで、OSレベルのパフォーマンスチューニングを実施する方法を解説します。

- **[クラスタ・ネットワークのトポロジーを考慮したノード間通信最適化方法](/ocitutorials/hpc/benchmark/topology-aware-cn-tuning/)**

    **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** は、業界標準のRoCEv2を採用する高帯域・低遅延のRDMA対応インターコネクトネットワークサービスで、そのトポロジーがFat treeのため同一リーフスイッチに接続するノード間とスパインスイッチを介して異なるリーフスイッチに接続するノード間で、ノード間通信のレイテンシが大きく異なります。このため、この特性を意識して適切な計算/GPUノードにジョブを配置することで、レイテンシに影響を受け易いワークロードの性能や高並列実行時のスケーラビリティを改善できる場合があります。  
    本パフォーマンス関連Tipsは、この **クラスタ・ネットワーク** のレイテンシ特性を生かしてマルチノードジョブをクラスタ内に配置することで、ノード間通信性能を最適化する方法を解説します。

- **[CFD解析フローのコストパフォーマンを向上させるOpenFOAM関連Tips](/ocitutorials/hpc/benchmark/openfoam-tuning/)**

    **[OpenFOAM](https://www.openfoam.com/)** は、CAE分野で多くの利用実績を持つオープンソースのCFDアプリケーションで、計算時に多くのメモリ帯域を使用したり実行中に多くのデータをファイルシステムに書き出したりする特性があるため、これらを考慮した実行方法を採用することでその性能を大きく向上させることが可能です。  
    本パフォーマンス関連Tipsは、HPCワークロードの実行に最適なベアメタルインスタンス **[BM.Optimized3.36](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized)** を **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** でノード間接続するHPCクラスタで **OpenFOAM** を使用する際、CFD解析フローをコストパフォーマンス良く実行するという観点で有益なTipsを解説します。

- **[OpenMPIのMPI通信性能に影響するパラメータとその関連Tips](/ocitutorials/hpc/benchmark/openmpi-perftips/)**

    **[OpenMPI](https://www.open-mpi.org/)** は、最新のMPI言語規格に準拠し、HPC/機械学習ワークロード実行に必要とされる様々な機能を備えたオープンソースのMPI実装です。  
    **OpenMPI** は、 **[Modular Component Architecture (MCA)](https://docs.open-mpi.org/en/v5.0.x/mca.html)** を採用し、ビルド時に組み込むコンポーネントを介して多彩な機能を提供する設計となっており、この **MCA** が用意する多数のパラメータを制御することで、MPI通信性能を最適化することが可能です。  
    また **OpenMPI** は、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** を介して高帯域・低遅延のMPIプロセス間通信を実現するための通信フレームワークに **[UCX](https://openucx.org/)** を採用し、MPI通信性能を最適化するためには **UCX** のパラメータを適切に設定することが求められます。  
    本パフォーマンス関連Tipsは、 **OpenMPI** のMPI通信性能に影響するパラメーターやその指定方法に関する有益なTipsを解説します。

- **[パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Optimized3.36編）](/ocitutorials/hpc/benchmark/cpu-binding/)**

    NUMAアーキテクチャを採用するインスタンスに於けるMPIやOpenMPの並列プログラム実行は、生成されるプロセスやスレッドをどのようにインスタンスのコアに割当てるかでその性能が大きく変動するため、その配置を意識してアプリケーションを実行することが求められます。  
    このため、使用するシェイプに搭載されるプロセッサのアーキテクチャやアプリケーションの特性に合わせて意図したとおりにプロセスやスレッドをコアに配置するために必要な、MPI実装、OpenMP実装、及びジョブスケジューラがそれぞれ有するコア割当て制御機能に精通している必要があります。  
    本パフォーマンス関連Tipsは、 **MPI** 実装に **[OpenMPI](https://www.open-mpi.org/)** 、 **OpenMP** 実装にGNUコンパイラ、及びジョブスケジューラに **[Slurm](https://slurm.schedmd.com/)** を取り上げ、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** 対応のHPCワークロード向けベア・メタル・シェイプ **[BM.Optimized3.36](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized)** でこれらのコア割当て機能を駆使してMPIプロセスやOpenMPスレッドのコア割当てを行う方法を解説します。

- **[パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Standard.E5.192編）](/ocitutorials/hpc/benchmark/cpu-binding-e5/)**

    NUMAアーキテクチャを採用するインスタンスに於けるMPIやOpenMPの並列プログラム実行は、生成されるプロセスやスレッドをどのようにインスタンスのコアに割当てるかでその性能が大きく変動するため、その配置を意識してアプリケーションを実行することが求められます。  
    このため、使用するシェイプに搭載されるプロセッサのアーキテクチャやアプリケーションの特性に合わせて意図したとおりにプロセスやスレッドをコアに配置するために必要な、MPI実装、OpenMP実装、及びジョブスケジューラがそれぞれ有するコア割当て制御機能に精通している必要があります。  
    本パフォーマンス関連Tipsは、 **MPI** 実装に **[OpenMPI](https://www.open-mpi.org/)** 、 **OpenMP** 実装にGNUコンパイラ、及びジョブスケジューラに **[Slurm](https://slurm.schedmd.com/)** を取り上げ、第4世代 **AMD EPYC** プロセッサを搭載するベア・メタル・シェイプ **[BM.Standard.E5.192](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-standard)** でこれらのコア割当て機能を駆使してMPIプロセスやOpenMPスレッドのコア割当てを行う方法を解説します。

- **[パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Standard.E6.256編）](/ocitutorials/hpc/benchmark/cpu-binding-e6/)**

    NUMAアーキテクチャを採用するインスタンスに於けるMPIやOpenMPの並列プログラム実行は、生成されるプロセスやスレッドをどのようにインスタンスのコアに割当てるかでその性能が大きく変動するため、その配置を意識してアプリケーションを実行することが求められます。  
    このため、使用するシェイプに搭載されるプロセッサのアーキテクチャやアプリケーションの特性に合わせて意図したとおりにプロセスやスレッドをコアに配置するために必要な、MPI実装、OpenMP実装、及びジョブスケジューラがそれぞれ有するコア割当て制御機能に精通している必要があります。  
    本パフォーマンス関連Tipsは、 **MPI** 実装に **[OpenMPI](https://www.open-mpi.org/)** 、 **OpenMP** 実装にGNUコンパイラ、及びジョブスケジューラに **[Slurm](https://slurm.schedmd.com/)** を取り上げ、第5世代 **AMD EPYC** プロセッサを搭載するベア・メタル・シェイプ **[BM.Standard.E6.256](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-standard)** でこれらのコア割当て機能を駆使してMPIプロセスやOpenMPスレッドのコア割当てを行う方法を解説します。

- **[OpenMPIのMPI集合通信チューニング方法（BM.Optimized3.36編）](/ocitutorials/hpc/benchmark/openmpi-perftune/)**

    MPI並列アプリケーションは、MPI通信時間がボトルネックになっている場合そのMPI通信をチューニングすることで性能が向上しますが、ボトルネックのMPI通信が集合通信の場合は、使用する通信アルゴリズムやその切り替えメッセージサイズ等の実行時パラメータ、MPIプロセス分割方法や **NUMA nodes per socket** 等のアプリケーション実行環境まで、様々な要因がその性能に影響します。  
    本パフォーマンス関連Tipsは、MPIの実装に **[OpenMPI](https://www.open-mpi.org/)** を取り上げ、これが採用する **[Modular Component Architecture](https://docs.open-mpi.org/en/v5.0.x/mca.html)** や **[UCX](https://openucx.org/)** の実行時パラメーター、MPIプロセス分割方法や **NUMA nodes per socket** を組合せて、HPCワークロード向けベア・メタル・シェイプ **BM.Optimized3.36** でMPI集合通信をチューニングする方法を解説します。

- **[OpenMPIのMPI集合通信チューニング方法（BM.Standard.E5.192編）](/ocitutorials/hpc/benchmark/openmpi-perftune-e5/)**

    MPI並列アプリケーションは、MPI通信時間がボトルネックになっている場合そのMPI通信をチューニングすることで性能が向上しますが、ボトルネックのMPI通信が集合通信の場合は、使用する通信アルゴリズムやその切り替えメッセージサイズ等の実行時パラメータ、MPIプロセス分割方法や **NUMA nodes per socket** 等のアプリケーション実行環境まで、様々な要因がその性能に影響します。  
    本パフォーマンス関連Tipsは、MPIの実装に **[OpenMPI](https://www.open-mpi.org/)** を取り上げ、これが採用する **[Modular Component Architecture](https://docs.open-mpi.org/en/v5.0.x/mca.html)** や **[UCX](https://openucx.org/)** の実行時パラメーター、MPIプロセス分割方法や **NUMA nodes per socket** を組合せて、第4世代 **AMD EPYC** プロセッサを搭載するベア・メタル・シェイプ **[BM.Standard.E5.192](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-standard)** でMPI集合通信をチューニングする方法を解説します。

- **[OpenMPIのMPI集合通信チューニング方法（BM.Standard.E6.256編）](/ocitutorials/hpc/benchmark/openmpi-perftune-e6/)**

    MPI並列アプリケーションは、MPI通信時間がボトルネックになっている場合そのMPI通信をチューニングすることで性能が向上しますが、ボトルネックのMPI通信が集合通信の場合は、使用する通信アルゴリズムやその切り替えメッセージサイズ等の実行時パラメータ、MPIプロセス分割方法や **NUMA nodes per socket** 等のアプリケーション実行環境まで、様々な要因がその性能に影響します。  
    本パフォーマンス関連Tipsは、MPIの実装に **[OpenMPI](https://www.open-mpi.org/)** を取り上げ、これが採用する **[Modular Component Architecture](https://docs.open-mpi.org/en/v5.0.x/mca.html)** や **[UCX](https://openucx.org/)** の実行時パラメーター、MPIプロセス分割方法や **NUMA nodes per socket** を組合せて、第5世代 **AMD EPYC** プロセッサを搭載するベア・メタル・シェイプ **[BM.Standard.E6.256](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-standard)** でMPI集合通信をチューニングする方法を解説します。

## 2-3. プロファイリング関連Tips集

本章は、HPCアプリケーションのパフォーマンス解析やチューニングに必要な情報を収集する、プロファイリング関連Tips集を提供します。

- **[PAPIでHPCアプリケーションをプロファイリング](/ocitutorials/hpc/benchmark/papi-profiling/)**

    HPCワークロードの実行に最適なベアメタル・インスタンスでアプリケーションを実行する場合、高価な計算資源を有効活用出来ているかを検証するため、アプリケーションのプロファイリングを実施することが一般的です。  
    **[PAPI](https://icl.utk.edu/papi/)** は、HPCワークロード向け **ベアメタル・シェイプ** に採用されている **Intel Ice Lake** や **AMD EPYC 9004シリーズ** のCPUが持つハードウェアカウンタから浮動小数点演算数やキャッシュヒット数といったプロファイリングに有益な情報を取得するAPIを提供し、HPCアプリケーションのプロファイリングに欠かせないツールとなっています。  
    本プロファイリング関連Tipsは、ベアメタルインスタンス上で実行するHPCアプリケーションを **PAPI** を使ってプロファイリングする方法を解説します。

- **[Score-P・Scalasca・CubeGUIで並列アプリケーションをプロファイリング](/ocitutorials/hpc/benchmark/scorep-profiling/)**

    並列アプリケーションは、ロードバランス不均衡やプロセス間通信の影響等で並列実行数の増加と共にスケーラビリティの低下が発生しますが、HPCワークロードの実行に最適なベアメタル・インスタンスでアプリケーションを高並列実行する場合、高価な計算資源を有効活用する観点から、スケーラビリティ低下の原因を調査しチューニングでスケーラビリティーを改善する開発プロセスを踏んだり、最も効率的な並列実行数を見極める必要があり、これらの判断に必要な情報を得るためにアプリケーションをプロファイリングすることが重要です。  
    本プロファイリング関連Tipsは、ベアメタルインスタンス上で実行するOpenMPやMPIでコーディングされた並列プログラムをオープンソースの **[Score-P](https://www.vi-hps.org/projects/score-p/)** 、 **[Scalasca](https://www.scalasca.org/)** 、及び **[CubeGUI](https://www.scalasca.org/scalasca/software/cube-4.x/download.html)** を駆使してプロファイリングし、並列アプリケーションを効果的に実行するための有益な情報を取得する方法を解説します。

***
# 3. OCI HPCテクニカルTips集

## 3-0. 概要

本章は、HPC/機械学習ワークロードを実行する際に有益なテクニカルTipsを集めた、 **OCI HPCテクニカルTips集** です。

各テクニカルTipsは、以下のカテゴリに分類されます。

- **[クラスタ・ネットワーク](#3-1-クラスタネットワーク)**

    このカテゴリは、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** に関連するテクニカルTipsを集めています。

- **[ストレージ](#3-2-ストレージ)**

    このカテゴリは、HPC/GPUクラスタと共に使用するストレージに関連するテクニカルTipsを集めています。

- **[クラスタ管理](#3-3-クラスタ管理)**

    このカテゴリは、多数の計算/GPUノードを効果的に構築・運用するためのクラスタ管理に関するテクニカルTipsを集めています。

- **[機械学習](#3-4-機械学習)**

    このカテゴリは、機械学習ワークロード実行に関するテクニカルTipsを集めています。

- **[ソフトウェア環境](#3-5-ソフトウェア環境)**

    このカテゴリは、HPC/機械学習ワークロードを実行する環境を構築する際に必要となる、プログラム開発環境（コンパイラ、MPI、線形代数演算ライブラリ）やリソース管理・ジョブ管理システム（ジョブスケジューラ、コンテナランタイム）等のソフトウェア環境を構築・整備する際に有益なテクニカルTipsを集めています。

- **[その他](#3-6-その他)**

    このカテゴリは、以上の何れのカテゴリにも属さないテクニカルTipsを集めています。

## 3-1. クラスタ・ネットワーク

- **[クラスタネットワーキングイメージを使ったクラスタ・ネットワーク接続方法](/ocitutorials/hpc/tech-knowhow/howto-connect-clusternetwork/)**

    **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** 対応シェイプを使用して **クラスター・ネットワーク** に接続するインスタンスは、ベースOSに **Oracle Linux** を使用する **[クラスタネットワーキングイメージ](#5-13-クラスタネットワーキングイメージ)** を使用することで、 **クラスタ・ネットワーク** 接続に必要なソフトウェアのインストールやセットアップ等の作業を大幅に簡素化することが可能です。  
    本テクニカルTipsは、この **クラスタネットワーキングイメージ** を使用してインスタンスを **クラスタ・ネットワーク** に接続する方法を解説します。

- **[クラスタネットワーキングイメージの選び方](/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/)**

    **クラスタ・ネットワーク** を使用するインスタンスは、接続に必要なソフトウェアがインストールされている必要がありますが、これらを含んだOSイメージである **[クラスタネットワーキングイメージ](#5-13-クラスタネットワーキングイメージ)** が **[マーケットプレース](#5-5-マーケットプレイス)** から提供されています。  
    本テクニカルTipsは、この **クラスタネットワーキングイメージ** の適切な選び方を解説します。

- **[クラスタ・ネットワーク未対応OSを使ったクラスタ・ネットワーク接続方法](/ocitutorials/hpc/tech-knowhow/howto-create-cnenabled-osimage/)**

    **クラスタ・ネットワーク** に接続するインスタンスは、接続に必要なソフトウェアがインストールされている必要があり、これらを含んだ **[クラスタネットワーキングイメージ](#5-13-クラスタネットワーキングイメージ)** が **[マーケットプレース](#5-5-マーケットプレイス)** から提供されていますが、これらのベースとなるOSは、 **Oracle Linux** のみです。  
    本テクニカルTipsは、 **クラスタ・ネットワーク** 対応シェイプの **[BM.Optimized3.36](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized)** や **[BM.GPU4.8/BM.GPU.A100-v2.8](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-gpu)** を使用するインスタンスを **Oracle Linux** 以外のOSで **クラスタ・ネットワーク** に接続する方法を解説します。

- **[クラスタ・ネットワークに接続する計算/GPUノード作成時の問題判別方法](/ocitutorials/hpc/tech-knowhow/determine-cnrelated-issue/)**

    **クラスタ・ネットワーク** に接続する計算/GPUノードの作成は、接続する **クラスタ・ネットワーク** の論理的なパーティション内で利用可能なインスタンスが不足していると、これに失敗します。  
    本テクニカルTipsは、このようなケースで発生する問題を特定する方法を解説します。

- **[クラスタ・ネットワーク統計情報の取得方法](/ocitutorials/hpc/tech-knowhow/howto-get-cnrelated-statistics/)**

    複数ノードに跨るHPC/機械学習ワークロードを実行するHPC/GPUクラスタは、ノード間通信に使用する **クラスタ・ネットワーク** が想定通りに使用されて初めてその性能を発揮することが出来ます。  
    ここで、インスタンスを **クラスタ・ネットワーク** に接続するNIC（ **NVIDIA Mellanox ConnectX** ）は、これを介して通信する際の様々な統計情報を記録するハードウェアカウンタを備えており、インスタンスのOS上でこれらを取得することが可能です。  
    本テクニカルTipsは、 **クラスタ・ネットワーク** に接続するインスタンスで **クラスタ・ネットワーク** の利用状況や問題判別に役立つ統計情報を取得する方法を解説します。

## 3-2. ストレージ

- **[ベアメタルインスタンスのNVMe SSDローカルディスク領域ファイルシステム作成方法](/ocitutorials/hpc/tech-knowhow/nvme-filesystem/)**

    高速スクラッチ領域として利用することを想定したNVMe SSDローカルディスクを内蔵するHPCクラスタ向けベアメタルシェイプ **[BM.Optimized3.36](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized)** やGPUクラスタ向けベアメタルシェイプ **[BM.GPU4.8/BM.GPU.A100-v2.8/BM.GPU.H100.8](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-gpu)** は、NVMe SSDローカルディスクをOSのファイルシステムとして利用するための設定をユーザ自身が行う必要があります。  
    本テクニカルTipsは、このファイルシステム作成方法を解説します。

- **[HPC/GPUクラスタ向けファイル共有ストレージの最適な構築手法](/ocitutorials/hpc/tech-knowhow/howto-configure-sharedstorage/)**

    HPC/GPUクラスタを運用する際必須となるファイル共有ストレージは、その選択肢にマネージドサービスを使用する方法と、高帯域のネットワークポートを持つベア・メタル・インスタンスとストレージサービスで構築する方法があります。  
    本テクニカルTipsは、コストパフォーマンス、可用性、構築・運用のしやすさ、最大性能等を考慮し、自身の要件に沿った最適なファイル共有ストレージ構築手法を選定する方法を解説します。

- **[ブロック・ボリュームを使用するNFSサーバのインスタンス障害からの復旧方法](/ocitutorials/hpc/tech-knowhow/bv-sharedstorage-recovery/)**

    **ブロック・ボリューム** とベアメタルインスタンスを組み合わせたNFSサーバは、HPC/GPUクラスタで必須となるファイル共有ストレージをコストパフォーマンス良く運用するためには最適なソリューションですが、NFSサーバとなるベアメタルインスタンスに障害が発生し起動できなくなった場合、どのようにデータ領域を復旧すればよいでしょうか。  
    本テクニカルTipsは、このシナリオで発生する障害の復旧方法を解説します。

- **[計算/GPUノードのブート・ボリューム動的拡張方法](/ocitutorials/hpc/tech-knowhow/boot-volume-extension/)**

    インスタンスのルートファイルシステムを格納する **ブート・ボリューム** は、OSを停止することなく動的にその容量を拡張することが可能です。  
    ただこの動的拡張は、 **OCI** コンソールやインスタンスOSで複数のオペレーションを実施する必要があり、ノード数が多くなるクラスタ環境の計算/GPUノードでは、これらのオペレーションを効率的に実施することが求められます。  
    本テクニカルTipsは、HPC/GPUクラスタの多数の計算/GPUノードに対し、 **ブート・ボリューム** の動的拡張を効率的に実施する方法を解説します。

- **[ファイル共有ストレージ向けバックアップ環境の最適な構築手法](/ocitutorials/hpc/tech-knowhow/howto-choose-osbackuptool/)**

    HPC/GPUクラスタを運用する際必須となるファイル共有ストレージは、コストパフォーマンスを考慮すると **ベア・メタル・インスタンス** と **ブロック・ボリューム** 等のストレージサービスで構築することになりますが、そのバックアップ環境は自身で構築する必要があり、バックアップを格納するストレージはその安価な容量単価から **オブジェクト・ストレージ** や **ブロック・ボリューム** の **より低いコスト** が有力な選択肢になります。  
    本テクニカルTipsは、ファイル共有ストレージのバックアップを容量単価の安価なストレージに取得することを念頭に、自身のバックアップ要件に沿った最適なバックアップ環境構築手法を選定する方法を解説します。

## 3-3. クラスタ管理

- **[計算/GPUノードの効果的な名前解決方法](/ocitutorials/hpc/tech-knowhow/compute-name-resolution/)**

    ノード数が多くなるHPC/GPUクラスタの計算/GPUノードの名前解決は、どのように行うのが効果的でしょうか。  
    本テクニカルTipsは、 **仮想クラウド・ネットワーク** のDNSを使用した効果的な計算/GPUノードの名前解決方法を解説します。

- **[計算/GPUノードデプロイ時の効果的なOSカスタマイズ方法](/ocitutorials/hpc/tech-knowhow/compute-os-customization/)**

    ノード数が多くなるHPC/GPUクラスタの計算/GPUノードは、デプロイ時に実施するOSカスタマイズをどのように行うのが効果的でしょうか。  
    本テクニカルTipsは、計算/GPUノードデプロイ時のOSカスタマイズ方法の選択肢と、それぞれの利用方法について解説します。

- **[計算/GPUノードのホスト名リスト作成方法](/ocitutorials/hpc/tech-knowhow/compute-host-list/)**

    ノード数が多くなるHPC/GPUクラスタは、全ての計算/GPUノードのホスト名の一覧を記載したホスト名リストを作成することで、構築・運用作業を効率的に進めることが可能になります。  
    本テクニカルTipsは、HPC/GPUクラスタの計算/GPUノードのホスト名リストを効果的に作成する方法を解説します。

- **[計算/GPUノードの追加・削除・入れ替え方法](/ocitutorials/hpc/tech-knowhow/cluster-resize/)**

    HPC/GPUクラスタは、実行するワークロードの増減に伴い計算/GPUノードのノード数を増減する必要が生じることがあります。またハードウェア障害が発生すると、利用可能なノード数を維持するために当該ノードを別のノードに置き換える必要が生じます。  
    本テクニカルTipsは、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** を使用するHPC/GPUクラスタで計算/GPUノードのノード数を増減する方法や置き換える方法を解説します。

- **[pdshで効率的にクラスタ管理オペレーションを実行](/ocitutorials/hpc/tech-knowhow/cluster-with-pdsh/)**

    ノード数が多くなるHPC/GPUクラスタは、クラスタに含まれるノードに対して様々な管理オペレーションを実施する必要があります。この時、これらのオペレーションを実現するためのコマンドを全てのノードに適用する際、どのような方法が効果的でしょうか。  
    本テクニカルTipsは、 **pdsh** を使用してHPC/GPUクラスタの管理オペレーションを効率的に実施する方法を解説します。

- **[オンデマンドクラスタ実現のためのインスタンス・プリンシパル認証設定方法](/ocitutorials/hpc/tech-knowhow/instance-principal-auth/)**

    パブリッククラウドは、ワークロード発生時に必要な規模のHPC/GPUクラスタを構築し、ワークロード終了時にこれを削除する、オンデマンドクラスタ環境を構築するには最適なサービスです。  
    オンデマンドクラスタの管理は、ソフトウェアにより自動化することが一般的ですが、HPC/GPUクラスタに必要なリソースの作成・終了をこのアプリケーションに許可するための仕組みとして、 **[インスタンス・プリンシパル](#5-15-インスタンスプリンシパル)** 認証が用意されています。  
    本テクニカルTipsは、オンデマンドクラスタを念頭とした **インスタンス・プリンシパル** 認証の設定方法を解説します。

- **[OCIロギングとGrafanaを使用したHPC/GPUクラスタのログ監視方法](/ocitutorials/hpc/tech-knowhow/log-monitoring/)**

    ノード数が多くなるHPC/GPUクラスタは、各計算/GPUノードに分散するログを一元的に監視するフレームワークを構築することで、運用管理工数の低減や監視対象ログの見落としを防ぎ、システムセキュリティーを効率的に維持することが可能です。  
    このフレームワーク構築に活用できるソフトウェアはいくつかありますが、 **[OCIロギング](https://docs.oracle.com/ja-jp/iaas/Content/Logging/home.htm)** と **[Grafana](https://grafana.com/)** を統合したログ監視は、 **Grafana** の多彩な機能を活用できる点で有力な選択肢です。  
    本テクニカルTipsは、 **OCIロギング** と **Grafana** を使用してHPC/GPUクラスタのログを効率的に監視する方法を解説します。

- **[OCIモニタリングとGrafanaを使用したHPC/GPUクラスタのメトリック監視方法](/ocitutorials/hpc/tech-knowhow/metric-monitoring/)**

    HPCワークロードや機械学習ワークロードを実行するHPC/GPUクラスタは、ワークロード実行中のCPU/GPU使用率、メモリ使用率、ネットワーク使用帯域等のメトリックを定期的に監視し、高価な計算資源を有効活用することが求められますが、ノード数が多くなるHPC/GPUクラスタでは、これらメトリックの監視が一元的・効率的に行える必要があります。  
    このフレームワーク構築に活用できるソフトウェアはいくつかありますが、 **[OCIモニタリング](https://docs.oracle.com/ja-jp/iaas/Content/Monitoring/home.htm)** と **[Grafana](https://grafana.com/)** を統合したメトリック監視は、 **Grafana** の多彩な機能を活用できる点で有力な選択肢です。  
    本テクニカルTipsは、 **OCIモニタリング** と **Grafana** を使用してHPC/GPUクラスタのメトリックを効率的に監視する方法を解説します。

## 3-4. 機械学習

- **[UbuntuをOSとする機械学習ワークロード向けGPUノード構築方法](/ocitutorials/hpc/tech-knowhow/gpu-with-ubuntu/)**

    機械学習ワークロード実行のためのGPU搭載ノードは、NVIDIAが提供する様々なGPU関連ソフトウェアの開発が主に **Ubuntu** で行われていることから、そのOSに **Ubuntu** を使用するのが主流になっていますが、 **Ubuntu** をOSに指定してGPU搭載インスタンスをデプロイする場合、GPUを利用するためのソフトウェアを自身でインストール・セットアップする必要があります。  
    本テクニカルTipsは、 **Ubuntu** をGPU搭載インスタンスと共にデプロイした後GPU利用に必要なソフトウェアをインストール・セットアップすることで、機械学習ワークロード向けGPUノードを構築する方法を解説します。

## 3-5. ソフトウェア環境

- **[Slurm環境での利用を前提とするUCX通信フレームワークベースのOpenMPI構築方法](/ocitutorials/hpc/tech-knowhow/build-openmpi/)**

    **[OpenMPI](https://www.open-mpi.org/)** は、最新のMPI言語規格に準拠し、HPC/機械学習ワークロード実行に必要とされる様々な機能を備えたオープンソースのMPI実装です。  
    **OpenMPI** で作成したアプリケーションのHPC/GPUクラスタに於ける実行は、計算リソース有効利用の観点から通常ジョブスケジューラを介したバッチジョブとして行いますが、ジョブスケジューラが **[Slurm](https://slurm.schedmd.com/)** の場合、 **[PMIx](https://pmix.github.io/)** を使用することでMPIアプリケーションの起動や通信初期化のスケーラビリティを向上させることが可能です。  
    また **[UCX](https://openucx.org/)** は、 **OpenMPI** が **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** を介して高帯域・低遅延のMPIプロセス間通信を実現するために欠かせない通信フレームワークです。  
    本テクニカルTipsは、 **PMIx** を使用する **Slurm** 環境で通信フレームワークに **UCX** の使用を前提とする **OpenMPI** 構築方法を解説します。

- **[Slurmによるリソース管理・ジョブ管理システム構築方法](/ocitutorials/hpc/tech-knowhow/setup-slurm-cluster/)**

    HPC/GPUクラスタのリソース管理・ジョブ管理は、ジョブスケジューラを活用することでこれを効率的かつ柔軟に運用することが可能です。近年のHPC/機械学習ワークロードの大規模化は、MPI等を使ったノード間並列ジョブの重要性を増大させ、このような大規模ジョブを様々な運用ポリシーに沿って処理出来る機能をジョブスケジューラに求めています。オープンソースのジョブスケジューラ **[Slurm](https://slurm.schedmd.com/)** は、この要求を満足出来る代表的なジョブスケジューラとして現在人気を集めています。  
    本テクニカルTipsは、HPC/機械学習ワークロードの実行に最適なベアメタルインスタンスを高帯域・低遅延RDMAインターコネクトサービスの **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** で接続するHPC/GPUクラスタで、リソース管理・ジョブ管理システムを **Slurm** で構築する方法を解説します。

- **[線形代数演算ライブラリインストール・利用方法](/ocitutorials/hpc/tech-knowhow/install-blas/)**

    HPCワークロードを実行する際、行列やベクトルの線形代数演算を高速に実行する必要が生じます。  
    これらの演算は、ソースコードを自作することで対応することも出来ますが、オープンソースで配布されている線形代数演算ライブラリである **[BLAS](https://www.netlib.org/blas/)** や **[OpenBLAS](https://github.com/OpenMathLib/OpenBLAS/wiki)** を利用することで、開発工数の削減、保証された計算精度、高速な演算の実行等、様々なメリットを享受することが可能です。  
    本テクニカルTipsは、 **BLAS** と **OpenBLAS** をHPCワークロードの実行に最適なベアメタルインスタンスにインストールし、Fortranのサンプルコードからこれを利用する方法を解説します。

- **[OpenFOAMインストール・利用方法](/ocitutorials/hpc/tech-knowhow/install-openfoam/)**

    **[OpenFOAM](https://www.openfoam.com/)** は、CAE分野で多くの利用実績を持つオープンソースのCFDアプリケーションです。  
    **OpenFOAM** は、メッシュ作成等のプリ処理、ソルバーによる解析処理、及び解析結果を可視化するポスト処理の全てのCFD解析フローを、自身が提供するツール群と外部のツール群を組合せてオープンソースソフトウェアで完結することが可能です。また **OpenFOAM** が提供するソルバーは、MPIで並列化されており、1万コアを超える並列実行の実績も報告されています。  
    本テクニカルTipsは、 **OpenFOAM** とこれを中核とするCFD解析フローに有用なオープンソースのツール群をHPCワークロードの実行に最適なベアメタルインスタンスにインストールし、これを利用する方法を解説します。

- **[Slurmによるリソース管理・ジョブ管理システム運用Tips](/ocitutorials/hpc/tech-knowhow/slurm-tips/)**

    オープンソースの **[Slurm](https://slurm.schedmd.com/)** は、HPC/GPUクラスタのリソース管理・ジョブ管理をコストパフォーマンス良く運用するためのジョブスケジューラとして、現在有力な選択肢です。  
    本テクニカルTipsは、構築するHPC/GPUクラスタのリソース管理・ジョブ管理を **Slurm** で効果的に運用するための様々なテクニカルTipsをご紹介します。

- **[Oracle Linuxプラットフォーム・イメージベースのHPCワークロード実行環境構築方法](/ocitutorials/hpc/tech-knowhow/build-oraclelinux-hpcenv/)**

    HPCワークロードは、複数の計算ノードを **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** でノード間接続するHPCクラスタで実行することが主流ですが、 **[BM.Standard.E6.256](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-standard)** のような高性能のベアメタル・シェイプは、11 TFLOPSを超える理論性能と3 TBのDDR5メモリを有し、単一ノードでも十分大規模なHPCワークロードを実行することが可能です。  
    このように単一ノードでHPCワークロードを実行する場合は、ベースOSの **Oracle Linux** のバージョンに制約のある **[クラスタネットワーキングイメージ](#5-13-クラスタネットワーキングイメージ)** を使用する必要が無く、 **[プラットフォーム・イメージ](#5-17-プラットフォームイメージ)** から最新の **Oracle Linux** を選択することが可能になります。  
    本テクニカルTipsは、単一ノードでHPCワークロードを実行することを念頭に、 **プラットフォーム・イメージ** から提供される最新の **Oracle Linux** 上に **[OpenMPI](https://www.open-mpi.org/)** と **[Slurm](https://slurm.schedmd.com/)** をインストールしてHPC環境を構築する方法を解説します。

## 3-6. その他

- **[ベアメタルインスタンスのカーネルダンプ取得方法](/ocitutorials/hpc/tech-knowhow/kdump-on-baremetal/)**

    カーネルダンプは、Linuxカーネルに関連する問題を解析する際に重要な情報ですが、ルートファイルシステムを含む **ブート・ボリューム** をiSCSI接続するベアメタルインスタンスは、仮想マシンインスタンスと比較してその取得に特別な準備が必要です。  
    本テクニカルTipsは、ベアメタルインスタンスでカーネルダンプを取得し、これをcrashコマンドで読み込むまでの手順を解説します。

- **[サイト間VPNによるOCIとの拠点間接続方法](/ocitutorials/hpc/tech-knowhow/site-to-site-vpn/)**

    HPC/機械学習ワークロードをオンプレミスから **OCI** に移行する際、多くのケースでオンプレミスと **OCI** を拠点間接続する必要が生じます。  
    例えば、オンプレミス側の有償CAEアプリケーション用ライセンスサーバに **OCI** 側の計算ノードからライセンス取得のためにアクセスするケースや、オンプレミス側の端末から **OCI** 側のログインノードにログインしインタラクティブ処理を行うケースです。  
    本テクニカルTipsは、 **[サイト間VPN](https://docs.oracle.com/ja-jp/iaas/Content/Network/Tasks/managingIPsec.htm)** を使用してIPSecのトンネルモードでオンプレミスと **OCI** を拠点間接続し、 **OCI** 側プライベートサブネットに接続する計算ノード相当のインスタンスとオンプレミス側プライベートサブネットに接続するライセンスサーバ相当のインスタンスの疎通を可能とするための手順を解説します。

***
# 4. OCI HPC関連情報リンク集

## 4-0. 概要

本章は、HPC/機械学習ワークロードを実行する際の有益な情報を提供するウェブサイトの情報を集めた、 **OCI HPC関連情報リンク集** です。

各ウェブサイトは、以下のカテゴリに分類されます。

- **[クラスタ・ネットワーク](#4-1-クラスタネットワーク)**

    このカテゴリは、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** に関連する情報を提供するウェブサイトのリンク集です。

- **[ストレージ](#4-2-ストレージ)**

    このカテゴリは、HPC/GPUクラスタと共に使用するストレージに関連する情報を提供するウェブサイトのリンク集です。

## 4-1. クラスタ・ネットワーク

- **[First Principles: Building a high-performance network in the public cloud](https://blogs.oracle.com/cloud-infrastructure/post/building-high-performance-network-in-the-cloud)**

    **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** は、データリンク層にイーサネット、トランスポート層にIBトランスポートを使用し、マイクロ秒レベルの低レイテンシでRDMA通信を実現しますが、パケットロスの発生を前提とするイーサネット上（Lossy network）でTCP/IPのような再送制御を持たないIBトランスポートを使用し、どのようにパケットロスにセンシティブなRDMA通信を可能にしているのでしょうか。  
    本ブログとそこからリンクされている同タイトルのYouTubeビデオは、輻輳制御の仕組みである **DCQCN（Data Center Quantitized Congention Notification）** を使用して **クラスタ・ネットワーク** がこの問題に対処している点、実行するワークロードタイプに応じて **DCQCN** の特性をレイテンシと帯域幅の観点で最適化している点を解説しています。

- **[Congestion Control for Large-Scale RDMA Deployments](https://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p523.pdf)**

    **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** は、輻輳制御の仕組みである **DCQCN（Data Center Quantitized Congention Notification）** と、イーサネットでロスレスネットワークを実現する仕組みである **PFC（Priority Flow Control）** を使用して、高帯域・低レイテンシのRDMA通信をRoCEv2上に実現します。  
    2015年のSIGCOMMカンファレンスで発表された本論文は、 **クラスタ・ネットワーク** で採用している **DCQCN** と **PFC** の組み合わせで実現する輻輳制御を初めて提案しました。  
    ここで提案する輻輳制御は、 **PFC** のもつ輻輳伝搬の問題を新たに考案した **DCQCN** を組合せることで解決し、RoCEv2ファブリックの帯域幅・レイテンシ・通信安定性を向上させますが、本論文の中ではこれらをシミュレーションや実験の結果から示しています。

## 4-2. ストレージ

- **[The best choice for NFS file service running on Oracle Cloud Infrastructure](https://blogs.oracle.com/cloud-infrastructure/post/best-choice-for-nfs-file-service-running-on-oracle-cloud-infrastructure)**

    HPC/GPUクラスタを運用する際必須となるファイル共有ストレージは、 **ブロック・ボリューム** とベアメタルインスタンスの **[BM.Optimized3.36](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized)** を組み合わせてNFSでサービスすることで、コストパフォーマンスを最大化することが可能です。  
    その理由は、このシェイプが50 GbpsのTCP/IP接続用ポートを2個搭載し、それぞれをiSCSI接続の **ブロック・ボリューム** アクセス用途とNFSクライアントへのNFSサービス用途に割当ててこれらを同時に使用することで、NFSサービスに50 Gbpsの帯域をフルに使用することが可能となるためです。  
    本ブログは、 **ブロック・ボリューム** と **BM.Optimized3.36** を組み合わせたNFSサーバに対し、IORとmdtestを使用したIO性能と信頼性の評価を行い、その結果を解説しています。

- **[Gfarm development environment on OCI managed by Terraform and Ansible](https://github.com/oss-tsukuba/incus-auto/tree/main/example/gfarm-terraform-oci)**

    **[Gfarmファイルシステム](http://oss-tsukuba.org/software/gfarm)** は、 **[NPO法人つくばOSS技術支援センター](http://oss-tsukuba.org/)** が技術支援を提供し、オープンソースとして利用可能な大容量、高信頼、高性能なストレージを低コストで提供する、分散ファイルシステムです。  
    **Gfarmファイルシステム** は、ファイルサーバとして機能するファイルシステムノードを複数用意し、これらのノードにデータを複成して分散配置することで、高い帯域幅と可用性を実現します。  
    本GitHubサイトは、 **Gfarm** 環境を **OCI** 上に **[Terraform](#5-12-terraform)** と **Ansible** で自動構築するスクリプト群を提供し、トップページの構築手順に従うことでその構築作業を大幅に簡素化します。

***
# 5. OCI HPC用語集

## 5-0. 概要

本章は、 **OCI** のHPCに関連する用語を解説する、 **OCI HPC用語集** です。  
本ポータルサイトで提供するコンテンツを読み進める際、不明な **OCI** 用語の理解に役立ててください。

ここで解説する用語は、本ポータルサイトで提供するコンテンツの使用箇所からリンクされており、適宜参照することが出来ます。

## 5-1. クラスタ・ネットワーク

**クラスタ・ネットワーク** は、ノード間を高帯域・低遅延に接続するインターコネクトネットワークサービスで、以下の特徴を持ちます。

- RoCEv2を採用するリンク当たり100/200 Gbpsの高帯域・低遅延RDMAインターコネクト
- オーバーサブスクリプションの無いノンブロッキングトポロジーから来る高い性能再現性

**クラスタ・ネットワーク** の作成は、予め作成した **[インスタンス構成](#5-7-インスタンス構成)** と **クラスタ・ネットワーク** に接続するインスタンス数を指定して行い、この **インスタンス構成** に紐づく **[インスタンス・プール](#5-8-インスタンスプール)** が自動的に作成され、この **インスタンス・プール** が **クラスタ・ネットワーク** に接続するインスタンスを **インスタンス構成** に従ってデプロイします。  
**OCI** リソースとしての **クラスタ・ネットワーク** は、共にデプロイされるインスタンスを高帯域・低遅延で接続する、 **仮想クラウド・ネットワーク** とは独立したノード間インターコネクトとして機能します。

**クラスタ・ネットワーク** に接続するインスタンスは、 **[ここ](https://docs.public.oneportal.content.oci.oraclecloud.com/ja-jp/iaas/Content/Compute/Tasks/managingclusternetworks.htm#supported-shapes)** に記載の **クラスタ・ネットワーク** 対応シェイプでから選択することが可能で、以下のソフトウェアがインストールされている必要があります。

- Mellanox OFED
- wpa_supplicant
- 802.1X認証関連ユーティリティソフトウェア
- **クラスタ・ネットワーク** 設定ユーティリティソフトウェア

これらの **クラスタ・ネットワーク** 接続に必要なソフトウェアは、自身でインストールすることも出来ますが、これらが予めインストールされた **[クラスタネットワーキングイメージ](#5-13-クラスタネットワーキングイメージ)** も用意されています。

**クラスタ・ネットワーク** と共に作成するインスタンスは、デプロイ後にOS上で以下の処理を完了することで、 **クラスタ・ネットワーク** を利用することが可能になります。

- **クラスタ・ネットワーク** との802.1X認証
- **クラスタ・ネットワーク** 接続用ネットワークインターフェースへのIPアドレス付与

これらの処理は、前述の802.1X認証関連ユーティリティソフトウェアと **クラスタ・ネットワーク** 設定ユーティリティソフトウェアで行います。  
この手順は、 **[OCI HPCテクニカルTips集](#3-oci-hpcテクニカルtips集)** の **[クラスタネットワーキングイメージを使ったクラスタ・ネットワーク接続方法](/ocitutorials/hpc/tech-knowhow/howto-connect-clusternetwork/)** を参照してください。

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Compute/Tasks/managingclusternetworks.htm)** を参照してください。

**OCI** コンソールの **クラスタ・ネットワーク** メニューは、 **[ここ](https://cloud.oracle.com/compute/cluster-networks)** をクリックしてアクセスします。 **OCI** へのログインを要求された場合は、ログインを完了して下さい。

## 5-2. リソース・マネージャ

**リソース・マネージャ** は、 **OCI** のリソース管理（作成・変更・削除）の自動化をIaC（Infrastructure-as-code）のコンセプトで実現する、 **OCI** コンソールからGUIで操作するマネージド **[Terraform](#5-12-terraform)** サービスで、 **OCI** の様々なサービスを組み合わせた複雑なシステムの構築を自動化することで、システム構築や運用に必要な工数を大幅に削減します。

**リソース・マネージャ** は、様々な **OCI** サービスを組み合わせたシステム構築作業を、メニュー形式でオプションを選択することで予め作成された **[スタック](#5-3-スタック)** を **OCI** コンソールから適用・破棄することで、1クリックで実施することを可能にします。

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/ResourceManager/Concepts/resourcemanager.htm)** を参照してください。

**リソース・マネージャ** の利用方法は、チュートリアル **[リソース・マネージャを使ってサンプルアプリケーションをデプロイする](https://oracle-japan.github.io/ocitutorials/intermediates/resource-manager/)** を参照してください。

**OCI** コンソールの **リソース・マネージャ** メニューは、 **[ここ](https://cloud.oracle.com/resourcemanager/overview)** をクリックしてアクセスします。 **OCI** へのログインを要求された場合は、ログインを完了して下さい。

## 5-3. スタック

**スタック** は、 **[リソース・マネージャ](#5-2-リソースマネージャ)** から作成する **OCI** リソースで、作成する **OCI** リソースの構成情報を記述した **[Terraform](#5-12-terraform)** スクリプトと **[スキーマ・ドキュメント](#5-4-スキーマドキュメント)** を元に作成され、 **リソース・マネージャ** が管理する **OCI** リソースの定義情報を持ちます。

作成された **スタック** は、 **リソースマネージャ** から適用・破棄することで、定義した一連のリソースを **OCI** コンソールから1クリックで作成・削除することを可能にします。

**リソース・マネージャ** から **スタック** を作成する代表的な方法は、予め用意した **Terraform** スクリプトと **スキーマ・ドキュメント** をアーカイブ・圧縮した **スタック** ファイルを **リソース・マネージャ** に読み込ませ、メニュー形式の選択肢を自身の要望に沿って選択して作成する方法です。  
この **スタック** ファイルは、様々な環境構築向けのものが **[マーケットプレース](#5-5-マーケットプレイス)** から提供されており、その多くが無償で利用可能です。

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/ResourceManager/Concepts/resourcemanager.htm#concepts)** の **スタック** を参照してください。

**スタック** の作成方法は、チュートリアル **[リソース・マネージャを使ってサンプルアプリケーションをデプロイする](https://oracle-japan.github.io/ocitutorials/intermediates/resource-manager/)** を参照してください。

**OCI** コンソールの **スタック** メニューは、 **[ここ](https://cloud.oracle.com/resourcemanager/stacks)** をクリックしてアクセスします。 **OCI** へのログインを要求された場合は、ログインを完了して下さい。

## 5-4. スキーマ・ドキュメント

**スキーマ・ドキュメント** は、 **[リソース・マネージャ](#5-2-リソースマネージャ)** から **[スタック](#5-3-スタック)** を作成する際、作成するリソースを選択するメニューを定義する、YAML形式のファイルです。  
例えば、作成するインスタンスのシェイプを選択するメニュー、インスタンスにアタッチするブートボリュームのサイズを指定するメニュー、インスタンスにログインする際に使用するSSH鍵を指定するメニュー等を **スキーマ・ドキュメント** で実現することが出来ます。

**スキーマ・ドキュメント** は、 **リソース・マネージャ** から作成する **OCI** リソースの構成情報をコードとして記述した **[Terraform](#5-12-terraform)** スクリプトと共に **スタック** ファイルを構成し、 **リソース・マネージャ** に読み込ませることで利用します。

**スキーマ・ドキュメント** は、 **[マーケットプレース](#5-5-マーケットプレイス)** から提供される様々な環境構築向け **スタック** ファイルに予め含まれているため、通常これをそのまま利用するだけですが、自身の環境向けにカスタマイズした **スタック** ファイルを作成する場合は、その構文を理解する必要があります。

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/ResourceManager/Concepts/terraformconfigresourcemanager_topic-schema.htm#top)** を参照してください。

## 5-5. マーケットプレイス

**マーケットプレイス** は、 **[スタック](#5-3-スタック)** ファイルと **[カスタム・イメージ](#5-6-カスタムイメージ)** を提供する、オラクルが運営するオンライン・ストアです。

**マーケットプレイス** から提供される、様々な環境を自動構築する **スタック** ファイルやアプリケーション等が事前にインストールされた **カスタム・イメージ** は、オラクルはもちろんそのパートナーが提供するものも含まれ、その多くが無料で提供されています。

これら **マーケットプレイス** から提供されるリソースは、これを有効に活用することでHPCシステムを構築する作業工数を大幅に削減することが可能で、**[OCI HPCチュートリアル集](#1-oci-hpcチュートリアル集)** でもこれらを活用する方法が紹介されています。

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Marketplace/Concepts/marketoverview.htm)** を参照してください。

**OCI** コンソールの **マーケットプレイス** メニューは、 **[ここ](https://cloud.oracle.com/marketplace/home)** をクリックしてアクセスします。 **OCI** へのログインを要求された場合は、ログインを完了して下さい。

## 5-6. カスタム・イメージ

**カスタムイメージ** は、実行中のインスタンスから取得するOSのブートイメージで、カスタマイズを適用したインスタンスと同じ状態のインスタンスの複製を可能にします。

HPC/GPUクラスタ構築の際、OSレベルのカスタマイズを加えたインスタンスから取得した **カスタム・イメージ** を活用し、インスタンス作成後のカスタマイズ処理を省略することで、構築時間を短縮することが可能です。

**[クラスタネットワーキングイメージ](#5-13-クラスタネットワーキングイメージ)** は、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** に接続するためのソフトウェア等を含む、 **カスタム・イメージ** として提供されます。

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Compute/Tasks/managingcustomimages.htm)** を参照してください。

**OCI** コンソールの **カスタム・イメージ** メニューは、 **[ここ](https://cloud.oracle.com/compute/images)** をクリックしてアクセスします。 **OCI** へのログインを要求された場合は、ログインを完了して下さい。

## 5-7. インスタンス構成

**インスタンス構成** は、作成するインスタンスの構成情報（ **コンパートメント** 、 **可用性ドメイン** 、シェイプ、イメージ、ブートボリュームサイズ、SSH公開鍵、 **[cloud-init](#5-11-cloud-init)** 構成ファイル等）を持ちます。

**インスタンス構成** は、同じ構成のインスタンスを複製することを可能とし、HPC/GPUクラスタの構築・管理に必須の機能です。

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Compute/Concepts/instancemanagement.htm#config)** を参照してください。

**OCI** コンソールの **インスタンス構成** メニューは、 **[ここ](https://cloud.oracle.com/compute/instance-configs)** をクリックしてアクセスします。 **OCI** へのログインを要求された場合は、ログインを完了して下さい。

## 5-8. インスタンス・プール

**インスタンス・プール** は、同じ構成のインスタンス群を一括作成・スケールアウト・スケールイン・一括削除する機能を持ち、HPCシステムの構築・管理に必須の機能です。

**インスタンス・プール** は、 **[インスタンス構成](#5-7-インスタンス構成)** を指定して作成し、新たなインスタンスをこのインスタンス構成に基づいて作成します。

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Compute/Concepts/instance-pools.htm)** を参照してください。

**OCI** コンソールの **インスタンス・プール** メニューは、 **[ここ](https://cloud.oracle.com/compute/instance-pools)** をクリックしてアクセスします。 **OCI** へのログインを要求された場合は、ログインを完了して下さい。

## 5-9. クラスタオートスケーリング

**クラスタオートスケーリング** は、オラクルが作成・メンテナンスしているオンデマンドクラスタ管理ツールで、 **Slurm** に投入されるジョブに応じて必要なベアメタルインスタンスを **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** と共に作成してワークロードを実行するためのクラスタ環境を動的に構築、ジョブ終了後設定したアイドル時間が経過すると自動的にこのクラスタ環境を削除します。   
このため **クラスタオートスケーリング** は、1本のジョブのために1クラスタを割当てる **Cluster per job** アプローチを採用していると言えます。

**クラスタオートスケーリング** は、 **[HPCクラスタスタック](#5-10-hpcクラスタスタック)** に含まれるPythonとシェルで書かれたスクリプト群で実現され、この **[スタック](#5-3-スタック)** の **Autoscaling** フィールドの **Scheduler based autoscaling** チェックボックスをオンにしてスタックを適用することで、これらスクリプト群がインストールされ利用可能になります。

## 5-10. HPCクラスタスタック

**HPCクラスタスタック** は、HPC/GPUクラスタを自動構築するための、 **[マーケットプレース](#5-5-マーケットプレイス)** から提供される **[スタック](#5-3-スタック)** で、 **スタック** の利用自体は無料です。

この **スタック** は、作成時のメニューを適切に選択することで、以下のクラスタ管理機能を有効にすることが可能です。

- ジョブスケジューラ（ **[Slurm](https://slurm.schedmd.com/)** ）
- オンデマンドクラスタ管理（ **[クラスタオートスケーリング](#5-9-クラスタオートスケーリング)** ）
- クラスタ内ファイル共有（ **ファイル・ストレージ** ）
- クラスタ内ユーザ統合管理（LDAP）

またGPUクラスタでは、選択するメニューにより以下の機能を利用することが可能です。

- コンテナランタイム（ **[Enroot](https://github.com/NVIDIA/enroot/)** ）
- ジョブスケジューラからのコンテナインポート・起動・停止（ **[Pyxis](https://github.com/NVIDIA/pyxis)** ）

このスタックによる自動構築は、大きく2つのステップに分かれており、前半は **[Terraform](#5-12-terraform)** を使用した **OCI** リソース作成フェーズで、後半は **Terraform** から起動される **Ansible** によるOSカスタマイズフェーズです。  
具体的には、使用する機能により以下のような処理が行われます。

［ **Terraform** による **OCI** リソース作成フェーズ］

- **仮想クラウド・ネットワーク** と関連するネットワークリソース作成
- **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** と関連リソース作成
- Bastionノードインスタンス作成
- 計算/GPUノードインスタンス作成
- **ファイル・ストレージ** 作成
- **Ansible** 関連ソフトウェアインストール
- **クラスタオートスケーリング** 関連ソフトウェアインストール

[ **Ansible** によるOSレベルカスタマイズフェーズ]

- firewalld停止
- NVMe SSDローカルディスク領域ファイルシステム作成
- /etc/hostsファイル作成
- NFSファイル共有環境構築
- LDAPユーザ統合環境構築
- **クラスタ・ネットワーク** 用ネットワークインターフェース作成
- **Slurm** 環境構築
- **Enroot** 環境構築

このスタックを利用すると、通常であれば数日かかるようなHPC/GPUクラスタ構築作業を、 **OCI** コンソールのGUIから10項目程度のメニューを選択するだけで実施することが可能になります。

**マーケットプレイス** の **HPCクラスタスタック** ページは、 **[ここ](https://cloud.oracle.com/marketplace/application/67628143/)** をクリックしてアクセスします。 **OCI** へのログインを要求された場合は、ログインを完了して下さい。

## 5-11. cloud-init

**cloud-init** は、主要なクラウドサービスプロバイダーで利用可能なOSカスタマイズのための仕組みで、指定された文法に沿った設定ファイル（cloud-config）やシェルスクリプトをインスタンスや **[インスタンス構成](#5-7-インスタンス構成)** を作成する際に指定することで、インスタンス作成直後に実施する様々なカスタマイズをOSに適用します。

**cloud-init** の設定ファイルは、cloud-configと呼ばれるYAML形式で記述されるテキストファイルで、プラットフォーム間・異なるOS間で共通に使用できる **cloud-init** のモジュールを使用してOSカスタマイズ内容を以下のように記述し、異なるプラットフォーム・OSに跨って可搬性を高めた簡潔な記述を可能にします。

```sh
#cloud-config
# Change time zone to JST
timezone: Asia/Tokyo
runcmd:
# Stop firewalld
  - systemctl stop firewalld
  - systemctl disable firewalld
```

**cloud-init** は、基本的にインスタンス作成後に一度だけ実施されるため、NVMe SSDローカルディスクにファイルシステムを作成する等の、HPC/機械学習ワークロード向けベアメタルインスタンスを作成する際に必須となるようなOS上のカスタマイズを実施するのに最適です。

**cloud-init** は、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** 作成時に指定する **インスタンス構成** と紐づけることで、 **クラスタ・ネットワーク** に接続する全ての計算/GPUノードに一斉にOSカスタマイズを適用することを可能にします。

**cloud-init** 公式ドキュメントは、 **[ここ](https://cloudinit.readthedocs.io/en/latest/)** を参照してください。

## 5-12. Terraform

**Terraform** は、主要なクラウドサービスプロバイダーで利用可能なInfrastructure as code（IaS）のコンセプトに基づくツールで、IaaSからPaaSまでほぼすべてのクラウド上のリソースのライフサイクル（構築・変更・破棄）管理を安全かつ効率的に実施することを可能にします。

**Terraform** で管理されるリソースは、拡張子 **.tf** を持つ **Terraform** スクリプトで定義され、これを **Terraform** が読み込んでライフサイクル管理が行われます。

**OCI** における **Terraform** は、 **[リソース・マネージャ](#5-2-リソースマネージャ)** から利用する方法と、ソフトウェアをインストールして **OCI** との認証関係を結んだ **Terraform** 実行環境から **Terraform** CLIで利用する方法があります。

以下は、 **Terraform** 実行環境で **Terraform** CLIを利用して **OCI** リソースを作成する様子を示しています。

```sh
$ ls -l
total 24
-rw-r--r--. 1 opc opc 3606 May 31 14:51 main.tf
-rw-r--r--. 1 opc opc  272 May 31 14:51 outputs.tf
-rw-r--r--. 1 opc opc  223 May 31 14:51 provider.tf
-rw-r--r--. 1 opc opc 4510 May 31 14:52 terraform.tfvars
-rw-r--r--. 1 opc opc 1701 May 31 14:51 variables.tf
$ terraform init

Initializing the backend...
:
Terraform has been successfully initialized!
$ terraform apply --auto-approve

Terraform used the selected providers to generate the following execution plan. Resource actions are
indicated with the following symbols:
  + create
:
Apply complete! Resources: 9 added, 0 changed, 0 destroyed.
$ terraform destroy --auto-approve
:
Destroy complete! Resources: 9 destroyed.
$
```

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/developer-tutorials/tutorials/tf-provider/01-summary.htm)** を参照してください。

**Terraform** CLIを使用して **OCI** リソースを自動構築する方法は、 **[OCI HPCチュートリアル集](https://oracle-japan.github.io/ocitutorials/)** の **[TerraformでOCIの構築を自動化する](https://oracle-japan.github.io/ocitutorials/intermediates/terraform/)** を参照してください。

## 5-13. クラスタネットワーキングイメージ

**クラスタネットワーキングイメージ** は、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** への接続に必要な以下ソフトウェアが予めインストールされた **Oracle Linux** をベースとする **[カスタム・イメージ](#5-6-カスタムイメージ)** で、 **[マーケットプレイス](#5-5-マーケットプレイス)** から提供されています。

- Mellanox OFED
- wpa_supplicant
- 802.1X認証関連ユーティリティソフトウェア
- **クラスタ・ネットワーク** 設定ユーティリティソフトウェア

802.1X認証関連ユーティリティソフトウェアと **クラスタ・ネットワーク** 設定ユーティリティソフトウェアは、 **[Oracle Cloud Agent](https://docs.oracle.com/ja-jp/iaas/Content/Compute/Tasks/manage-plugins.htm)** のプラグインとしてこれらを提供する **クラスタネットワーキングイメージ** と、個別のRPMパッケージとして提供する **クラスタネットワーキングイメージ** が存在します。  
また、使用するシェイプがGPUを搭載するかどうかにより、HPC **クラスタネットワーキングイメージ** とGPU **クラスタネットワーキングイメージ** が存在します。  
またベースOSの **Oracle Linux** は、バージョン7系と8系の **クラスタネットワーキングイメージ** が存在します。  
これらの組み合わせから自身の用途に合わせて **クラスタネットワーキングイメージ** を適切に選択する方法は、 **[OCI HPCテクニカルTips集](#3-oci-hpcテクニカルtips集)** の **[クラスタネットワーキングイメージの選び方](/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/)** を参照してください。

**マーケットプレイス** の **クラスタネットワーキングイメージ** は、HPC **クラスタネットワーキングイメージ** は **[ここ](https://cloud.oracle.com/marketplace/application/63394796/)** 、GPU **クラスタネットワーキングイメージ** は  **[ここ](https://cloud.oracle.com/marketplace/application/134254210/)** をクリックしてアクセスします。 **OCI** へのログインを要求された場合は、ログインを完了して下さい。

## 5-14. 構成ソース・プロバイダ

**構成ソース・プロバイダ** は、 **GitHub** 等のソースコード管理サービスで公開されているソースコードを **[リソース・マネージャ](#5-2-リソースマネージャ)** に **[スタック](#5-3-スタック)** として取り込むための、ソースコード管理サービスへの接続情報を持つ **OCI** リソースです。

**GitHub** 等で公開されている **[Terraform](#5-12-terraform)** スクリプトを基にHPC/GPUクラスタ等を自動構築する場合、まずこの **構成ソース・プロバイダ** を作成し、これを介して  **Terraform** スクリプトを **GitHub** 等から取り込んで **リソース・マネージャ** に **スタック** を作成、この **スタック** を適用してリソースを作成します。

**[OCI HPCチュートリアル集](#1-oci-hpcチュートリアル集)** で紹介する **Terraform** スクリプトを使用する手法は、ソースコード管理サービスに **GitHub** を使用しますが、 **GitHub** にアクセスするための **構成ソース・プロバイダ** の作成は、 **GitHub** のアカウントを持っておりこのアカウントで **Personal access token** を発行しておく必要があります。  
**GitHub** のアカウント作成は **[ここ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F&source=header-home)** 、 **Personal access token** の発行は **[ここ](https://docs.github.com/ja/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)** を参照してください。

**構成ソース・プロバイダ** の作成手順は、 **OCI** 公式ドキュメントの **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/ResourceManager/Tasks/create-csp-github.htm#top)** を参照してください。  
**OCI HPCチュートリアル集** で使用する **構成ソース・プロバイダ** は、以下で作成します。

- **パブリック・エンドポイント/プライベート・エンドポイント :** パブリック・エンドポイント
- **タイプ :** **GitHub**
- **サーバーURL :** https://github.com/

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/ResourceManager/Tasks/managingconfigurationsourceproviders.htm)** を参照してください。

## 5-15. インスタンス・プリンシパル

**インスタンス・プリンシパル** は、 **IAM** が認証・認可を管理する単位の一つで、**[動的グループ](#5-16-動的グループ)** と **IAMポリシー** を組合せて使用することで、特定のインスタンスから実行するAPIコール（ **OCI** リソースの作成等）に対する認証・認可を制御する際に使用します。  
この **IAM** の認証・認可を管理する単位は、 **インスタンス・プリンシパル** の他に、 **OCI** コンソールにログインする際に使用する **ユーザ・プリンシパル** が存在します。

**インスタンス・プリンシパル** を使用する **IAM** 認証・認可の仕組みは、 **インスタンス・プリンシパル** 認証と呼ばれ、動的にHPC/GPUクラスタのライフサイクルを管理するソリューションに適用することが可能です。  
このソリューションでは、クラスタ管理ノードを **インスタンス・プリンシパル** として **動的グループ** に登録（認証）し、このクラスタ管理ノードから発行するAPIコールを **IAMポリシー** で許可（認可）することで、クラスタ管理ノードからHPC/GPUクラスタのライフサイクルを動的に管理することを可能にします。  
**[OCI HPCチュートリアル集](#1-oci-hpcチュートリアル集)** で紹介している **[クラスタオートスケーリング](#5-9-クラスタオートスケーリング)** を使用するオンデマンドクラスタソリューションは、この仕組みを使用しています。

このオンデマンドクラスタ実現のための **インスタンス・プリンシパル** 認証の設定方法は、 **[OCI HPCテクニカルTips集](#3-oci-hpcテクニカルtips集)** の **[オンデマンドクラスタ実現のためのインスタンス・プリンシパル認証設定方法](/ocitutorials/hpc/tech-knowhow/instance-principal-auth/)** を参照してください。

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Identity/Tasks/callingservicesfrominstances.htm)** を参照してください。

## 5-16. 動的グループ

**動的グループ** は、 **[インスタンス・プリンシパル](#5-15-インスタンスプリンシパル)** をグループ化する仕組みで、 **インスタンス・プリンシパル** による **IAM** 認証・認可を実施するには欠かせません。

**インスタンス・プリンシパル** と **動的グループ** の関係は、ユーザとこれを含むグループの関係に似ており、インスタンス等を含む **動的グループ** を定義し、これに対する **IAMポリシー** を設定することで、 **ユーザ・プリンシパル** 認証を使用することなく、 **動的グループ** に含まれる **OCI** リソースからの各種APIコールを制御することが出来ます。

**動的グループ** に含める **インスタンス・プリンシパル** は、以下のように定義することが可能です。

- 特定のOCIDを持つインスタンスをメンバーとする
- 特定のOCIDを持つ **コンパートメント** に属するインスタンスをメンバーとする
- 特定の **タグ・キー** でタグ付けされたインスタンスをメンバーとする

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Identity/Tasks/managingdynamicgroups.htm)** を参照してください。

**OCI** コンソールの **動的グループ** メニューは、 **[ここ](https://cloud.oracle.com/identity/domains/)** をクリックして表示される **ルートコンパートメント内のドメイン** フィールドで利用している **ドメイン** を選択し、左側の **動的グループ** メニューを選択します。 **OCI** へのログインを要求された場合は、ログインを完了して下さい。

## 5-17. プラットフォーム・イメージ

**プラットフォーム・イメージ** は、インスタンスを作成する際に使用できる **OCI** が提供するOSイメージで、2025年4月時点で以下のものが用意されています。

- **Oracle Linux**
- **Ubuntu**
- **CentOS**
- **Windows**

**プラットフォーム・イメージ** は、各ディストリビューションが提供するOSイメージに **OCI** で利用するための最低限の設定を加えており、インスタンス作成後にアプリケーションのインストールやカスタマイズを加えて自身の要件に沿ったインスタンスを作成する、ベースイメージとして利用します。  
これに対し **[マーケットプレイス](#5-5-マーケットプレイス)** から提供されるイメージは、これを提供するパートナーが予めアプリケーションをインストールして提供しており、自身の要件に沿ったイメージが存在する場合は、インスタンス作成直後に自身のワークロードの実行を開始することが可能です。

関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/en-us/iaas/images/)** を参照してください。
