---
title: "OCI HPCポータル"
excerpt: "OCIを活用してHPC/機械学習ワークロードを実行する際の有益な情報を技術面にフォーカスしてお届けする、OCI HPCポータルです。ベアメタルインスタンス、GPUインスタンス、クラスタ・ネットワーク等のリソースをリソース・マネージャ、Terraform、Ansibleを駆使して効率的に構築するチュートリアル集、ベンチマーク情報、テクニカルTips集、関連情報リンク集等をお届けします。チュートリアルで構築するHPCクラスタは、NFS、LDAP、Slurm、OpenMPI等、システム運用・利用に欠かせないソフトウェアが使えます。またGPUクラスタは、NVIDIA Container Toolkit、DockerやEnrootのコンテナランタイム等、大規模な分散機械学習ワークロード実行に必須のソフトウェアが使えます。"
permalink: /hpc/
layout: single
tags: "hpc"
toc: true
---
<style>
table, th, td {
    font-size: 80%;
}
</style>

このページは、Oracle Cloud Infrastructure（以降OCIと記載）を活用してHPC/機械学習ワークロードを実行する際の有益な情報を技術面にフォーカスしてお届けする、 **OCI HPCポータル** です。

**OCI HPCポータル** は、以下のコンテンツを提供します。

1.  **[OCI HPCチュートリアル集](/ocitutorials/hpc/#1-oci-hpcチュートリアル集)**   
HPC/機械学習ワークロードの実行に最適なベアメタルインスタンス、GPUインスタンス、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** 等の各種IaaSサービスを組み合わせて、様々な用途のHPC/GPUクラスタを構築する手順を解説するチュートリアル集で、自身の要件に合わせて利用するチュートリアルを選択できるようになっています。  
その構築手法は、 **[リソース・マネージャ](#5-2-リソースマネージャ)** 、 **[Terraform](#5-12-terraform)** 、 **Ansible** を駆使する **[マーケットプレース](#5-5-マーケットプレイス)** 提供の **[スタック](#5-3-スタック)** や予め用意された **[Terraform](#5-12-terraform)** スクリプトを使用する自動構築と、OCIのコンソールから各ステップを自身で確認・実行しながら実施する手動構築から選択出来、また構築するHPCシステムは、クラスタ管理機能の有無や利用できるソフトウェアから選択することが可能です。

2. **[OCI HPCパフォーマンス関連情報](#2-oci-hpcパフォーマンス関連情報)**  
HPC/機械学習ワークロードを実行する際のパフォーマンス関連情報を提供します。  
提供するパフォーマンス関連情報は、以下のカテゴリに分かれています。

    - **[標準ベンチマーク実行方法](#2-1-標準ベンチマーク実行方法)**
    - **[パフォーマンス関連Tips集](#2-2-パフォーマンス関連tips集)**
    - **[プロファイリング関連Tips集](#2-3-プロファイリング関連tips集)**

3. **[OCI HPCテクニカルTips集](#3-oci-hpcテクニカルtips集)**  
HPC/機械学習ワークロードを実行する際に有益なテクニカルTipsを集めてます。  
提供するテクニカルTipsは、以下のカテゴリに分かれています。

    - **[クラスタ・ネットワーク](#3-1-クラスタネットワーク)**
    - **[ストレージ](#3-2-ストレージ)**
    - **[クラスタ管理](#3-3-クラスタ管理)**
    - **[機械学習](#3-4-機械学習)**
    - **[ソフトウェア環境](#3-5-ソフトウェア環境)**
    - **[その他](#3-6-その他)**

4. **[OCI HPC関連情報リンク集](#4-oci-hpc関連情報リンク集)**  
HPC/機械学習ワークロードを実行する際に有益なテクニカル情報を提供するウェブサイトの情報を集めています。

5. **[OCI HPC用語集](#5-oci-hpc用語集)**  
本ポータルサイトを読み進めるうえで理解が必要なHPC関連のOCI用語を解説しています。

提供するコンテンツは、随時追加・更新されますので、定期的にチェックしてみて下さい。  
また本ポータルサイト内のコンテンツは、作成者が誠心誠意作成していますが、間違いや不正確な記述などを見つけた場合は、 **[ここ](https://github.com/oracle-japan/ocitutorials/issues)** からIssue登録でご連絡ください。

***
# 1. OCI HPCチュートリアル集

## 1-0. 概要

本章は、様々な用途のHPCシステムを構築するチュートリアルを集めた、 **OCI HPCチュートリアル集** です。

各チュートリアルは、以下4つのカテゴリに分類されます。

- **[HPCクラスタ](#1-1-hpcクラスタ)**

   このカテゴリは、最新のCPUを搭載するベアメタルインスタンスを **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** で高帯域・低遅延に接続する、CPUワークロード向けHPCクラスタを構築するためのチュートリアルを集めています。  
   チュートリアルで構築するHPCクラスタは、NFS、LDAP、 **Slurm** 、 **OpenMPI** 等、システム運用・利用に欠かせないソフトウェアが使えます。

- **[機械学習環境](#1-2-機械学習環境)**

   このカテゴリは、GPUを搭載する単一のVMインスタンスを使用する小規模機械学習環境から、複数のGPUを搭載するベアメタルインスタンスを **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** で高帯域・低遅延に接続する複数ノードに跨る大規模機械学習ワークロードに最適なGPUクラスタまで、機械学習環境を構築するためのチュートリアルを集めています。  
   チュートリアルで構築するGPUクラスタは、 **NVIDIA Container Toolkit** 、 **Docker Community Edition** や **Enroot** のコンテナランタイム等、大規模な分散機械学習ワークロード実行に必須のソフトウェアが使えます。

- **[ファイル共有ストレージ](#1-3-ファイル共有ストレージ)**

   このカテゴリは、HPCクラスタやGPUクラスタを運用する際に必須となるファイル共有ストレージを、OCIのサービスを組み合わせて構築するためのチュートリアルを集めています。

- **[チュートリアルを組み合わせた実践的HPCシステム構築](#1-4-チュートリアルを組み合わせた実践的hpcシステム構築)**

   このカテゴリは、前述3つのカテゴリのチュートリアルを組み合わせることで、より実践的なHPCシステムを構築するためのチュートリアルを集めています。

**[HPCクラスタ](#1-1-hpcクラスタ)** と **[機械学習環境](#1-2-機械学習環境)** のカテゴリは、利用目的や構築手法の異なるチュートリアルを複数用意しており、自身の要件に合わせて以下の観点で使用するチュートリアルを選択します。

- 構築手法

   構築手法は、 **[マーケットプレース](#5-5-マーケットプレイス)** 提供の **[スタック](#5-3-スタック)** を使用する自動構築、予め用意された **[Terraform](#5-12-terraform)** スクリプトを使用する自動構築、及びOCIコンソールから各リソースを順次構築する手動構築に分かれ、それぞれ以下の利点・欠点があります。  
   なお、ここで紹介する自動構築（スタック）に分類されるチュートリアルの多くは、 **[HPCクラスタスタック](#5-10-hpcクラスタスタック)** を活用しています。


    |                     | 利点                                                            | 欠点                                                | 備考                                                                                                              |     |
    | :-----------------: | ------------------------------------------------------------- | ------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- | --- |
    | 自動構築<br>（スタック）      | ユーザの作業時間（※1）が短い<br>GUIによる操作（※2）が可能                            | 構築手順のブラックボックス化<br>　・システム構成の変更が難しい<br>　・問題発生時原因究明難 | ※1）スタックメニュー選択の時間<br>※2）OCIコンソール                                                                                 |     |
    | 自動構築<br>（Terraform） | ユーザの作業時間（※3）が短い<br>CLI/GUI（※4）を選択可能                           | Terraform実行環境（※5）が必要                              | ※3）スタックメニュー選択の時間<br>or<br>Terraformスクリプト内変数修正<br>に要する時間<br>※4）Terraform CLI/OCIコンソール<br>※5）Terraform CLIを選択した場合 |     |
    | 手動構築<br>（OCIコンソール）  | 構築手順が明確<br>　・システム構成の変更が容易<br>　・問題発生時原因究明容易<br>GUIによる操作（※6）が可能 | ユーザの作業時間が長い                                       | ※6）OCIコンソール操作                                                                                                   |     |



- クラスタ管理機能の有無

   構築するHPCクラスタやGPUクラスタは、以下のクラスタ管理機能を利用可能とするか、これらを含まない基礎インフラストラクチャのみを構築するかを選択する事が可能です。

   - ユーザ統合管理（LDAP)
   - ファイル共有ストレージ（NFS）
   - 計算/GPUノードのリソース管理・ジョブ管理（ **Slurm** ） 

- スタティック/オンデマンドクラスタ

   構築するHPCクラスタやGPUクラスタは、ワークロードを実行する際に動的に必要な計算リソースをデプロイするオンデマンドクラスタとして構築するか、ワークロードの有無にかかわらず常に計算リソースを起動しておくスタティッククラスタとして構築するかを選択することが可能です。  
   なおオンデマンドクラスタは、 **[クラスタオートスケーリング](#5-9-クラスタオートスケーリング)** が実現します。

- コンテナランタイム

   **[機械学習環境](#1-2-機械学習環境)** カテゴリに含まれるチュートリアルは、使用するコンテナランタイムを以下から選択することが可能です。
   
   - **Docker Community Edition**
   - **Enroot** with **Pyxis** integrated in **Slurm**

## 1-1. HPCクラスタ

本章は、HPCクラスタを構築するチュートリアルを集めています。自身の要件に合わせてチュートリアルを選んだら、そのチュートリアル名をクリックします。

| チュートリアル名                                                                                      | 構築手法                | クラスタ<br>管理機能 | スタティック/<br>オンデマンド | 計算ノードOS                |
| :-------------------------------------------------------------------------------------------: | :-----------------: | :----------: | :---------------: | :--------------------: |
| **[HPCクラスタを構築する<br>(基礎インフラ手動構築編)](/ocitutorials/hpc/spinup-cluster-network/)**                | 手動構築  | 無し           | スタティック            | **Oracle Linux** 7.9/8.8 |
| **[HPCクラスタを構築する<br>(基礎インフラ自動構築編)](/ocitutorials/hpc/spinup-hpc-cluster-withterraform/)**      | 自動構築<br>（Terraform） | 無し           | スタティック            | **Oracle Linux** 7.9/8.8 |
| **[HPCクラスタを構築する<br>(スタティッククラスタ自動構築編)](/ocitutorials/hpc/spinup-hpc-cluster)**                 | 自動構築<br>（スタック）      | 有り           | スタティック            | **Oracle Linux** 7.9/8.8 |
| **[HPCクラスタを構築する<br>(オンデマンドクラスタ自動構築編)](/ocitutorials/hpc/spinup-hpc-cluster-withautoscaling)** | 自動構築<br>（スタック）      | 有り           | オンデマンド            | **Oracle Linux** 7.9/8.8 |

## 1-2. 機械学習環境

本章は、機械学習環境を構築するチュートリアルを集めています。自身の要件に合わせてチュートリアルを選んだら、そのチュートリアル名をクリックします。

| チュートリアル名                                                                                       | GPUノード数 | 構築手法                | クラスタ<br>管理機能 | スタティック/<br>オンデマンド | コンテナ<br>ランタイム     | GPUノードOS             |
| :--------------------------------------------------------------------------------------------: | :--------: | :-----------------: | :----------: | :---------------: | :-----------: | :------------------: |
| **[GPUインスタンスで<br>機械学習にトライ](/ocitutorials/hpc/spinup-ml-instance/)**                            | 単一         | 手動構築                | 無し           | スタティック            | **Docker CE** | **Oracle Linux** 7.9/8.8 |
| **[GPUクラスタを構築する<br>(基礎インフラ手動構築編)](/ocitutorials/hpc/spinup-gpu-cluster/)**                     | 複数         | 手動構築                | 無し           | スタティック            | **Docker CE** | **Oracle Linux** 7.9/8.8 |
| **[GPUクラスタを構築する<br>(基礎インフラ自動構築編)](/ocitutorials/hpc/spinup-gpu-cluster-withterraform/)**       | 複数         | 自動構築<br>（Terraform） | 無し           | スタティック            | **Docker CE** | **Oracle Linux** 7.9/8.8 |
| **[GPUクラスタを構築する<br>(スタティッククラスタ自動構築編)](/ocitutorials/hpc/spinup-gpu-cluster-withstack/)**       | 複数         | 自動構築<br>（スタック）      | 有り           | スタティック            | **Enroot**    | **Oracle Linux** 7.9/8.8 |
| **[GPUクラスタを構築する<br>(オンデマンドクラスタ自動構築編)](/ocitutorials/hpc/spinup-gpu-cluster-withautoscaling/)** | 複数         | 自動構築<br>（スタック）      | 有り           | オンデマンド            | **Enroot**    | **Oracle Linux** 7.9/8.8 |
| **[GPUクラスタを構築する<br>(Ubuntu OS編)](/ocitutorials/hpc/spinup-gpu-cluster-withubuntu/)**           | 複数         | 手動構築                  | 無し           | スタティック            | -             | **Ubuntu** 20.04     |

## 1-3. ファイル共有ストレージ

本章は、HPCクラスタやGPUクラスタから利用するファイル共有ストレージを構築するチュートリアルを集めています。

現時点は、 **[マーケットプレース](#5-5-マーケットプレイス)** から無料で利用可能な **[スタック](#5-3-スタック)** を利用して **ブロック・ボリューム** とベアメタルインスタンスからNFSでファイル共有ストレージを自動構築するチュートリアル **[ブロック・ボリュームでNFSサーバを構築する](/ocitutorials/hpc/spinup-nfs-server/)** を利用することが可能です。

## 1-4. チュートリアルを組み合わせた実践的HPCシステム構築

本章は、異なるカテゴリのチュートリアルを組み合わせてより実践的なHPCシステムを構築する手法を紹介します。自身の要件に合わせてチュートリアルを選んだら、そのチュートリアル名をクリックします。

| No. | チュートリアル名                                                                                          | 組み合わせるチュートリアル                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 構築するシステム概要                                                              |
| :-: | ------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| 1   | **[ブロック・ボリュームNFSサーバと<br>基礎インフラ編HPC/GPUクラスタを<br>組み合わせる](/ocitutorials/hpc/cluster-with-bv-base/)** | **[ブロック・ボリュームで<br>NFSサーバを構築する](/ocitutorials/hpc/spinup-nfs-server/)**<br><br>**[HPCクラスタを構築する<br>(基礎インフラ手動構築編)](/ocitutorials/hpc/spinup-cluster-network/)**<br>or<br>**[GPUクラスタを構築する<br>(基礎インフラ手動構築編)](/ocitutorials/hpc/spinup-gpu-cluster/)**                                                                                                                                                                                                                             | 基礎インフラ編のHPC/GPUクラスタの<br>ファイル共有ストレージを<br> **ブロック・ボリューム** NFSサーバで<br>サービス |
| 2   | **[ブロック・ボリュームNFSサーバと<br>自動構築編HPC/GPUクラスタを<br>組み合わせる](/ocitutorials/hpc/cluster-with-bv-stack/)**  | **[ブロック・ボリュームで<br>NFSサーバを構築する](/ocitutorials/hpc/spinup-nfs-server/)**<br><br>**[HPCクラスタを構築する<br>(スタティッククラスタ自動構築編)](/ocitutorials/hpc/spinup-hpc-cluster)**<br>or<br>**[HPCクラスタを構築する<br>(オンデマンドクラスタ自動構築編)](/ocitutorials/hpc/spinup-hpc-cluster-withautoscaling)**<br>or<br>**[GPUクラスタを構築する<br>(スタティッククラスタ自動構築編)](/ocitutorials/hpc/spinup-gpu-cluster-withstack/)**<br>or<br>**[GPUクラスタを構築する<br>(オンデマンドクラスタ自動構築編)](/ocitutorials/hpc/spinup-gpu-cluster-withautoscaling/)** | 自動構築編のHPC/GPUクラスタの<br>ファイル共有ストレージを<br> **ブロック・ボリューム** NFSサーバで<br>サービス   |

下表は、各チュートリアルで構築するシステム仕様を示します。

| No. | 構築手法 | クラスタ管理機能 | スタティック/オンデマンド          | コンテナランタイム（※8） |
| :-: | :--: | :------: | :--------------------: | :-----------: |
| 1   | 手動   | 無し（※7）   | スタティック                 | **Docker CE** |
| 2   | 自動   | 有り       | スタティック<br>or<br>オンデマンド | **Enroot**    |

※7）ファイル共有ストレージは、 **ブロック・ボリューム** NFSサーバが提供します。  
※8）GPUクラスタが対象です。

***
# 2. OCI HPCパフォーマンス関連情報

## 2-0. 概要

本章は、HPC/機械学習ワークロードを実行する際のパフォーマンス関連情報を提供します。

提供するコンテンツは、以下のカテゴリに分類されます。

- **[標準ベンチマーク実行方法](#2-1-標準ベンチマーク実行方法)**

   このカテゴリは、OCI上に構築したHPC/GPUクラスタに対して、これらが想定通りの性能となっているかを確認する際に有効な標準ベンチマークの実行方法を解説しています。

- **[パフォーマンス関連Tips集](#2-2-パフォーマンス関連tips集)**

   このカテゴリは、パフォーマンスに影響するパラメータをどのように設定すればよいかといった、パフォーマンス関連Tipsを集めています。

- **[プロファイリング関連Tips集](#2-3-プロファイリング関連tips集)**

   このカテゴリは、HPCアプリケーションのパフォーマンス解析やチューニングに必要な情報を収集する、プロファイリング関連Tipsを集めています。

## 2-1. 標準ベンチマーク実行方法

本章は、HPC/機械学習ワークロードを実行する際に性能の指標となる、OCIの各種IaaSサービスの基礎性能を計測するデファクトスタンダードな以下の標準ベンチマークについて、HPC/機械学習ワークロード向けシェイプや **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** を使用して実行する方法を解説します。

- **HPL**
- **STREAM**
- **Intel MPI Benchmark**
- **NCCL Tests**

各ベンチマークの実行方法は、下表の対象シェイプ部分のリンクをクリックして参照下さい。

| 名称                      | ベンチマークサイトURL                                                                                               | 対象シェイプ                                                                                                                                  |
| :---------------------: | :--------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------: |
| **HPL**                 | **[Link](https://www.netlib.org/benchmark/hpl/)**                                                          | **[BM.Optimized3.36](/ocitutorials/hpc/benchmark/run-hpl/)**<br>**[BM.Standard.E5.192](/ocitutorials/hpc/benchmark/run-hpl-e5/)**       |
| **STREAM**              | **[Link](https://www.cs.virginia.edu/stream/)**                                                            | **[BM.Optimized3.36](/ocitutorials/hpc/benchmark/run-stream/)**<br>**[BM.Standard.E5.192](/ocitutorials/hpc/benchmark/run-stream-e5/)** |
| **Intel MPI Benchmark** | **[Link](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-mpi-benchmarks.html)** | **[BM.Optimized3.36](/ocitutorials/hpc/benchmark/run-imb/)**                                                                            |
| **NCCL Tests**          | **[Link](https://github.com/NVIDIA/nccl-tests)**                                                           | **[BM.GPU.A100-v2.8/BM.GPU4.8](/ocitutorials/hpc/benchmark/run-nccltests/)**                                                            |

## 2-2. パフォーマンス関連Tips集

本章は、HPC/機械学習ワークロードの実行時パフォーマンスに関連する、パフォーマンス関連Tipsを提供します。

- **[パフォーマンスに関連するベアメタルインスタンスのBIOS設定方法](/ocitutorials/hpc/benchmark/bios-setting/)**

   ベアメタルインスタンスは、デプロイする際にBIOSの設定を指定することが可能です。  
   これらの設定は、 **NPS** （NUMA nodes per socket）や **SMT** （Simultanious Multi Threading）といった、当該インスタンスの性能に影響するものが少なくありません。  
   本パフォーマンス関連Tipsは、これらのBIOS設定を指定してHPC/GPUクラスタを構築する方法を解説します。

- **[不要サービス停止によるパフォーマンスチューニング方法](/ocitutorials/hpc/benchmark/stop-unused-service/)**

   計算リソースを極限まで使用するHPCワークロードの実行は、些細な計算リソースを使用するOS常駐サービスがその性能に影響することがあります。  
   特に高並列実行時は、HPCクラスタ内の1ノードでこのようなサービスが稼働していることで、そのスケーラビリティに影響を及ぼします。  
   本パフォーマンス関連Tipsは、OS標準で稼働している常駐サービスの中でリソースを多く消費しているものを特定しこれを停止することで、OSレベルのパフォーマンスチューニングを実施する方法を解説します。

- **[クラスタ・ネットワークのトポロジーを考慮したノード間通信最適化方法](/ocitutorials/hpc/benchmark/topology-aware-cn-tuning/)**

   **[クラスタ・ネットワーク](/ocitutorials/hpc/#5-1-クラスタネットワーク)** は、業界標準のRoCEv2を採用する高帯域・低遅延のRDMA対応インターコネクトネットワークサービスで、そのトポロジーがFat treeのため同一リーフスイッチに接続するノード間とスパインスイッチを介して異なるリーフスイッチに接続するノード間で、ノード間通信のレイテンシが大きく異なります。このため、この特性を意識して適切な計算/GPUノードにジョブを配置することで、レイテンシに影響を受け易いワークロードの性能や高並列実行時のスケーラビリティを改善できる場合があります。  
   本パフォーマンス関連Tipsは、この **クラスタ・ネットワーク** のレイテンシ特性を生かしてマルチノードジョブをクラスタ内に配置することで、ノード間通信性能を最適化する方法を解説します。

## 2-3. プロファイリング関連Tips集

本章は、HPCアプリケーションのパフォーマンス解析やチューニングに必要な情報を収集する、プロファイリング関連Tips集を提供します。

- **[PAPIでHPCアプリケーションをプロファイリング](/ocitutorials/hpc/benchmark/papi-profiling/)**

    HPCワークロードの実行に最適なベアメタル・インスタンスでアプリケーションを実行する場合、高価な計算資源を有効活用出来ているかを検証するため、アプリケーションのプロファイリングを実施することが一般的です。  
    **[PAPI](https://icl.utk.edu/papi/)** は、OCIのHPCワークロード向け **ベアメタル・シェイプ** に採用されている **Intel Ice Lake** や **AMD EPYC 9004シリーズ** のCPUが持つハードウェアカウンタから浮動小数点演算数やキャッシュヒット数といったプロファイリングに有益な情報を取得するAPIを提供し、HPCアプリケーションのプロファイリングに欠かせないツールとなっています。  
    本プロファイリング関連Tipsは、 **ベアメタル・インスタンス** 上で実行するHPCアプリケーションを **PAPI** を使ってプロファイリングする方法を解説します。

***
# 3. OCI HPCテクニカルTips集

## 3-0. 概要

本章は、HPC/機械学習ワークロードを実行する際に有益なテクニカルTipsを集めた、 **OCI HPCテクニカルTips集** です。

各テクニカルTipsは、以下のカテゴリに分類されます。

- **[クラスタ・ネットワーク](#3-1-クラスタネットワーク)**

   このカテゴリは、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** に関連するテクニカルTipsを集めています。

- **[ストレージ](#3-2-ストレージ)**

   このカテゴリは、HPC/GPUクラスタと共に使用するストレージに関連するテクニカルTipsを集めています。

- **[クラスタ管理](#3-3-クラスタ管理)**

   このカテゴリは、多数の計算/GPUノードを効果的に構築・運用するためのクラスタ管理に関するテクニカルTipsを集めています。

- **[機械学習](#3-4-機械学習)**

   このカテゴリは、機械学習ワークロード実行に関するテクニカルTipsを集めています。

- **[ソフトウェア環境](#3-5-ソフトウェア環境)**

   このカテゴリは、HPC/機械学習ワークロードを実行する環境を構築する際に必要となる、プログラム開発環境（コンパイラ、MPI、線形代数演算ライブラリ）やリソース管理・ジョブ管理システム（ジョブスケジューラ、コンテナランタイム）等のソフトウェア環境を構築・整備する際に有益なテクニカルTipsを集めています。

- **[その他](#3-6-その他)**

   このカテゴリは、以上の何れのカテゴリにも属さないテクニカルTipsを集めています。

## 3-1. クラスタ・ネットワーク

- **[クラスタネットワーキングイメージを使ったクラスタ・ネットワーク接続方法](/ocitutorials/hpc/tech-knowhow/howto-connect-clusternetwork/)**

   **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** 対応シェイプを使用するインスタンスは、デプロイ時のイメージに **Oracle Linux** をベースOSとする **[クラスタネットワーキングイメージ](#5-13-クラスタネットワーキングイメージ)** を使用することで、 **クラスタ・ネットワーク** 接続に必要なソフトウェアのインストールやセットアップ等の作業を大幅に簡素化することが可能です。  
   本テクニカルTipsは、この **クラスタ・ネットワーキングイメージ** を使用してインスタンスを **クラスタ・ネットワーク** に接続する方法を解説します。

- **[クラスタネットワーキングイメージの選び方](/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/)**

   **クラスタ・ネットワーク** を使用するインスタンスは、接続に必要なソフトウェアがインストールされている必要がありますが、これらを含んだOSイメージである **[クラスタネットワーキングイメージ](#5-13-クラスタネットワーキングイメージ)** が **[マーケットプレース](#5-5-マーケットプレイス)** から提供されています。  
   本テクニカルTipsは、この **クラスタネットワーキングイメージ** の適切な選び方を解説します。

- **[クラスタ・ネットワーク非対応OSイメージを使ったクラスタ・ネットワーク接続方法](/ocitutorials/hpc/tech-knowhow/howto-create-cnenabled-osimage/)**

   **クラスタ・ネットワーク** に接続するインスタンスは、接続に必要なソフトウェアがインストールされている必要があり、これらを含んだ **[クラスタネットワーキングイメージ](#5-13-クラスタネットワーキングイメージ)** が **[マーケットプレース](#5-5-マーケットプレイス)** から提供されていますが、これらのベースとなるOSは、 **Oracle Linux** のみです。  
   本テクニカルTipsは、 **クラスタ・ネットワーク** 対応シェイプの **[BM.Optimized3.36](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized)** や **[BM.GPU4.8/BM.GPU.A100-v2.8](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-gpu)** を使用するインスタンスを **Oracle Linux** 以外のOSで **クラスタ・ネットワーク** に接続する方法を解説します。

- **[クラスタ・ネットワークに接続する計算/GPUノードデプロイ時の問題判別方法](/ocitutorials/hpc/tech-knowhow/determine-cnrelated-issue/)**

   **クラスタ・ネットワーク** に接続する計算/GPUノードのデプロイは、接続する **クラスタ・ネットワーク** の論理的なパーティション内で利用可能なインスタンスが不足していると、これに失敗します。  
   本テクニカルTipsは、このようなケースで発生するデプロイ時の問題を特定する方法を解説します。

- **[クラスタ・ネットワーク統計情報の取得方法](/ocitutorials/hpc/tech-knowhow/howto-get-cnrelated-statistics/)**

   複数ノードに跨るHPC/機械学習ワークロードを実行するHPC/GPUクラスタは、ノード間通信に使用する **クラスタ・ネットワーク** が想定通りに使用されて初めてその性能を発揮することが出来ます。  
   ここで、インスタンスを **クラスタ・ネットワーク** に接続するNIC（ **NVIDIA Mellanox ConnectX** ）は、これを介して通信する際の様々な統計情報を記録するハードウェアカウンタを備えており、インスタンスのOS上でこれらを取得することが可能です。  
   本テクニカルTipsは、 **クラスタ・ネットワーク** に接続するインスタンスで **クラスタ・ネットワーク** の利用状況や問題判別に役立つ統計情報を取得する方法を解説します。

## 3-2. ストレージ

- **[ベアメタルインスタンスの内蔵NVMe SSD領域ファイルシステム作成方法](/ocitutorials/hpc/tech-knowhow/nvme-filesystem/)**

   高速スクラッチ領域として利用することを想定したNVMe SSDを内蔵するHPCクラスタ向けベアメタルシェイプ **[BM.Optimized3.36](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized)** やGPUクラスタ向けベアメタルシェイプ **[BM.GPU4.8/BM.GPU.GM4.8](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-gpu)** は、NVMe SSDをOSのファイルシステムとして利用するための設定をユーザ自身が行う必要があります。  
   本テクニカルTipsは、このファイルシステム作成方法を解説します。

- **[コストパフォーマンスの良いファイル共有ストレージ構築方法](/ocitutorials/hpc/tech-knowhow/howto-configure-sharedstorage/)**

   HPC/GPUクラスタを運用する際必須となるファイル共有ストレージは、NFSでこれを構築することが一般的ですが、この際の選択肢として、NFSのマネージドサービスである **ファイル・ストレージ** を使用する方法と、 **ブロック・ボリューム** とベアメタルインスタンスを組み合わせたNFSサーバを構築する方法があります。  
   本テクニカルTipsは、コストパフォーマンス等を考慮してどちらの方法を選択すればよいか、 **ブロック・ボリューム** とベアメタルインスタンスを組み合わせたNFSサーバを構築する場合どのように **ブロック・ボリューム** とベアメタルインスタンスを構成すればよいか、解説します。

- **[ブロック・ボリュームを使用するNFSサーバのインスタンス障害からの復旧方法](/ocitutorials/hpc/tech-knowhow/bv-sharedstorage-recovery/)**

   **ブロック・ボリューム** とベアメタルインスタンスを組み合わせたNFSサーバは、HPC/GPUクラスタで必須となるファイル共有ストレージをコストパフォーマンス良く運用するためには最適なソリューションですが、NFSサーバとなるベアメタルインスタンスに障害が発生し起動できなくなった場合、どのようにデータ領域を復旧すればよいでしょうか。  
   本テクニカルTipsは、このシナリオで発生する障害の復旧方法を解説します。

- **[計算/GPUノードのブート・ボリューム動的拡張方法](/ocitutorials/hpc/tech-knowhow/boot-volume-extension/)**

   インスタンスのルートファイルシステムを格納する **ブート・ボリューム** は、OSを停止することなく動的にその容量を拡張することが可能です。  
   ただこの動的拡張は、OCIコンソールやインスタンスOSで複数のオペレーションを実施する必要があり、ノード数が多くなるクラスタ環境の計算/GPUノードでは、これらのオペレーションを効率的に実施することが求められます。  
   本テクニカルTipsは、HPC/GPUクラスタの多数の計算/GPUノードに対し、 **ブート・ボリューム** の動的拡張を効率的に実施する方法を解説します。

## 3-3. クラスタ管理

- **[計算/GPUノードの効果的な名前解決方法](/ocitutorials/hpc/tech-knowhow/compute-name-resolution/)**

   ノード数が多くなるHPCクラスタやGPUクラスタの計算/GPUノードの名前解決は、どのように行うのが効果的でしょうか。  
   本テクニカルTipsは、 **仮想クラウド・ネットワーク** のDNSを使用した効果的な計算/GPUノードの名前解決方法を解説します。

- **[計算/GPUノードデプロイ時の効果的なOSカスタマイズ方法](/ocitutorials/hpc/tech-knowhow/compute-os-customization/)**

   ノード数が多くなるHPCクラスタやGPUクラスタの計算/GPUノードは、デプロイ時に実施するOSカスタマイズをどのように行うのが効果的でしょうか。  
   本テクニカルTipsは、計算/GPUノードデプロイ時のOSカスタマイズ方法の選択肢と、それぞれの利用方法について解説します。

- **[計算/GPUノードのホスト名リスト作成方法](/ocitutorials/hpc/tech-knowhow/compute-host-list/)**

   ノード数が多くなるHPCクラスタやGPUクラスタは、全ての計算/GPUノードのホスト名の一覧を記載したホスト名リストを作成することで、構築・運用作業を効率的に進めることが可能になります。  
   本テクニカルTipsは、HPC/GPUクラスタの計算/GPUノードのホスト名リストを効果的に作成する方法を解説します。

- **[計算/GPUノードの追加・削除・入れ替え方法](/ocitutorials/hpc/tech-knowhow/cluster-resize/)**

   HPC/GPUクラスタは、実行するワークロードの増減に伴い計算/GPUノードのノード数を増減する必要が生じることがあります。またハードウェア障害が発生すると、利用可能なノード数を維持するために当該ノードを別のノードに置き換える必要が生じます。  
   本テクニカルTipsは、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** を使用するHPC/GPUクラスタで計算/GPUノードのノード数を増減する方法や置き換える方法を解説します。

- **[pdshで効率的にクラスタ管理オペレーションを実行](/ocitutorials/hpc/tech-knowhow/cluster-with-pdsh/)**

   ノード数が多くなるHPC/GPUクラスタは、クラスタに含まれるノードに対して様々な管理オペレーションを実施する必要があります。この時、これらのオペレーションを実現するためのコマンドを全てのノードに適用する際、どのような方法が効果的でしょうか。  
   本テクニカルTipsは、 **pdsh** を使用して計算/GPUクラスタの管理オペレーションを効率的に実施する方法を解説します。

- **[オンデマンドクラスタ実現のためのインスタンス・プリンシパル認証設定方法](/ocitutorials/hpc/tech-knowhow/instance-principal-auth/)**

   パブリッククラウドは、ワークロード発生時に必要な規模のHPC/GPUクラスタを構築し、ワークロード終了時にこれを削除する、オンデマンドクラスタ環境を構築するには最適なサービスです。  
   オンデマンドクラスタの管理は、ソフトウェアにより自動化することが一般的ですが、HPC/GPUクラスタに必要なOCIリソースの作成・終了をこのアプリケーションに許可するための仕組みとして、 **[インスタンス・プリンシパル](/ocitutorials/hpc/#5-15-インスタンスプリンシパル)** 認証が用意されています。  
   本テクニカルTipsは、オンデマンドクラスタを念頭とした **インスタンス・プリンシパル** 認証の設定方法を解説します。

- **[OCIロギングとGrafanaを使用したHPC/GPUクラスタのログ監視方法](/ocitutorials/hpc/tech-knowhow/log-monitoring/)**

   ノード数が多くなるHPC/GPUクラスタは、各計算/GPUノードに分散するログを一元的に監視するフレームワークを構築することで、運用管理工数の低減や監視対象ログの見落としを防ぎ、システムセキュリティーを効率的に維持することが可能です。  
   OCI上にこのフレームワークを構築する際、活用できるソフトウェアはいくつかありますが、 **[OCIロギング](https://docs.oracle.com/ja-jp/iaas/Content/Logging/home.htm)** と **[Grafana](https://grafana.com/)** を統合したログ監視は、 **Grafana** の多彩な機能を活用できる点で有力な選択肢です。  
   本テクニカルTipsは、 **OCIロギング** と **Grafana** を使用してHPC/GPUクラスタのログを効率的に監視する方法を解説します。

- **[OCIモニタリングとGrafanaを使用したHPC/GPUクラスタのメトリック監視方法](/ocitutorials/hpc/tech-knowhow/metric-monitoring/)**

   HPCワークロードや機械学習ワークロードを実行するHPC/GPUクラスタは、ワークロード実行中のCPU/GPU使用率、メモリ使用率、ネットワーク使用帯域等のメトリックを定期的に監視し、高価な計算資源を有効活用することが求められますが、ノード数が多くなるHPC/GPUクラスタでは、これらメトリックの監視が一元的・効率的に行える必要があります。  
   OCI上にこのフレームワークを構築する際、活用できるソフトウェアはいくつかありますが、 **[OCIモニタリング](https://docs.oracle.com/ja-jp/iaas/Content/Monitoring/home.htm)** と **[Grafana](https://grafana.com/)** を統合したメトリック監視は、 **Grafana** の多彩な機能を活用できる点で有力な選択肢です。  
   本テクニカルTipsは、 **OCIモニタリング** と **Grafana** を使用してHPC/GPUクラスタのメトリックを効率的に監視する方法を解説します。

## 3-4. 機械学習

- **[UbuntuをOSとする機械学習ワークロード向けGPUノード構築方法](/ocitutorials/hpc/tech-knowhow/gpu-with-ubuntu/)**

   機械学習ワークロード実行のためのGPU搭載ノードは、NVIDIAが提供する様々なGPU関連ソフトウェアの開発が主に **Ubuntu** で行われていることから、そのOSに **Ubuntu** を使用するのが主流になっていますが、 **Ubuntu** をOSに指定してGPU搭載インスタンスをデプロイする場合、GPUを利用するためのソフトウェアを自身でインストール・セットアップする必要があります。  
   本テクニカルTipsは、 **Ubuntu** をGPU搭載インスタンスと共にデプロイした後GPU利用に必要なソフトウェアをインストール・セットアップすることで、機械学習ワークロード向けGPUノードを構築する方法を解説します。

## 3-5. ソフトウェア環境

- **[Slurm環境での利用を前提とするOpenMPI構築方法](/ocitutorials/hpc/tech-knowhow/build-openmpi/)**

   **[OpenMPI](https://www.open-mpi.org/)** は、最新のMPI言語規格に準拠し、HPC/機械学習ワークロード実行に必要とされる様々な機能を備えたオープンソースのMPI実装です。  
   **OpenMPI** で作成したアプリケーションのHPC/GPUクラスタに於ける実行は、計算リソース有効利用の観点から通常ジョブスケジューラを介したバッチジョブとして行いますが、ジョブスケジューラが **[Slurm](https://slurm.schedmd.com/)** の場合、 **[PMIx](https://pmix.github.io/)** を使用することでMPIアプリケーションの起動や通信初期化のスケーラビリティを向上させることが可能です。  
   本テクニカルTipsは、 **PMIx** を使用する **Slurm** 環境での利用を前提とする **OpenMPI** 構築方法を解説します。

- **[Slurmによるリソース管理・ジョブ管理システム構築方法](/ocitutorials/hpc/tech-knowhow/setup-slurm-cluster/)**

   HPC/GPUクラスタのリソース管理・ジョブ管理は、ジョブスケジューラを活用することでこれを効率的かつ柔軟に運用することが可能です。近年のHPC/機械学習ワークロードの大規模化は、MPI等を使ったノード間並列ジョブの重要性を増大させ、このような大規模ジョブを様々な運用ポリシーに沿って処理出来る機能をジョブスケジューラに求めています。オープンソースのジョブスケジューラ **[Slurm](https://slurm.schedmd.com/)** は、この要求を満足出来る代表的なジョブスケジューラとして現在人気を集めています。  
   本テクニカルTipsは、HPC/機械学習ワークロードの実行に最適なベアメタルインスタンスを高帯域・低遅延RDMAインターコネクトサービスの **[クラスタ・ネットワーク](/ocitutorials/hpc/#5-1-クラスタネットワーク)** で接続するHPC/GPUクラスタで、リソース管理・ジョブ管理システムを **Slurm** で構築する方法を解説します。

- **[線形代数演算ライブラリインストール・利用方法](/ocitutorials/hpc/tech-knowhow/install-blas/)**

   HPCワークロードを実行する際、行列やベクトルの線形代数演算を高速に実行する必要が生じます。  
   これらの演算は、ソースコードを自作することで対応することも出来ますが、オープンソースで配布されている線形代数演算ライブラリである **[BLAS](https://www.netlib.org/blas/)** や **[OpenBLAS](https://github.com/OpenMathLib/OpenBLAS/wiki)** を利用することで、開発工数の削減、保証された計算精度、高速な演算の実行等、様々なメリットを享受することが可能です。  
   本テクニカルTipsは、BLASとOpenBLASをHPCワークロードの実行に最適なベアメタルインスタンスにインストールし、Fortranのサンプルコードからこれを利用する方法を解説します。

## 3-6. その他

- **[ベアメタル・インスタンスのカーネルダンプ取得方法](/ocitutorials/hpc/tech-knowhow/kdump-on-baremetal/)**

   カーネルダンプは、Linuxカーネルに関連する問題を解析する際に重要な情報ですが、ルートファイルシステムを含む **ブート・ボリューム** をiSCSI接続する **ベアメタル・インスタンス** は、 **仮想マシン・インスタンス** と比較してその取得に特別な準備が必要です。  
   本テクニカルTipsは、 **ベアメタル・インスタンス** でカーネルダンプを取得し、これをcrashコマンドで読み込むまでの手順を解説します。

- **[サイト間VPNによるOCIとの拠点間接続方法](/ocitutorials/hpc/tech-knowhow/site-to-site-vpn/)**

   HPC/機械学習ワークロードをオンプレミスからOCIに移行する際、多くのケースでオンプレミスとOCIを拠点間接続する必要が生じます。  
   例えば、オンプレミス側の有償CAEアプリケーション用ライセンスサーバにOCI側の計算ノードからライセンス取得のためにアクセスするケースや、オンプレミス側の端末からOCI側のログインノードにログインしインタラクティブ処理を行うケースです。  
   本テクニカルTipsは、 **[サイト間VPN](https://docs.oracle.com/ja-jp/iaas/Content/Network/Tasks/managingIPsec.htm)** を使用してIPSecのトンネルモードでオンプレミスとOCIを拠点間接続し、OCI側プライベートサブネットに接続する計算ノード相当のインスタンスとオンプレミス側プライベートサブネットに接続するライセンスサーバ相当のインスタンスの疎通を可能とするための手順を解説します。

***
# 4. OCI HPC関連情報リンク集

## 4-0. 概要

本章は、HPC/機械学習ワークロードを実行する際の有益な情報を提供するウェブサイトの情報を集めた、 **OCI HPC関連情報リンク集** です。

各ウェブサイトは、以下のカテゴリに分類されます。

- **[クラスタ・ネットワーク](#4-1-クラスタネットワーク)**

   このカテゴリは、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** に関連する情報を提供するウェブサイトのリンク集です。

- **[ストレージ](#4-2-ストレージ)**

   このカテゴリは、HPC/GPUクラスタと共に使用するストレージに関連する情報を提供するウェブサイトのリンク集です。

## 4-1. クラスタ・ネットワーク

- **[First Principles: Building a high-performance network in the public cloud](https://blogs.oracle.com/cloud-infrastructure/post/building-high-performance-network-in-the-cloud)**

   **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** は、データリンク層にイーサネット、トランスポート層にIBトランスポートを使用し、マイクロ秒レベルの低レイテンシでRDMA通信を実現しますが、パケットロスの発生を前提とするイーサネット上（Lossy network）でTCP/IPのような再送制御を持たないIBトランスポートを使用し、どのようにパケットロスにセンシティブなRDMA通信を可能にしているのでしょうか。  
   本ブログとそこからリンクされている同タイトルのYouTubeビデオは、輻輳制御の仕組みである **DCQCN（Data Center Quantitized Congention Notification）** を使用して **クラスタ・ネットワーク** がこの問題に対処している点、実行するワークロードタイプに応じて **DCQCN** の特性をレイテンシと帯域幅の観点で最適化している点を解説しています。

- **[Congestion Control for Large-Scale RDMA Deployments](https://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p523.pdf)**

   **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** は、輻輳制御の仕組みである **DCQCN（Data Center Quantitized Congention Notification）** と、イーサネットでロスレスネットワークを実現する仕組みである **PFC（Priority Flow Control）** を使用して、高帯域・低レイテンシのRDMA通信をRoCEv2上に実現します。  
   2015年のSIGCOMMカンファレンスで発表された本論文は、 **クラスタ・ネットワーク** で採用している **DCQCN** と **PFC** の組み合わせで実現する輻輳制御を初めて提案しました。  
   ここで提案する輻輳制御は、 **PFC** のもつ輻輳伝搬の問題を新たに考案した **DCQCN** を組合せることで解決し、RoCEv2ファブリックの帯域幅・レイテンシ・通信安定性を向上させますが、本論文の中ではこれらをシミュレーションや実験の結果から示しています。

## 4-2. ストレージ

- **[The best choice for NFS file service running on Oracle Cloud Infrastructure](https://blogs.oracle.com/cloud-infrastructure/post/best-choice-for-nfs-file-service-running-on-oracle-cloud-infrastructure)**

   HPC/GPUクラスタを運用する際必須となるファイル共有ストレージは、 **ブロック・ボリューム** とベアメタルインスタンスの **[BM.Optimized3.36](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized)** を組み合わせてNFSでサービスすることで、コストパフォーマンスを最大化することが可能です。  
   その理由は、このシェイプが50 GbpsのTCP/IP接続用ポートを2個搭載し、それぞれをiSCSI接続の **ブロック・ボリューム** アクセス用途とNFSクライアントへのNFSサービス用途に割当ててこれらを同時に使用することで、NFSサービスに50 Gbpsの帯域をフルに使用することが可能となるためです。  
   本ブログは、 **ブロック・ボリューム** と **BM.Optimized3.36** を組み合わせたNFSサーバに対し、IORとmdtestを使用したIO性能と信頼性の評価を行い、その結果を解説しています。

***
# 5. OCI HPC用語集

本章は、OCIのHPCに関連する用語を解説する用語集です。  
本ポータルサイトを読み進める際、不明なOCI用語の理解に役立ててください。

## 5-1. クラスタ・ネットワーク

**クラスタ・ネットワーク** は、ノード間を高帯域・低遅延に接続するインターコネクトネットワークサービスで、以下の特徴を持ちます。

- RoCEv2を採用するリンク当たり100 Gbpsの高帯域・低遅延RDMAインターコネクト
- オーバーサブスクリプションの無いノンブロッキングトポロジーから来る高い性能再現性

**クラスタ・ネットワーク** の作成は、予め作成した **[インスタンス構成](#5-7-インスタンス構成)** と **クラスタ・ネットワーク** に接続するインスタンス数を指定して行い、この **インスタンス構成** に紐づく **[インスタンス・プール](#5-8-インスタンスプール)** が自動的に作成され、この **インスタンス・プール** が **クラスタ・ネットワーク** に接続するインスタンスを **インスタンス構成** に従ってデプロイします。  
OCIリソースとしての **クラスタ・ネットワーク** は、共にデプロイされるインスタンスを高帯域・低遅延で接続する、 **仮想クラウド・ネットワーク** とは独立したノード間インターコネクトとして機能します。

**クラスタ・ネットワーク** に接続するインスタンスは、 **[ここ](https://docs.public.oneportal.content.oci.oraclecloud.com/ja-jp/iaas/Content/Compute/Tasks/managingclusternetworks.htm#supported-shapes)** に記載の **クラスタ・ネットワーク** 対応シェイプで、以下のソフトウェアがインストールされている必要があります。

   - Mellanox OFED
   - wpa_supplicant
   - 802.1X認証関連ユーティリティソフトウェア
   - **クラスタ・ネットワーク** 設定ユーティリティソフトウェア

これらの **クラスタ・ネットワーク** 接続に必要なソフトウェアは、自身でインストールすることも出来ますが、これらが予めインストールされた **[クラスタネットワーキングイメージ](/ocitutorials/hpc/#5-13-クラスタネットワーキングイメージ)** も用意されています。

**クラスタ・ネットワーク** と共に作成するインスタンスは、デプロイ後にOS上で以下の処理を完了することで、 **クラスタ・ネットワーク** に接続可能になります。

- **クラスタ・ネットワーク** との802.1X認証
- **クラスタ・ネットワーク** 接続用ネットワークインターフェースへのIPアドレス付与

これらの処理は、前述の802.1X認証関連ユーティリティソフトウェアと **クラスタ・ネットワーク** 設定ユーティリティソフトウェアで行います。  
この手順は、 **[OCI HPCテクニカルTips集](/ocitutorials/hpc/#3-oci-hpcテクニカルtips集)** の **[クラスタネットワーキングイメージを使ったクラスタ・ネットワーク接続方法](/ocitutorials/hpc/tech-knowhow/howto-connect-clusternetwork/)** を参照下さい。

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Compute/Tasks/managingclusternetworks.htm)** を参照下さい。

OCIコンソールの **クラスタ・ネットワーク** メニューは、 **[ここ](https://cloud.oracle.com/compute/cluster-networks)** をクリックしてアクセスします。OCIへのログインを要求された場合は、ログインを完了して下さい。

## 5-2. リソース・マネージャ

**リソース・マネージャ** は、OCIのリソース管理（作成・変更・削除）の自動化をInfrastructure-as-code （IaC）のコンセプトで実現する、OCIコンソールからGUIで操作するマネージド **[Terraform](#5-12-terraform)** サービスで、OCIの様々なサービスを組み合わせた複雑なシステムの構築を自動化することで、システム構築や運用に必要な工数を大幅に削減します。

**リソース・マネージャ** は、様々なOCIサービスを組み合わせたシステム構築作業を、メニュー形式で予めオプションを選択して作成された **[スタック](#5-3-スタック)** をOCIコンソールから適用・破棄することで、1クリックで実施することを可能にします。

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/ResourceManager/Concepts/resourcemanager.htm)** を参照下さい。

**リソース・マネージャ** の利用方法は、チュートリアル **[リソース・マネージャを使ってサンプルアプリケーションをデプロイする](https://oracle-japan.github.io/ocitutorials/intermediates/resource-manager/)** を参照下さい。

OCIコンソールの **リソース・マネージャ** メニューは、 **[ここ](https://cloud.oracle.com/resourcemanager/overview)** をクリックしてアクセスします。OCIへのログインを要求された場合は、ログインを完了して下さい。

## 5-3. スタック

**スタック** は、 **[リソース・マネージャ](#5-2-リソースマネージャ)** から作成するOCIリソースで、デプロイするOCIリソースの構成情報を記述した **[Terraform](#5-12-terraform)** スクリプトと **[スキーマ・ドキュメント](#5-4-スキーマドキュメント)** を元に作成され、 **リソース・マネージャ** が管理するOCIリソースの定義情報を持ちます。

作成された **スタック** は、 **リソースマネージャ** から適用・破棄することで、定義したリソースをOCIコンソールから1クリックでデプロイ・削除することを可能にします。

**リソース・マネージャ** から **スタック** を作成する代表的な方法は、予め用意した **Terraform** スクリプトと **スキーマ・ドキュメント** をアーカイブ・圧縮した **スタック** ファイルを **リソース・マネージャ** に読み込ませ、メニュー形式の選択肢を自身の要望に沿って選択して作成する方法です。  
この **スタック** ファイルは、様々な環境構築向けのものが **[マーケットプレース](#5-5-マーケットプレイス)** から提供されており、その多くが無償で利用可能です。

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/ResourceManager/Concepts/resourcemanager.htm#concepts)** の **スタック** を参照下さい。

**スタック** の作成方法は、チュートリアル **[リソース・マネージャを使ってサンプルアプリケーションをデプロイする](https://oracle-japan.github.io/ocitutorials/intermediates/resource-manager/)** を参照下さい。

OCIコンソールの **スタック** メニューは、 **[ここ](https://cloud.oracle.com/resourcemanager/stacks)** をクリックしてアクセスします。OCIへのログインを要求された場合は、ログインを完了して下さい。

## 5-4. スキーマ・ドキュメント

**スキーマ・ドキュメント** は、 **[リソース・マネージャ](#5-2-リソースマネージャ)** から **[スタック](#5-3-スタック)** を作成する際、デプロイするシステム構成情報を選択するメニューを定義する、YAML形式のファイルです。例えば、デプロイするインスタンスのシェイプを選択するメニュー、インスタンスにアタッチするブートボリュームのサイズを指定するメニュー、インスタンスにログインするSSH公開鍵を指定するメニューを、適切な **スキーマ・ドキュメント** を作成することで実現出来ます。

**スキーマ・ドキュメント** は、 **リソース・マネージャ** からデプロイするOCIリソースの構成情報をコードとして記述した **[Terraform](#5-12-terraform)** スクリプトと共に **スタック** ファイルを構成し、 **リソース・マネージャ** に読み込ませることで利用します。

**スキーマ・ドキュメント** は、 **[マーケットプレース](#5-5-マーケットプレイス)** から提供される様々な環境構築向け **スタック** ファイルに予め含まれているため、通常これをそのまま利用するだけですが、自身の環境向けにカスタマイズした **スタック** ファイルを作成する場合は、その構文を理解する必要があります。

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/ResourceManager/Concepts/terraformconfigresourcemanager_topic-schema.htm#top)** を参照下さい。

## 5-5. マーケットプレイス

**マーケットプレイス** は、 **[スタック](#5-3-スタック)** ファイルと **[カスタム・イメージ](#5-6-カスタムイメージ)** を提供する、オラクルが運営するオンライン・ストアです。

**マーケットプレイス** から提供される **スタック** ファイルや **カスタム・イメージ** は、オラクルはもちろんそのパートナーが提供するものも含まれ、その多くが無料で提供されます。これら **マーケットプレイス** から提供されるリソースを活用することで、OCI上にHPCシステムを構築する作業工数を大幅に削減することが可能で、**[OCI HPCチュートリアル集](#1-oci-hpcチュートリアル集)** でもこれらを活用する方法が紹介されています。

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Marketplace/Concepts/marketoverview.htm)** を参照下さい。

OCIコンソールの **マーケットプレイス** メニューは、 **[ここ](https://cloud.oracle.com/marketplace/home)** をクリックしてアクセスします。OCIへのログインを要求された場合は、ログインを完了して下さい。

## 5-6. カスタム・イメージ

**カスタムイメージ** は、実行中のインスタンスから取得するOSのブートイメージで、カスタマイズを適用したインスタンスと同じ状態のインスタンスの複製を可能にします。

HPCクラスタやGPUクラスタ構築の際、OSレベルのカスタマイズを加えたインスタンスから取得した **カスタム・イメージ** を活用し、デプロイ後のカスタマイズ処理を省略することで、構築時間を短縮することが可能です。

**[クラスタネットワーキングイメージ](#5-13-クラスタネットワーキングイメージ)** は、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** に接続するためのソフトウェア等を含む、 **カスタム・イメージ** として提供されます。

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Compute/Tasks/managingcustomimages.htm)** を参照下さい。

OCIコンソールの **カスタム・イメージ** メニューは、 **[ここ](https://cloud.oracle.com/compute/images)** をクリックしてアクセスします。OCIへのログインを要求された場合は、ログインを完了して下さい。

## 5-7. インスタンス構成

**インスタンス構成** は、デプロイするインスタンスの構成情報（ **コンパートメント** 、 **可用性ドメイン** 、シェイプ、イメージ、ブートボリュームサイズ、SSH公開鍵、 **[cloud-init](/ocitutorials/hpc/#5-11-cloud-init)** 構成ファイル等）を持ちます。

**インスタンス構成** は、同じ構成のインスタンスを複製することを可能とし、HPCシステムの構築・管理に必須の機能です。

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Compute/Concepts/instancemanagement.htm#config)** を参照下さい。

OCIコンソールの **インスタンス構成** メニューは、 **[ここ](https://cloud.oracle.com/compute/instance-configs)** をクリックしてアクセスします。OCIへのログインを要求された場合は、ログインを完了して下さい。

## 5-8. インスタンス・プール

**インスタンス・プール** は、同じイメージのインスタンス群を一括デプロイ・スケールアウト・スケールイン・一括削除する機能を持ち、HPCシステムの構築・管理に必須の機能です。

**インスタンス・プール** は、 **[インスタンス構成](#5-7-インスタンス構成)** を指定して作成し、新たなインスタンスをこのインスタンス構成に基づいてデプロイします。

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Compute/Concepts/instancemanagement.htm#Instance)** を参照下さい。

OCIコンソールの **インスタンス・プール** メニューは、 **[ここ](https://cloud.oracle.com/compute/instance-pools)** をクリックしてアクセスします。OCIへのログインを要求された場合は、ログインを完了して下さい。

## 5-9. クラスタオートスケーリング

**クラスタオートスケーリング** は、オラクルが作成・メンテナンスしているオンデマンドクラスタ管理ツールで、 **Slurm** に投入されるジョブに応じて必要なベアメタルインスタンスを **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** と共にデプロイしてワークロードを実行するためのクラスタ環境を動的に構築、ジョブ終了後設定したアイドル時間が経過すると自動的にこのクラスタ環境を削除します。   
このため **クラスタオートスケーリング** は、1本のジョブのために1クラスタを割当てる **Cluster per job** アプローチを採用していると言えます。

**クラスタオートスケーリング** は、 **[HPCクラスタスタック](#5-10-hpcクラスタスタック)** に含まれるPythonとシェルで書かれたスクリプト群で実現され、この **[スタック](#5-3-スタック)** の **Autoscaling** フィールドの **Scheduler based autoscaling** チェックボックスをオンにしてスタックを適用することで、これらスクリプト群がインストールされ利用可能になります。

## 5-10. HPCクラスタスタック

**HPCクラスタスタック** は、CPUワークロード向けHPCクラスタや分散機械学習ワークロード向けGPUクラスタを自動構築するための、 **[マーケットプレース](#5-5-マーケットプレイス)** から提供される **[スタック](#5-3-スタック)** で、 **スタック** の利用自体は無料です。

この **スタック** は、作成時のメニューを適切に選択することで、以下のクラスタ管理機能を有効にすることが可能です。

- ジョブスケジューラ（ **Slurm** ）
- オンデマンドクラスタ管理（ **[クラスタオートスケーリング](#5-9-クラスタオートスケーリング)** ）
- クラスタ内ファイル共有（ **ファイル・ストレージ** ）
- クラスタ内ユーザ統合管理（LDAP）

またGPUクラスタでは、選択するメニューにより以下の機能を利用することが可能です。

- コンテナランタイム（ **Enroot** ）
- ジョブスケジューラからのコンテナインポート・起動・停止（ **Pyxis** ）

またこのスタックは、構築を大きく2つのステップに分けて実行しており、前半は **[Terraform](#5-12-terraform)** を使用したOCIレベルのリソース構築フェーズで、後半は **[Terraform](/ocitutorials/hpc/#5-12-terraform)** から起動される **Ansible** によるOSレベルのカスタマイズフェーズです。  
具体的には、使用する機能により以下のような処理が行われます。

［ **Terraform** によるOCIリソース構築フェーズ］

- **仮想クラウド・ネットワーク** と関連するネットワークリソース構築
- **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** と関連リソース構築
- Bastionノードインスタンス構築
- 計算/GPUノードインスタンス構築
- **ファイル・ストレージ** 構築
- **Ansible** 関連ソフトウェアインストール
- **クラスタオートスケーリング** 関連ソフトウェアインストール

[ **Ansible** によるOSレベルカスタマイズフェーズ]

- firewalld停止
- NVMe SSD領域ファイルシステム構築
- /etc/hostsファイル生成
- NFSファイル共有環境構築
- LDAPユーザ統合環境構築
- **クラスタ・ネットワーク** 用ネットワークインターフェース構築
- **Slurm** 環境構築
- **Enroot** 環境構築

このスタックを利用すると、通常であれば数日かかるようなクラスタ構築作業を、OCIコンソールのGUIから10項目程度のメニューを選択するだけで実施することが可能になります。

**マーケットプレイス** の **HPCクラスタスタック** ページは、 **[ここ](https://cloud.oracle.com/marketplace/application/67628143/)** をクリックしてアクセスします。OCIへのログインを要求された場合は、ログインを完了して下さい。

## 5-11. cloud-init

**cloud-init** は、主要なクラウドサービスプロバイダーで利用可能なOSカスタマイズのための仕組みで、指定された文法に沿った設定ファイル（cloud-config）やシェルスクリプトをインスタンスや **[インスタンス構成](#5-7-インスタンス構成)** を作成する際に指定することで、インスタンスデプロイ直後に実施する様々なカスタマイズをOSに適用します。

**cloud-init** の設定ファイルは、cloud-configと呼ばれるYAML形式で記述されるテキストファイルで、プラットフォーム間・異なるOS間で共通に使用できる **cloud-init** のモジュールを使用してOSカスタマイズ内容を以下のように記述し、異なるプラットフォーム・OSに跨って可搬性を高めた簡潔な記述を可能にします。

```sh
#cloud-config
# Change time zone to JST
timezone: Asia/Tokyo
runcmd:
# Stop firewalld
  - systemctl stop firewalld
  - systemctl disable firewalld
```

**cloud-init** によるOSカスタマイズは、基本的にインスタンスデプロイ後に一度だけ実施されるため、HPC/機械学習ワークロード向けベアメタルインスタンスをデプロイする際に必須となるNVMeローカルディスク領域ファイルシステム作成や、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** 用ネットワークインターフェース設定を実施するのに最適です。

**cloud-init** は、 **クラスタ・ネットワーク** 作成時に指定する **インスタンス構成** と紐づけることで、 **クラスタ・ネットワーク** に接続する全ての計算ノードやGPUノードに一斉にOSカスタマイズを適用することを可能にします。

**cloud-init** 公式ドキュメントは、 **[ここ](https://cloudinit.readthedocs.io/en/latest/)** を参照下さい。

## 5-12. Terraform

**Terraform** は、主要なクラウドサービスプロバイダーで利用可能なIaS（Infrastructure as code）のコンセプトに基づくツールで、IaaSからPaaSまでほぼすべてのクラウド上のリソースのライフサイクル（構築・変更・破棄）管理を安全かつ効率的に実施することを可能にします。  
**Terraform** で管理されるリソースは、拡張子 **.tf** を持つ **Terraform** スクリプトで定義され、これを **Terraform** が読み込んでライフサイクル管理が行われます。

OCIにおける **Terraform** は、 **[リソース・マネージャ](#5-2-リソースマネージャ)** から利用する方法と、ソフトウェアをインストールしてOCIとの認証関係を結んだ **Terraform** 実行環境から **Terraform** CLIで利用する方法があります。

以下は、 **Terraform** 実行環境で **Terraform** CLIを利用してOCI上にリソースをデプロイする様子を示しています。

```sh
$ ls -l
total 24
-rw-r--r--. 1 opc opc 3606 May 31 14:51 main.tf
-rw-r--r--. 1 opc opc  272 May 31 14:51 outputs.tf
-rw-r--r--. 1 opc opc  223 May 31 14:51 provider.tf
-rw-r--r--. 1 opc opc 4510 May 31 14:52 terraform.tfvars
-rw-r--r--. 1 opc opc 1701 May 31 14:51 variables.tf
$ terraform init

Initializing the backend...
:
Terraform has been successfully initialized!
$ terraform apply --auto-approve

Terraform used the selected providers to generate the following execution plan. Resource actions are
indicated with the following symbols:
  + create
:
Apply complete! Resources: 9 added, 0 changed, 0 destroyed.
$ terraform destroy --auto-approve
:
Destroy complete! Resources: 9 destroyed.
$
```

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/developer-tutorials/tutorials/tf-provider/01-summary.htm)** を参照下さい。

**Terraform** CLIを使用してOCI上にリソースを自動構築する方法は、 **[OCI チュートリアル](https://oracle-japan.github.io/ocitutorials/)** の **[TerraformでOCIの構築を自動化する](https://oracle-japan.github.io/ocitutorials/intermediates/terraform/)** を参照下さい。

## 5-13. クラスタネットワーキングイメージ

**クラスタネットワーキングイメージ** は、 **[クラスタ・ネットワーク](#5-1-クラスタネットワーク)** への接続に必要な以下ソフトウェアが予めインストールされた **Oracle Linux** をベースとする **[カスタム・イメージ](#5-6-カスタムイメージ)** で、 **[マーケットプレイス](#5-5-マーケットプレイス)** から提供されています。

- Mellanox OFED
- wpa_supplicant
- 802.1X認証関連ユーティリティソフトウェア
- **クラスタ・ネットワーク** 設定ユーティリティソフトウェア

802.1X認証関連ユーティリティソフトウェアと **クラスタ・ネットワーク** 設定ユーティリティソフトウェアは、 **[Oracle Cloud Agent](https://docs.oracle.com/ja-jp/iaas/Content/Compute/Tasks/manage-plugins.htm)** のプラグインとしてこれらを提供する **クラスタネットワーキングイメージ** と、個別のRPMパッケージとして提供する **クラスタネットワーキングイメージ** が存在します。  
また対応するシェイプは、GPUを搭載するかどうかでHPC **クラスタネットワーキングイメージ** とGPU **クラスタネットワーキングイメージ** が存在します。  
またベースOSの **Oracle Linux** は、バージョン7系と8系の **クラスタネットワーキングイメージ** が存在します。  
これらの組み合わせから自身の用途に合わせて **クラスタネットワーキングイメージ** を適切に選択する方法は、 **[OCI HPCテクニカルTips集](/ocitutorials/hpc/#3-oci-hpcテクニカルtips集)** の **[クラスタネットワーキングイメージの選び方](/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/)** を参照下さい。

**マーケットプレイス** の **クラスタネットワーキングイメージ** は、HPC **クラスタネットワーキングイメージ** は **[ここ](https://cloud.oracle.com/marketplace/application/63394796/)** 、GPU **クラスタネットワーキングイメージ** は  **[ここ](https://cloud.oracle.com/marketplace/application/134254210/)** をクリックしてアクセスします。OCIへのログインを要求された場合は、ログインを完了して下さい。

## 5-14. 構成ソース・プロバイダ

**構成ソース・プロバイダ** は、 **GitHub** 等のソースコード管理サービスで公開されているソースコードを **[リソース・マネージャ](/ocitutorials/hpc/#5-2-リソースマネージャ)** に **[スタック](/ocitutorials/hpc/#5-3-スタック)** として取り込むための、ソースコード管理サービスへの接続情報を持つOCIリソースです。

**GitHub** 等で公開されている **[Terraform](/ocitutorials/hpc/#5-12-terraform)** スクリプトを基にOCI上にHPC/GPUクラスタを構築する場合、まずこの **構成ソース・プロバイダ** を作成し、これを介して  **Terraform** スクリプトを **GitHub** 等から取り込んで **リソース・マネージャ** に **スタック** を作成、この **スタック** を適用してデプロイします。

**[OCI HPCチュートリアル集](#1-oci-hpcチュートリアル集)** で紹介する **Terraform** スクリプトを使用する手法は、ソースコード管理サービスに **GitHub** を使用しますが、 **GitHub** にアクセスするための **構成ソース・プロバイダ** の作成は、 **GitHub** のアカウントを持っておりこのアカウントで **Personal access token** を発行しておく必要があります。  
**GitHub** のアカウント作成は **[ここ](https://github.com/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F&source=header-home)** 、 **Personal access token** の発行は **[ここ](https://docs.github.com/ja/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)** を参照下さい。

**構成ソース・プロバイダ** の作成手順は、OCI公式ドキュメントの **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/ResourceManager/Tasks/create-csp-github.htm#top)** を参照下さい。  
**OCI HPCチュートリアル集** で使用する **構成ソース・プロバイダ** は、以下で作成します。

- **パブリック・エンドポイント/プライベート・エンドポイント :** パブリック・エンドポイント
- **タイプ :** **GitHub**
- **サーバーURL :** https://github.com/

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/ResourceManager/Tasks/managingconfigurationsourceproviders.htm)** を参照下さい。

## 5-15. インスタンス・プリンシパル

**インスタンス・プリンシパル** は、 **IAM** が認証・認可を管理する単位の一つで、**[動的グループ](/ocitutorials/hpc/#5-16-動的グループ)** と **IAMポリシー** を組合せて使用することで、特定のインスタンスから実行するOCIサービスへのAPIコール（OCIリソースのデプロイ等）に対する認証・認可を制御する際に使用します。  
この **IAM** の認証・認可を管理する単位は、 **インスタンス・プリンシパル** の他に、OCIコンソールにログインする際に使用する **ユーザ・プリンシパル** が存在します。

**インスタンス・プリンシパル** を使用する **IAM** 認証・認可の仕組みは、 **インスタンス・プリンシパル** 認証と呼ばれ、動的にHPC/GPUクラスタのライフサイクルを管理するソリューションに適用することが可能です。  
このソリューションでは、クラスタ管理ノードを **インスタンス・プリンシパル** として **動的グループ** に登録（認証）し、このクラスタ管理ノードから発行するAPIコールを **IAMポリシー** で許可（認可）することで、クラスタ管理ノードからHPC/GPUクラスタのライフサイクルを動的に管理することを可能にします。  
**[OCI HPCチュートリアル集](#1-oci-hpcチュートリアル集)** で紹介している **[クラスタオートスケーリング](/ocitutorials/hpc/#5-9-クラスタオートスケーリング)** を使用するオンデマンドクラスタソリューションは、この仕組みを使用しています。

このオンデマンドクラスタ実現のための **インスタンス・プリンシパル** 認証の設定方法は、 **[OCI HPCテクニカルTips集](#3-oci-hpcテクニカルtips集)** の **[オンデマンドクラスタ実現のためのインスタンス・プリンシパル認証設定方法](/ocitutorials/hpc/tech-knowhow/instance-principal-auth/)** を参照下さい。

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Identity/Tasks/callingservicesfrominstances.htm)** を参照下さい。

## 5-16. 動的グループ

**動的グループ** は、 **[インスタンス・プリンシパル](/ocitutorials/hpc/#5-15-インスタンスプリンシパル)** をグループ化する仕組みで、 **インスタンス・プリンシパル** による **IAM** 認証・認可を実施するには欠かせません。

**インスタンス・プリンシパル** と **動的グループ** の関係は、ユーザとこれを含むグループの関係に似ており、インスタンス等を含む **動的グループ** を定義し、これに対する **IAMポリシー** を設定することで、 **ユーザ・プリンシパル** 認証を使用することなく、 **動的グループ** に含まれるOCIリソースによる各種OCIサービスへのAPIコールを制御することが出来ます。

**動的グループ** に含める **インスタンス・プリンシパル** は、以下のように定義することが可能です。

- 特定のOCIDを持つインスタンスをメンバーとする
- 特定のOCIDを持つ **コンパートメント** に属するインスタンスをメンバーとする
- 特定の **タグ・キー** でタグ付けされたインスタンスをメンバーとする

関連するOCI公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/Identity/Tasks/managingdynamicgroups.htm)** を参照下さい。

OCIコンソールの **動的グループ** メニューは、 **[ここ](https://cloud.oracle.com/identity/domains/)** をクリックして表示される **ルートコンパートメント内のドメイン** フィールドで利用している **ドメイン** を選択し、左側の **動的グループ** メニューを選択します。OCIへのログインを要求された場合は、ログインを完了して下さい。
