<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ja" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Slurmによるリソース管理・ジョブ管理システム構築方法 | Oracle Cloud Infrastructure チュートリアル</title>
<meta name="description" content="HPC/GPUクラスタのリソース管理・ジョブ管理は、ジョブスケジューラを活用することでこれを効率的かつ柔軟に運用することが可能です。近年のHPC/機械学習ワークロードの大規模化は、MPI等を使ったノード間並列ジョブの重要性を増大させ、このような大規模ジョブを様々な運用ポリシーに沿って処理出来る機能をジョブスケジューラに求めています。オープンソースのジョブスケジューラSlurmは、この要求を満足出来る代表的なジョブスケジューラとして現在人気を集めています。本テクニカルTipsは、HPC/機械学習ワークロードの実行に最適なベアメタルインスタンスを高帯域・低遅延RDMAインターコネクトサービスのクラスタ・ネットワークで接続するHPC/GPUクラスタで、リソース管理・ジョブ管理システムをSlurmで構築する方法を解説します。">


  <meta name="author" content="Oracle Japan Solution Engineers">
  
  <meta property="article:author" content="Oracle Japan Solution Engineers">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ja_JP">
<meta property="og:site_name" content="Oracle Cloud Infrastructure チュートリアル">
<meta property="og:title" content="Slurmによるリソース管理・ジョブ管理システム構築方法">
<meta property="og:url" content="https://oracle-japan.github.io/ocitutorials/hpc/tech-knowhow/setup-slurm-cluster/">


  <meta property="og:description" content="HPC/GPUクラスタのリソース管理・ジョブ管理は、ジョブスケジューラを活用することでこれを効率的かつ柔軟に運用することが可能です。近年のHPC/機械学習ワークロードの大規模化は、MPI等を使ったノード間並列ジョブの重要性を増大させ、このような大規模ジョブを様々な運用ポリシーに沿って処理出来る機能をジョブスケジューラに求めています。オープンソースのジョブスケジューラSlurmは、この要求を満足出来る代表的なジョブスケジューラとして現在人気を集めています。本テクニカルTipsは、HPC/機械学習ワークロードの実行に最適なベアメタルインスタンスを高帯域・低遅延RDMAインターコネクトサービスのクラスタ・ネットワークで接続するHPC/GPUクラスタで、リソース管理・ジョブ管理システムをSlurmで構築する方法を解説します。">



  <meta property="og:image" content="https://oracle-japan.github.io/ocitutorials/hpc/tech-knowhow/setup-slurm-cluster/architecture_diagram.png">





  <meta property="article:published_time" content="2024-02-16T13:01:50+09:00">






<link rel="canonical" href="https://oracle-japan.github.io/ocitutorials/hpc/tech-knowhow/setup-slurm-cluster/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "https://oracle-japan.github.io/ocitutorials/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/ocitutorials/feed.xml" type="application/atom+xml" rel="alternate" title="Oracle Cloud Infrastructure チュートリアル Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/ocitutorials/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/ocitutorials/"><img src="/ocitutorials/assets/images/social-og-oracle-badge.jpg" alt="OCI チュートリアル"></a>
        
        <a class="site-title" href="/ocitutorials/">
          OCI チュートリアル
          <span class="site-subtitle">Oracle Cloud Infrastructure を使ってみよう</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/ocitutorials/#%E3%83%81%E3%83%A5%E3%83%BC%E3%83%88%E3%83%AA%E3%82%A2%E3%83%AB%E3%82%B3%E3%83%B3%E3%83%86%E3%83%B3%E3%83%84%E4%B8%80%E8%A6%A7">チュートリアル一覧</a>
            </li><li class="masthead__menu-item">
              <a href="/ocitutorials/about/">このサイトについて</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">メニュー</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style=" background-image: linear-gradient(rgba(34, 66, 55, 0.7), rgba(34, 66, 55, 0.7)), url('/ocitutorials/hpc/tech-knowhow/setup-slurm-cluster/architecture_diagram.png');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Slurmによるリソース管理・ジョブ管理システム構築方法

        
      </h1>
      
        <p class="page__lead">HPC/GPUクラスタのリソース管理・ジョブ管理は、ジョブスケジューラを活用することでこれを効率的かつ柔軟に運用することが可能です。近年のHPC/機械学習ワークロードの大規模化は、MPI等を使ったノード間並列ジョブの重要性を増大させ、このような大規模ジョブを様々な運用ポリシーに沿って処理出来る機能をジョブスケジューラに求めています。オープンソースのジョブスケジューラSlurmは、この要求を満足出来る代表的なジョブスケジューラとして現在人気を集めています。本テクニカルTipsは、HPC/機械学習ワークロードの実行に最適なベアメタルインスタンスを高帯域・低遅延RDMAインターコネクトサービスのクラスタ・ネットワークで接続するHPC/GPUクラスタで、リソース管理・ジョブ管理システムをSlurmで構築する方法を解説します。
</p>
      
      


      
      
    </div>
  
  
</div>




  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="https://oracle-japan.github.io/ocitutorials/" itemprop="item"><span itemprop="name">ホーム</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">></span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/hpc" itemprop="item"><span itemprop="name">Hpc</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">></span>
      
    
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/tech-knowhow" itemprop="item"><span itemprop="name">Tech knowhow</span></a>
          <meta itemprop="position" content="3" />
        </li>
        <span class="sep">></span>
      
    
      
      
        <li class="current">Slurmによるリソース管理・ジョブ管理システム構築方法</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">メニュー</label>
  <ul class="nav__items">
    <li>
      
      <a href=""><span class="nav__sub-title">HPC編</span></a>
      <ul>
        
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-cluster-network/">HPCクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster-withterraform/">HPCクラスタを構築する(基礎インフラ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster/">HPCクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster-withautoscaling/">HPCクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-ml-instance/">GPUインスタンスで機械学習にトライ</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster/">GPUクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withterraform/">GPUクラスタを構築する(基礎インフラ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withstack/">GPUクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withautoscaling/">GPUクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withubuntu/">GPUクラスタを構築する(Ubuntu OS編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server/">ブロック・ボリュームでNFSサーバを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-base/">ブロック・ボリュームNFSサーバと基礎インフラ編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-stack/">ブロック・ボリュームNFSサーバと自動構築編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl/">HPL実行方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl-e5/">HPL実行方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream/">STREAM実行方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream-e5/">STREAM実行方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-imb/">Intel MPI Benchmark実行方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-nccltests/">NCCL Tests実行方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/bios-setting/">パフォーマンスに関連するベアメタルインスタンスのBIOS設定方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/stop-unused-service/">不要サービス停止によるパフォーマンスチューニング方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/topology-aware-cn-tuning/">クラスタ・ネットワークのトポロジーを考慮したノード間通信最適化方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-connect-clusternetwork/">クラスタネットワーキングイメージを使ったクラスタ・ネットワーク接続方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/rdma-interface-configure/">クラスタ・ネットワーク接続用ネットワークインターフェース作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/">クラスタネットワーキングイメージの選び方</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-create-cnenabled-osimage/">クラスタ・ネットワーク非対応OSイメージを使ったクラスタ・ネットワーク接続方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/determine-cnrelated-issue/">クラスタ・ネットワークに接続する計算/GPUノードデプロイ時の問題判別方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-get-cnrelated-statistics/">クラスタ・ネットワーク統計情報の取得方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/nvme-filesystem/">ベアメタルインスタンスの内蔵NVMe SSD領域ファイルシステム作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-configure-sharedstorage/">コストパフォーマンスの良いファイル共有ストレージ構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/bv-sharedstorage-recovery/">ブロック・ボリュームを使用するNFSサーバのインスタンス障害からの復旧方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/boot-volume-extension/">計算/GPUノードのブート・ボリューム動的拡張方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-name-resolution/">計算/GPUノードの効果的な名前解決方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-os-customization/">計算/GPUノードデプロイ時の効果的なOSカスタマイズ方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-host-list/">計算/GPUノードのホスト名リスト作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/cluster-resize/">計算/GPUノードの追加・削除・入れ替え方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/cluster-with-pdsh/">pdshで効率的にクラスタ管理オペレーションを実行</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/instance-principal-auth/">オンデマンドクラスタ実現のためのインスタンス・プリンシパル認証設定方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/log-monitoring/">OCIロギングとGrafanaを使用したHPC/GPUクラスタのログ監視方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/metric-monitoring/">OCIモニタリングとGrafanaを使用したHPC/GPUクラスタのメトリック監視方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/gpu-with-ubuntu/">UbuntuをOSとする機械学習ワークロード向けGPUノード構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするOpenMPI構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/setup-slurm-cluster/" class="active">Slurmによるリソース管理・ジョブ管理システム構築方法</a></li></p>
        
      </ul>
    </li>
  </ul>
</nav>
    
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Slurmによるリソース管理・ジョブ管理システム構築方法">
    <meta itemprop="description" content="HPC/GPUクラスタのリソース管理・ジョブ管理は、ジョブスケジューラを活用することでこれを効率的かつ柔軟に運用することが可能です。近年のHPC/機械学習ワークロードの大規模化は、MPI等を使ったノード間並列ジョブの重要性を増大させ、このような大規模ジョブを様々な運用ポリシーに沿って処理出来る機能をジョブスケジューラに求めています。オープンソースのジョブスケジューラSlurmは、この要求を満足出来る代表的なジョブスケジューラとして現在人気を集めています。本テクニカルTipsは、HPC/機械学習ワークロードの実行に最適なベアメタルインスタンスを高帯域・低遅延RDMAインターコネクトサービスのクラスタ・ネットワークで接続するHPC/GPUクラスタで、リソース管理・ジョブ管理システムをSlurmで構築する方法を解説します。">
    <meta itemprop="datePublished" content="2024-02-16T13:01:50+09:00">
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> 目次</h4></header>
              <ul class="toc__menu"><li><a href="#0-前提システム">0. 前提システム</a><ul><li><a href="#0-1-サブシステム構成">0-1. サブシステム構成</a></li><li><a href="#0-2-セキュリティーポリシー">0-2. セキュリティーポリシー</a></li></ul></li><li><a href="#1-slurm環境構築">1. Slurm環境構築</a><ul><li><a href="#1-0-概要">1-0. 概要</a></li><li><a href="#1-1-munge-インストールセットアップ">1-1. munge インストール・セットアップ</a></li><li><a href="#1-2-mariadb-インストールセットアップ">1-2. MariaDB インストール・セットアップ</a></li><li><a href="#1-3-openpmixインストール">1-3. OpenPMIxインストール</a></li><li><a href="#1-4-ucxインストール">1-4. UCXインストール</a></li><li><a href="#1-5-openmpiインストール">1-5. OpenMPIインストール</a></li><li><a href="#1-6-slurm-rpmパッケージ作成">1-6. Slurm rpmパッケージ作成</a></li><li><a href="#1-7-slurm-rpmパッケージインストールセットアップ">1-7. Slurm rpmパッケージインストール・セットアップ</a></li><li><a href="#1-8-slurm設定ファイル作成">1-8. Slurm設定ファイル作成</a></li><li><a href="#1-9-slurmサービス起動確認">1-9. Slurmサービス起動・確認</a></li></ul></li><li><a href="#2-slurm稼働確認">2. Slurm稼働確認</a></li></ul>

            </nav>
          </aside>
        
        <style>
table, th, td {
    font-size: 80%;
}
</style>

<p>HPC/GPUクラスタのリソース管理・ジョブ管理は、ジョブスケジューラを活用することでこれを効率的かつ柔軟に運用することが可能です。近年のHPC/機械学習ワークロードの大規模化は、MPI等を使ったノード間並列ジョブの重要性を増大させ、このような大規模ジョブを様々な運用ポリシーに沿って処理出来る機能をジョブスケジューラに求めています。オープンソースのジョブスケジューラ <strong><a href="https://slurm.schedmd.com/">Slurm</a></strong> は、この要求を満足出来る代表的なジョブスケジューラとして現在人気を集めています。<br />
本テクニカルTipsは、HPC/機械学習ワークロードの実行に最適なベアメタルインスタンスを高帯域・低遅延RDMAインターコネクトサービスの <strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong> で接続するHPC/GPUクラスタで、リソース管理・ジョブ管理システムを <strong>Slurm</strong> で構築する手順を解説します。</p>

<p>本テクニカルTipsは、次章で説明する前提システムが予めデプロイされており、このシステム上に <strong>Slurm</strong> 環境を構築する手順にフォーカスします。<br />
また <strong>Slurm</strong> のバージョンは、 <strong>23.11.0</strong> を前提とします。</p>

<hr />
<h1 id="0-前提システム">0. 前提システム</h1>

<p>本章は、本テクニカルTipsで解説する <strong>Slurm</strong> 環境構築手順の前提となるシステムを解説します。</p>

<h2 id="0-1-サブシステム構成">0-1. サブシステム構成</h2>

<p>本システムは、以下4種類のサブシステムから構成されます。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">サブシステム</th>
      <th style="text-align: center">使用するシェイプ</th>
      <th style="text-align: center">OS</th>
      <th style="text-align: center">ノード数</th>
      <th style="text-align: center">接続<br />サブネット</th>
      <th>役割</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Slurm<br />マネージャ</td>
      <td style="text-align: center">任意の仮想マシン<br />（※1）</td>
      <td style="text-align: center">Oracle Linux 8</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">プライベート</td>
      <td><strong>slurmctld</strong> と <strong>slurmdbd</strong> が稼働するSlurm管理ノード</td>
    </tr>
    <tr>
      <td style="text-align: center">Slurm<br />クライアント</td>
      <td style="text-align: center">任意の仮想マシン<br />（※1）</td>
      <td style="text-align: center">Oracle Linux 8</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">パブリック</td>
      <td>インターネットからログインするBastionノード<br /><strong>Slurm</strong> にジョブを投入するジョブサブミッションクライアント</td>
    </tr>
    <tr>
      <td style="text-align: center">計算ノード</td>
      <td style="text-align: center"><strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong><br />対応ベアメタルシェイプ<br />（※2）</td>
      <td style="text-align: center">Oracle Linux 8</td>
      <td style="text-align: center">2ノード以上</td>
      <td style="text-align: center">プライベート<br /> <strong>クラスタ・ネットワーク</strong></td>
      <td><strong>slurmd</strong> が稼働するジョブ実行ノード</td>
    </tr>
    <tr>
      <td style="text-align: center">NFSサーバ</td>
      <td style="text-align: center">-<br />（※3）</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">プライベート</td>
      <td>ジョブ投入ユーザのホームディレクトリをNFSでサービス（※4）</td>
    </tr>
  </tbody>
</table>

<p><img src="architecture_diagram.png" alt="画面ショット" /></p>

<p>※1）本テクニカルTipsは、 <strong>VM.Optimized3.Flex</strong> を使用します。<br />
※2）本テクニカルTipsは、 <strong><a href="https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized">BM.Optimized3.36</a></strong> を使用します。<br />
※3）<strong>ファイル・ストレージ</strong> や <strong>ブロック・ボリューム</strong> NFSサーバ等、任意の手法で構築されたNFSサーバです。NFSでサービスするファイル共有ストレージ構築方法は、 <strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/howto-configure-sharedstorage/">コストパフォーマンスの良いファイル共有ストレージ構築方法</a></strong> を参照ください。<br />
※4）NFSサーバがサービスするジョブ投入ユーザのホームディレクトリは、Slurmクライアントと計算ノードがNFSクライアントとなり、 <strong>/mnt/nfs/home/user_name</strong> でマウントします。</p>

<p>また、本テクニカルTipsの各サブシステムのホスト名は、以下とします。<br />
以降の章では、これらのホスト名を自身の環境に置き換えて使用して下さい。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">サブシステム</th>
      <th style="text-align: center">ホスト名</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Slurmマネージャ</td>
      <td style="text-align: center">slurm-srv</td>
    </tr>
    <tr>
      <td style="text-align: center">Slurmクライアント</td>
      <td style="text-align: center">slurm-cli</td>
    </tr>
    <tr>
      <td style="text-align: center">計算ノード</td>
      <td style="text-align: center">inst-qiuim-x9<br />inst-wxedu-x9</td>
    </tr>
  </tbody>
</table>

<h2 id="0-2-セキュリティーポリシー">0-2. セキュリティーポリシー</h2>

<p>各サブシステムのセキュリティポリシーは、接続するサブネットに応じて以下のように設定します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">接続するサブネット</th>
      <th style="text-align: center">firewalld</th>
      <th style="text-align: center">SElinux</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">パブリック</td>
      <td style="text-align: center">仮想クラウド・ネットワークの<br />CIDRからのアクセスを全て許可</td>
      <td style="text-align: center">NFS領域をホームディレクトリとして許可（※5）</td>
    </tr>
    <tr>
      <td style="text-align: center">プライベート</td>
      <td style="text-align: center">停止</td>
      <td style="text-align: center">無効化</td>
    </tr>
  </tbody>
</table>

<p>※5）以下コマンドを対象となるノードのopcユーザで実行します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>setsebool <span class="nt">-P</span> use_nfs_home_dirs on
</code></pre></div></div>

<p>また、接続するサブネットの <strong>セキュリティー・リスト</strong> も上記セキュリティーポリシーに合わせて必要な設定を適用します。</p>

<hr />
<h1 id="1-slurm環境構築">1. Slurm環境構築</h1>

<h2 id="1-0-概要">1-0. 概要</h2>
<p>本章は、既にデプロイされている <strong><a href="#0-前提システム">前提システム</a></strong> で解説したシステム上で、 <strong>Slurm</strong> 環境を構築する手順を解説します。</p>

<p><strong>Slurm</strong> のインストールは、多数の計算ノードに効率よくインストールする必要から、rpmbuildで作成するrpmパッケージによるインストール方法を採用します。<br />
また、 <strong>Slurm</strong> のプロセス間通信の認証に <strong><a href="https://dun.github.io/munge/">munge</a></strong> 、ジョブのアカウンティング情報格納用RDBMSに <strong><a href="https://mariadb.org/">MariaDB</a></strong> を使用します。<br />
また、ノードを跨ぐワークロード実行にMPIを想定し、 <strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするOpenMPI構築方法</a></strong> に従い、 <strong><a href="https://openpmix.github.io/">OpenPMIx</a></strong> を利用する <strong><a href="https://www.open-mpi.org/">OpenMPI</a></strong> をインストールします。</p>

<p>以上より、本章で解説する <strong>Slurm</strong> 環境構築は、以下の手順に沿って行います。</p>

<ol>
  <li><strong>munge</strong> インストール・セットアップ</li>
  <li><strong>MariaDB</strong> インストール・セットアップ</li>
  <li><strong>OpenPMIx</strong> インストール</li>
  <li><strong>UCX</strong> インストール</li>
  <li><strong>OpenMPI</strong> インストール</li>
  <li><strong>Slurm</strong> rpmパッケージ作成</li>
  <li><strong>Slurm</strong> rpmパッケージインストール・セットアップ</li>
  <li><strong>Slurm</strong> 設定ファイル作成</li>
  <li><strong>Slurm</strong> サービス起動・確認</li>
</ol>

<p>なお、 <strong>munge</strong> 、 <strong>MariaDB</strong> 及び各 <strong>Slurm</strong> サービスは、以下のサブシステムにインストールします。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">Slurmマネージャ</th>
      <th style="text-align: center">Slurmクライアント</th>
      <th style="text-align: center">計算ノード</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>munge</strong></td>
      <td style="text-align: center">〇</td>
      <td style="text-align: center">〇</td>
      <td style="text-align: center">〇</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>MariaDB</strong></td>
      <td style="text-align: center">〇</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>slurmctld</strong></td>
      <td style="text-align: center">〇</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>slurmdbd</strong></td>
      <td style="text-align: center">〇</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>slurmd</strong></td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">〇</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Slurm</strong><br />クライアントパッケージ</td>
      <td style="text-align: center">〇</td>
      <td style="text-align: center">〇</td>
      <td style="text-align: center">〇</td>
    </tr>
  </tbody>
</table>

<h2 id="1-1-munge-インストールセットアップ">1-1. munge インストール・セットアップ</h2>

<p>本章は、Slurmマネージャ、Slurmクライアント、及び計算ノードに <strong>munge</strong> をインストール・セットアップします。</p>

<ol>
  <li>
    <p>以下コマンドを対象となる全ノードのopcユーザで実行し、 <strong>munge</strong> プロセス起動ユーザを作成します。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo useradd -m -d /var/lib/munge -s /sbin/nologin -u 5001 munge
</code></pre></div>    </div>
  </li>
  <li>
    <p>以下コマンドを対象となる全ノードのopcユーザで実行し、 <strong>munge</strong> をインストールします。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>dnf <span class="nb">install</span> <span class="nt">-y</span> munge munge-libs
<span class="nv">$ </span><span class="nb">cd</span> ~<span class="p">;</span> wget https://rpmfind.net/linux/centos/8-stream/PowerTools/x86_64/os/Packages/munge-devel-0.5.13-2.el8.x86_64.rpm
<span class="nv">$ </span><span class="nb">sudo </span>rpm <span class="nt">-ivh</span> ./munge-devel-0.5.13-2.el8.x86_64.rpm
</code></pre></div>    </div>
  </li>
  <li>
    <p>以下コマンドをSlurmマネージャのopcユーザで実行し、 <strong>munge</strong> キーファイル（munge.key）を作成します。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo</span> /usr/sbin/create-munge-key
Generating a pseudo-random key using /dev/urandom completed.
<span class="nv">$ </span><span class="nb">sudo ls</span> <span class="nt">-la</span> /etc/munge
total 16
drwx------.   2 munge munge   23 Nov 24 14:34 <span class="nb">.</span>
drwxr-xr-x. 115 root  root  8192 Nov 24 14:33 ..
<span class="nt">-r--------</span><span class="nb">.</span>   1 munge munge 1024 Nov 24 14:34 munge.key
<span class="nv">$ </span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>先にSlurmマネージャで作成した <strong>munge</strong> キーファイルを、Slurmクライアントと計算ノードに同一パス・ファイル名でコピーします。<br />
この際、ファイルのオーナーとパーミッションがSlurmマネージャのキーファイルと同じとなるよう配慮します。</p>
  </li>
  <li>
    <p>以下コマンドを対象となる全ノードのopcユーザで実行し、 <strong>munge</strong> サービスを起動します。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> munge.service
</code></pre></div>    </div>
  </li>
  <li>
    <p>以下コマンドを対象となる全ノードのopcユーザで実行し、 <strong>munge</strong> が全てのノードで正常に動作していることを確認します。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>munge <span class="nt">-n</span> | unmunge | <span class="nb">grep </span>STATUS
STATUS:           Success <span class="o">(</span>0<span class="o">)</span>
<span class="err">$</span>
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="1-2-mariadb-インストールセットアップ">1-2. MariaDB インストール・セットアップ</h2>

<p>本章は、Slurmマネージャに <strong>MariaDB</strong> をインストール・セットアップします。</p>

<ol>
  <li>
    <p>以下コマンドをSlurmマネージャのopcユーザで実行し、 <strong>MariaDB</strong> をインストールします。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo dnf install -y mariadb-server mariadb-devel
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>MariaDB</strong> の設定ファイル（mariadb-server.cnf）の[mysqld]フィールドに以下の記述を追加します。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>diff /etc/my.cnf.d/mariadb-server.cnf_org /etc/my.cnf.d/mariadb-server.cnf
20a21,22
<span class="o">&gt;</span> <span class="nv">innodb_buffer_pool_size</span><span class="o">=</span>4096M
<span class="o">&gt;</span> <span class="nv">innodb_lock_wait_timeout</span><span class="o">=</span>900
<span class="err">$</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>以下コマンドをSlurmマネージャのopcユーザで実行し、 <strong>MariaDB</strong> サービスを起動します。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> mariadb
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong>MariaDB</strong> のデータベースに以下の登録を行うため、</p>

    <ul>
      <li>データベース（slurm_acct_db）</li>
      <li>ユーザ（slurm）</li>
      <li>ユーザ（slurm）のパスワード</li>
      <li>ユーザ（slurm）に対するデータベース（slurm_acct_db）への全権限付与</li>
    </ul>

    <p>以下コマンドをSlurmマネージャのopcユーザで実行します。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>mysql
MariaDB <span class="o">[(</span>none<span class="o">)]&gt;</span> create database slurm_acct_db<span class="p">;</span>
MariaDB <span class="o">[(</span>none<span class="o">)]&gt;</span> create user <span class="s1">'slurm'</span>@<span class="s1">'localhost'</span> identified by <span class="s1">'SLURM'</span><span class="p">;</span>
MariaDB <span class="o">[(</span>none<span class="o">)]&gt;</span> <span class="nb">set </span>password <span class="k">for </span>slurm@localhost <span class="o">=</span> password<span class="o">(</span><span class="s1">'passcord'</span><span class="o">)</span><span class="p">;</span>
MariaDB <span class="o">[(</span>none<span class="o">)]&gt;</span> grant all on slurm_acct_db.<span class="k">*</span> TO <span class="s1">'slurm'</span>@<span class="s1">'localhost'</span><span class="p">;</span>
MariaDB <span class="o">[(</span>none<span class="o">)]&gt;</span> FLUSH PRIVILEGES<span class="p">;</span>
MariaDB <span class="o">[(</span>none<span class="o">)]&gt;</span> Ctrl-c
<span class="err">$</span>
</code></pre></div>    </div>

    <p>なお、コマンド中の <strong>passcord</strong> は、自身の設定するパスワードに置き換えます。</p>
  </li>
  <li>
    <p>以下コマンドをSlurmマネージャのopcユーザで実行し、先に登録したデータベースとユーザが正しく登録されていることを確認します。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>mysql <span class="nt">--user</span><span class="o">=</span>slurm <span class="nt">--password</span><span class="o">=</span>passcord slurm_acct_db <span class="nt">-e</span> <span class="s1">'show databases;'</span>
+--------------------+
| Database           |
+--------------------+
| information_schema |
| slurm_acct_db      |
+--------------------+
<span class="err">$</span>
</code></pre></div>    </div>

    <p>なお、コマンド中の <strong>passcord</strong> は、自身の設定したパスワードに置き換えます。</p>
  </li>
</ol>

<h2 id="1-3-openpmixインストール">1-3. OpenPMIxインストール</h2>

<p>本章は、Slurmマネージャ、Slurmクライアント、及び計算ノードに <strong>OpenPMIx</strong> をインストールします。</p>

<p>具体的には、 <strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするOpenMPI構築方法</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/#1-openpmixインストール">1. OpenPMIxインストール</a></strong> を実施します。</p>

<h2 id="1-4-ucxインストール">1-4. UCXインストール</h2>

<p>本章は、Slurmマネージャと計算ノードに <strong>UCX</strong> をインストールします。</p>

<ol>
  <li>
    <p>以下コマンドを対象となる全ノードのopcユーザで実行し、 <strong>UCX</strong> をインストールします。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>dnf <span class="nb">install</span> <span class="nt">-y</span> gcc-c++ gcc-gfortran
<span class="nv">$ </span><span class="nb">cd</span> ~<span class="p">;</span> wget https://github.com/openucx/ucx/releases/download/v1.15.0/ucx-1.15.0.tar.gz
<span class="nv">$ </span><span class="nb">tar</span> <span class="nt">-xvf</span> ./ucx-1.15.0.tar.gz
<span class="nv">$ </span><span class="nb">cd </span>ucx-1.15.0<span class="p">;</span> ./contrib/configure-release <span class="nt">--prefix</span><span class="o">=</span>/opt/ucx
<span class="nv">$ </span>make
<span class="nv">$ </span><span class="nb">sudo </span>make <span class="nb">install</span>
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="1-5-openmpiインストール">1-5. OpenMPIインストール</h2>

<p>本章は、Slurmクライアントと計算ノードに <strong>OpenMPI</strong> をインストールします。</p>

<p>具体的には、 <strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするOpenMPI構築方法</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/#2-openmpiインストール">2. OpenMPIインストール</a></strong> を実施します。</p>

<h2 id="1-6-slurm-rpmパッケージ作成">1-6. Slurm rpmパッケージ作成</h2>

<p>本章は、Slurmマネージャでrpmパッケージを作成します。</p>

<ol>
  <li>
    <p>以下コマンドをSlurmマネージャのopcユーザで実行し、 <strong>Slurm</strong> の前提ソフトウェアをインストールします。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo dnf install -y rpm-build pam-devel perl readline-devel
</code></pre></div>    </div>
  </li>
  <li>
    <p>以下コマンドをSlurmマネージャのopcユーザで実行し、 <strong>Slurm</strong> rpmパッケージを作成します。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd ~; wget https://download.schedmd.com/slurm/slurm-23.11.0-0rc1.tar.bz2
$ rpmbuild --define '_prefix /opt/slurm' --define '_slurm_sysconfdir /opt/slurm/etc' --define '_with_pmix --with-pmix=/opt/pmix' --define '_with_ucx --with-ucx=/opt/ucx' -ta ./slurm-23.11.0-0rc1.tar.bz2
</code></pre></div>    </div>

    <p>作成されたパッケージは、以下のディレクトリに配置されるので、これらの全ファイルを他のサブシステムにコピーします。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ls -1 rpmbuild/RPMS/x86_64/
slurm-23.11.0-0rc1.el8.x86_64.rpm
slurm-contribs-23.11.0-0rc1.el8.x86_64.rpm
slurm-devel-23.11.0-0rc1.el8.x86_64.rpm
slurm-example-configs-23.11.0-0rc1.el8.x86_64.rpm
slurm-libpmi-23.11.0-0rc1.el8.x86_64.rpm
slurm-openlava-23.11.0-0rc1.el8.x86_64.rpm
slurm-pam_slurm-23.11.0-0rc1.el8.x86_64.rpm
slurm-perlapi-23.11.0-0rc1.el8.x86_64.rpm
slurm-sackd-23.11.0-0rc1.el8.x86_64.rpm
slurm-slurmctld-23.11.0-0rc1.el8.x86_64.rpm
slurm-slurmd-23.11.0-0rc1.el8.x86_64.rpm
slurm-slurmdbd-23.11.0-0rc1.el8.x86_64.rpm
slurm-torque-23.11.0-0rc1.el8.x86_64.rpm
$
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="1-7-slurm-rpmパッケージインストールセットアップ">1-7. Slurm rpmパッケージインストール・セットアップ</h2>

<p>本章は、先に作成した <strong>Slurm</strong> rpmパッケージを各サブシステムにインストールし、必要なセットアップ作業を実施します。</p>

<ol>
  <li>
    <p>以下コマンドをSlurmマネージャのopcユーザで実行し、Slurmマネージャに必要な <strong>Slurm</strong> rpmパッケージのインストール・セットアップを行います。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd ~/rpmbuild/RPMS/x86_64
$ sudo rpm -ivh ./slurm-23.11.0-0rc1.el8.x86_64.rpm ./slurm-slurmctld-23.11.0-0rc1.el8.x86_64.rpm ./slurm-slurmdbd-23.11.0-0rc1.el8.x86_64.rpm ./slurm-perlapi-23.11.0-0rc1.el8.x86_64.rpm
$ sudo useradd -m -d /var/lib/slurm -s /bin/bash -u 5000 slurm
$ sudo mkdir /var/spool/slurmctld; sudo chown slurm:slurm /var/spool/slurmctld
$ sudo mkdir /var/spool/slurmd; sudo chown slurm:slurm /var/spool/slurmd
$ sudo mkdir /var/log/slurm; sudo chown slurm:slurm /var/log/slurm
$ sudo mkdir /opt/slurm/etc; sudo chown slurm:slurm /opt/slurm/etc
$ sudo su - slurm
$ echo "export PATH=\$PATH:/opt/slurm/sbin:/opt/slurm/bin" | tee -a ~/.bash_profile
$ echo "export MANPATH=\$MANPATH:/opt/slurm/share/man" | tee -a ~/.bash_profile
$ source ~/.bash_profile
</code></pre></div>    </div>
  </li>
  <li>
    <p>計算ノードの <strong>Slurm</strong> rpmパッケージをコピーしたディレクトリで以下コマンドをopcユーザで実行し、計算ノードに必要な <strong>Slurm</strong> rpmパッケージのインストール・セットアップを行います。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo dnf install -y mariadb-devel
$ sudo rpm -ivh ./slurm-23.11.0-0rc1.el8.x86_64.rpm ./slurm-slurmd-23.11.0-0rc1.el8.x86_64.rpm ./slurm-perlapi-23.11.0-0rc1.el8.x86_64.rpm
$ sudo useradd -m -d /var/lib/slurm -s /bin/bash -u 5000 slurm
$ sudo mkdir /var/spool/slurmd; sudo chown slurm:slurm /var/spool/slurmd
$ sudo mkdir /var/log/slurm; sudo chown slurm:slurm /var/log/slurm
$ sudo mkdir /opt/slurm/etc; sudo chown slurm:slurm /opt/slurm/etc
</code></pre></div>    </div>
  </li>
  <li>
    <p>Slurmクライアントの <strong>Slurm</strong> rpmパッケージをコピーしたディレクトリで以下コマンドをopcユーザで実行し、Slurmクライアントに必要な <strong>Slurm</strong> rpmパッケージのインストール・セットアップを行います。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo dnf install -y mariadb-devel
$ sudo rpm -ivh ./slurm-23.11.0-0rc1.el8.x86_64.rpm ./slurm-perlapi-23.11.0-0rc1.el8.x86_64.rpm
$ sudo useradd -m -d /var/lib/slurm -s /bin/bash -u 5000 slurm
$ sudo mkdir /opt/slurm/etc; sudo chown slurm:slurm /opt/slurm/etc
$ echo '* soft memlock unlimited' | sudo tee -a /etc/security/limits.conf
$ echo '* hard memlock unlimited' | sudo tee -a /etc/security/limits.conf
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="1-8-slurm設定ファイル作成">1-8. Slurm設定ファイル作成</h2>

<p>本章は、以下3種類の <strong>Slurm</strong> 設定ファイルを作成し、これらを各サブシステムの/opt/slurm/etcディレクトリに配布します。<br />
この際、これらファイルのオーナーユーザ・オーナーグループをslurmとします。<br />
また、slurmdbd.confのパーミッションを600に設定します。</p>

<ul>
  <li>slurm.conf（全てのサブシステム）</li>
  <li>slurmdbd.conf（Slurmマネージャ）</li>
  <li>mpi.conf（Slurmマネージャ）</li>
</ul>

<p>[slurm.conf]</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">ClusterName</span><span class="o">=</span>sltest
<span class="nv">SlurmctldHost</span><span class="o">=</span>slurm-srv
<span class="nv">AuthType</span><span class="o">=</span>auth/munge
<span class="nv">PluginDir</span><span class="o">=</span>/opt/slurm/lib64/slurm
<span class="nv">SchedulerType</span><span class="o">=</span>sched/backfill
<span class="nv">SelectType</span><span class="o">=</span><span class="k">select</span>/linear
<span class="nv">SlurmUser</span><span class="o">=</span>slurm
<span class="nv">SlurmctldPort</span><span class="o">=</span>7002
<span class="nv">SlurmctldTimeout</span><span class="o">=</span>300
<span class="nv">SlurmdPort</span><span class="o">=</span>7003
<span class="nv">SlurmdSpoolDir</span><span class="o">=</span>/var/spool/slurmd
<span class="nv">SlurmdTimeout</span><span class="o">=</span>300
<span class="nv">SlurmctldLogFile</span><span class="o">=</span>/var/log/slurm/slurmctld.log
<span class="nv">SlurmdLogFile</span><span class="o">=</span>/var/log/slurm/slurmd.log
<span class="nv">SlurmdDebug</span><span class="o">=</span>3
<span class="nv">StateSaveLocation</span><span class="o">=</span>/var/spool/slurmd
<span class="nv">SwitchType</span><span class="o">=</span>switch/none
<span class="nv">AccountingStorageType</span><span class="o">=</span>accounting_storage/slurmdbd
<span class="nv">AccountingStorageHost</span><span class="o">=</span>slurm-srv
<span class="nv">AccountingStoragePort</span><span class="o">=</span>7004
<span class="nv">MpiDefault</span><span class="o">=</span>pmix
<span class="nv">NodeName</span><span class="o">=</span>DEFAULT <span class="nv">CPUs</span><span class="o">=</span>72 <span class="nv">Boards</span><span class="o">=</span>1 <span class="nv">SocketsPerBoard</span><span class="o">=</span>2 <span class="nv">CoresPerSocket</span><span class="o">=</span>18 <span class="nv">ThreadsPerCore</span><span class="o">=</span>2 <span class="nv">RealMemory</span><span class="o">=</span>500000 <span class="nv">TmpDisk</span><span class="o">=</span>10000 <span class="nv">State</span><span class="o">=</span>UNKNOWN
<span class="nv">NodeName</span><span class="o">=</span>inst-qiuim-x9,inst-wxedu-x9
<span class="nv">PartitionName</span><span class="o">=</span>sltest <span class="nv">Nodes</span><span class="o">=</span>ALL <span class="nv">Default</span><span class="o">=</span>YES <span class="nv">MaxTime</span><span class="o">=</span>INFINITE <span class="nv">State</span><span class="o">=</span>UP
</code></pre></div></div>

<p>なお、 <strong>SlurmctldHost</strong> 、 <strong>AccountingStorageHost</strong> 、及び <strong>NodeName</strong> の設定値は、自身の環境に合わせて修正します。</p>

<p>[slurmdbd.conf]</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">ArchiveEvents</span><span class="o">=</span><span class="nb">yes
</span><span class="nv">ArchiveJobs</span><span class="o">=</span><span class="nb">yes
</span><span class="nv">ArchiveResvs</span><span class="o">=</span><span class="nb">yes
</span><span class="nv">ArchiveSteps</span><span class="o">=</span>no
<span class="nv">ArchiveSuspend</span><span class="o">=</span>no
<span class="nv">ArchiveTXN</span><span class="o">=</span>no
<span class="nv">ArchiveUsage</span><span class="o">=</span>no
<span class="nv">AuthType</span><span class="o">=</span>auth/munge
<span class="nv">AuthInfo</span><span class="o">=</span>/var/run/munge/munge.socket.2
<span class="nv">DbdHost</span><span class="o">=</span>slurm-srv
<span class="nv">DbdPort</span><span class="o">=</span>7004
<span class="nv">DebugLevel</span><span class="o">=</span>info
<span class="nv">PurgeEventAfter</span><span class="o">=</span>1month
<span class="nv">PurgeJobAfter</span><span class="o">=</span>12month
<span class="nv">PurgeResvAfter</span><span class="o">=</span>1month
<span class="nv">PurgeStepAfter</span><span class="o">=</span>1month
<span class="nv">PurgeSuspendAfter</span><span class="o">=</span>1month
<span class="nv">PurgeTXNAfter</span><span class="o">=</span>12month
<span class="nv">PurgeUsageAfter</span><span class="o">=</span>24month
<span class="nv">LogFile</span><span class="o">=</span>/var/log/slurm/slurmdbd.log
<span class="nv">PidFile</span><span class="o">=</span>/var/run/slurmdbd/slurmdbd.pid
<span class="nv">SlurmUser</span><span class="o">=</span>slurm
<span class="nv">StorageType</span><span class="o">=</span>accounting_storage/mysql
<span class="nv">StorageUser</span><span class="o">=</span>slurm
<span class="nv">StoragePass</span><span class="o">=</span>passcord
<span class="nv">StorageLoc</span><span class="o">=</span>slurm_acct_db
</code></pre></div></div>

<p>なお、 <strong>DbdHost</strong> と <strong>StoragePass</strong> の設定値は、自身の環境に合わせて修正します。</p>

<p>[mpi.conf]</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">PMIxDirectConnEarly</span><span class="o">=</span><span class="nb">true
</span><span class="nv">PMIxDirectConnUCX</span><span class="o">=</span><span class="nb">true</span>
</code></pre></div></div>

<h2 id="1-9-slurmサービス起動確認">1-9. Slurmサービス起動・確認</h2>

<p>本章は、 <strong>Slurm</strong> の各systemdサービスを対象のサブシステムで起動します。</p>

<ol>
  <li>
    <p>以下コマンドを計算ノードのopcユーザで実行し、slurmdを起動します。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo systemctl enable --now slurmd
</code></pre></div>    </div>
  </li>
  <li>
    <p>以下コマンドをSlurmマネージャのopcユーザで実行し、slurmdbdとslurmctldを起動します。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo systemctl enable --now slurmdbd
$ sudo systemctl start slurmctld
</code></pre></div>    </div>
    <p>slurmctldは、計算ノードのslurmd起動完了後に起動するため、手動起動を想定して自動起動設定は行いません。</p>
  </li>
  <li>
    <p>以下コマンドをSlurmマネージャのslurmユーザで実行し、計算ノードがアイドルになっていることを確認します。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
sltest*      up   infinite      2   idle inst-qiuim-x9,inst-wxedu-x9
$
</code></pre></div>    </div>
  </li>
  <li>
    <p>以下コマンドをSlurmマネージャのslurmユーザで実行し、 <strong>OpenPMIx</strong> が利用可能になっていることを確認します。</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ srun --mpi=list
MPI plugin types are...
   none
   cray_shasta
   pmi2
   pmix
specific pmix plugin versions available: pmix_v4
$
</code></pre></div>    </div>
  </li>
</ol>

<hr />
<h1 id="2-slurm稼働確認">2. Slurm稼働確認</h1>

<p>本章は、構築した <strong>Slurm</strong> 環境を稼働確認するため、ホームディレクトリをNFSで共有するジョブ投入ユーザで <strong><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-mpi-benchmarks.html">Intel MPI Benchmark</a></strong> を実行するバッチジョブを実行し、その結果を確認します。<br />
またこのジョブの終了後、そのアカウンティング情報を取得できることを確認します。</p>

<ol>
  <li>
    <p>以下コマンドをSlurmクライアントのジョブ投入ユーザで実行し、 <strong>Slurm</strong> 関連の環境変数を設定します。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"export PATH=</span><span class="se">\$</span><span class="s2">PATH:/opt/slurm/sbin:/opt/slurm/bin"</span> | <span class="nb">tee</span> <span class="nt">-a</span> ~/.bash_profile
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"export MANPATH=</span><span class="se">\$</span><span class="s2">MANPATH:/opt/slurm/share/man"</span> | <span class="nb">tee</span> <span class="nt">-a</span> ~/.bash_profile
<span class="nv">$ </span><span class="nb">source</span> ~/.bash_profile
</code></pre></div>    </div>
  </li>
  <li>
    <p><strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするOpenMPI構築方法</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/#3-openmpi稼働確認">3. OpenMPI稼働確認</a></strong> の1.の手順を実施し、 <strong>Intel MPI Benchmark</strong> をインストールします。</p>
  </li>
  <li>
    <p><strong>Intel MPI Benchmark</strong> を実行する以下のジョブスクリプトをジョブ投入ユーザのホームディレクトリ直下にsubmit.shとして作成します。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c">#SBATCH -p sltest</span>
<span class="c">#SBATCH -n 2</span>
<span class="c">#SBATCH -N 2</span>
<span class="c">#SBATCH -J ping_ping</span>
<span class="c">#SBATCH -o stdout.%J</span>
<span class="c">#SBATCH -e stderr.%J</span>
<span class="nb">export </span><span class="nv">UCX_NET_DEVICES</span><span class="o">=</span>mlx5_2:1
srun /opt/openmpi-5.0.0/tests/imb/IMB-MPI1 <span class="nt">-msglog</span> 27:28 pingpong
</code></pre></div>    </div>
  </li>
  <li>
    <p>以下コマンドをSlurmクライアントのジョブ投入ユーザで実行し、バッチジョブの投入とその結果確認を行います。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cd</span> ~<span class="p">;</span> sbatch submit.sh
Submitted batch job xxx
<span class="nv">$ </span><span class="nb">cat </span>stdout.xxx
<span class="c">#----------------------------------------------------------------</span>
<span class="c">#    Intel(R) MPI Benchmarks 2021.7, MPI-1 part</span>
<span class="c">#----------------------------------------------------------------</span>
<span class="c"># Date                  : Thu Nov 30 18:27:27 2023</span>
<span class="c"># Machine               : x86_64</span>
<span class="c"># System                : Linux</span>
<span class="c"># Release               : 4.18.0-425.13.1.el8_7.x86_64</span>
<span class="c"># Version               : #1 SMP Tue Feb 21 15:09:05 PST 2023</span>
<span class="c"># MPI Version           : 3.1</span>
<span class="c"># MPI Thread Environment: </span>


<span class="c"># Calling sequence was: </span>

<span class="c"># /opt/openmpi-5.0.0/tests/imb/IMB-MPI1 -msglog 27:28 pingpong </span>

<span class="c"># Minimum message length in bytes:   0</span>
<span class="c"># Maximum message length in bytes:   268435456</span>
<span class="c">#</span>
<span class="c"># MPI_Datatype                   :   MPI_BYTE </span>
<span class="c"># MPI_Datatype for reductions    :   MPI_FLOAT </span>
<span class="c"># MPI_Op                         :   MPI_SUM  </span>
<span class="c"># </span>
<span class="c"># </span>

<span class="c"># List of Benchmarks to run:</span>

<span class="c"># PingPong</span>

<span class="c">#---------------------------------------------------</span>
<span class="c"># Benchmarking PingPong </span>
<span class="c"># #processes = 2 </span>
<span class="c">#---------------------------------------------------</span>
       <span class="c">#bytes #repetitions      t[usec]   Mbytes/sec</span>
            0         1000         1.59         0.00
    134217728            1     10969.75     12235.26
    268435456            1     21937.21     12236.53


<span class="c"># All processes entering MPI_Finalize</span>

<span class="err">$</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>以下コマンドをSlurmクライアントのジョブ投入ユーザで実行し、終了したジョブのアカウンティング情報が取得できることを確認します。</p>

    <div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>sacct <span class="nt">-j</span> xxx <span class="nt">-o</span> JobID,User,AllocCPUS,Start,End
JobID             User  AllocCPUS               Start                 End 
<span class="nt">------------</span> <span class="nt">---------</span> <span class="nt">----------</span> <span class="nt">-------------------</span> <span class="nt">-------------------</span> 
7                miyat        144 2023-11-30T18:27:27 2023-11-30T18:27:28 
7.batch                        72 2023-11-30T18:27:27 2023-11-30T18:27:28 
7.0                           144 2023-11-30T18:27:27 2023-11-30T18:27:28 
<span class="err">$</span>
</code></pre></div>    </div>
  </li>
</ol>

        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 更新日時:</strong> <time datetime="2024-02-16T13:01:50+09:00">February 16, 2024</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">共有</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Slurm%E3%81%AB%E3%82%88%E3%82%8B%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E7%AE%A1%E7%90%86%E3%83%BB%E3%82%B8%E3%83%A7%E3%83%96%E7%AE%A1%E7%90%86%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E6%A7%8B%E7%AF%89%E6%96%B9%E6%B3%95%20https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Ftech-knowhow%2Fsetup-slurm-cluster%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Ftech-knowhow%2Fsetup-slurm-cluster%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Ftech-knowhow%2Fsetup-slurm-cluster%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/" class="pagination--pager" title="Slurm環境での利用を前提とするOpenMPI構築方法
">前へ</a>
    
    
      <a href="#" class="pagination--pager disabled">次へ</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">関連記事</h4>
      <div class="grid__wrapper">
        
      </div>
    </div>
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="検索キーワードを入力してください..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>フォロー</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/#ocijp" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/oracle-japan" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/ocitutorials/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Oracle Cloud Infrastructure チュートリアル. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/ocitutorials/assets/js/main.min.js"></script>




<script src="/ocitutorials/assets/js/lunr/lunr.min.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-store.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6W7FEC5CEH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6W7FEC5CEH', { 'anonymize_ip': false});
</script>







  
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
  
    <script src="/ocitutorials/assets/js/clipboardrouge.js"></script>
  
    <script src="/ocitutorials/assets/js/tabs.js"></script>
  
    <script src="/ocitutorials/assets/js/sidebar.js"></script>
  



  </body>
</html>
