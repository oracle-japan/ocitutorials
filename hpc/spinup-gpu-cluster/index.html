<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ja" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>GPUクラスタを構築する(基礎インフラ手動構築編) | Oracle Cloud Infrastructure チュートリアル</title>
<meta name="description" content="GPUクラスタを構築してみましょう。このチュートリアルは、GPUクラスタのノード間接続に最適な高帯域・低遅延RDMA対応RoCEv2採用のクラスタ・ネットワークでベアメタルGPUインスタンスをノード間接続するGPUクラスタを、必要なリソースを順次コンソールから作成しながら構築します。">


  <meta name="author" content="Oracle Japan Solution Engineers">
  
  <meta property="article:author" content="Oracle Japan Solution Engineers">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ja_JP">
<meta property="og:site_name" content="Oracle Cloud Infrastructure チュートリアル">
<meta property="og:title" content="GPUクラスタを構築する(基礎インフラ手動構築編)">
<meta property="og:url" content="https://oracle-japan.github.io/ocitutorials/hpc/spinup-gpu-cluster/">


  <meta property="og:description" content="GPUクラスタを構築してみましょう。このチュートリアルは、GPUクラスタのノード間接続に最適な高帯域・低遅延RDMA対応RoCEv2採用のクラスタ・ネットワークでベアメタルGPUインスタンスをノード間接続するGPUクラスタを、必要なリソースを順次コンソールから作成しながら構築します。">



  <meta property="og:image" content="https://oracle-japan.github.io/ocitutorials/hpc/spinup-gpu-cluster/architecture_diagram.png">





  <meta property="article:published_time" content="2023-05-23T11:08:19+09:00">






<link rel="canonical" href="https://oracle-japan.github.io/ocitutorials/hpc/spinup-gpu-cluster/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "https://oracle-japan.github.io/ocitutorials/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/ocitutorials/feed.xml" type="application/atom+xml" rel="alternate" title="Oracle Cloud Infrastructure チュートリアル Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/ocitutorials/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/ocitutorials/"><img src="/ocitutorials/assets/images/social-og-oracle-badge.jpg" alt="OCI チュートリアル"></a>
        
        <a class="site-title" href="/ocitutorials/">
          OCI チュートリアル
          <span class="site-subtitle">Oracle Cloud Infrastructure を使ってみよう</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/ocitutorials/#%E3%83%81%E3%83%A5%E3%83%BC%E3%83%88%E3%83%AA%E3%82%A2%E3%83%AB%E3%82%B3%E3%83%B3%E3%83%86%E3%83%B3%E3%83%84%E4%B8%80%E8%A6%A7">チュートリアル一覧</a>
            </li><li class="masthead__menu-item">
              <a href="/ocitutorials/about/">このサイトについて</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">メニュー</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style=" background-image: linear-gradient(rgba(34, 66, 55, 0.7), rgba(34, 66, 55, 0.7)), url('/ocitutorials/hpc/spinup-gpu-cluster/architecture_diagram.png');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          GPUクラスタを構築する(基礎インフラ手動構築編)

        
      </h1>
      
        <p class="page__lead">GPUクラスタを構築してみましょう。このチュートリアルは、GPUクラスタのノード間接続に最適な高帯域・低遅延RDMA対応RoCEv2採用のクラスタ・ネットワークでベアメタルGPUインスタンスをノード間接続するGPUクラスタを、必要なリソースを順次コンソールから作成しながら構築します。
</p>
      
      


      
      
    </div>
  
  
</div>




  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="https://oracle-japan.github.io/ocitutorials/" itemprop="item"><span itemprop="name">ホーム</span></a>
          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">></span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/hpc" itemprop="item"><span itemprop="name">Hpc</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">></span>
      
    
      
      
        <li class="current">GPUクラスタを構築する(基礎インフラ手動構築編)</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">メニュー</label>
  <ul class="nav__items">
    <li>
      
      <a href=""><span class="nav__sub-title">HPC編</span></a>
      <ul>
        
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-cluster-network/">HPCクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster/">HPCクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster-withautoscaling/">HPCクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-ml-instance/">GPUインスタンスで機械学習にトライ</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster/" class="active">GPUクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withstack/">GPUクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withautoscaling/">GPUクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server/">ブロック・ボリュームでNFSファイルサーバを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-base/">ブロック・ボリュームNFSファイルサーバと基礎インフラ編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-stack/">ブロック・ボリュームNFSファイルサーバと自動構築編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/rdma-interface-configure/">クラスタ・ネットワーク接続用ネットワークインターフェース作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/">クラスタ・ネットワーク対応OSイメージの選び方</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-create-cnenabled-osimage/">クラスタ・ネットワーク非対応OSイメージを使ったクラスタ・ネットワーク接続方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/nvme-filesystem/">ベアメタルインスタンスの内蔵NVMe SSD領域ファイルシステム作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-name-resolution/">計算ノードの効果的な名前解決方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-os-customization/">計算ノードデプロイ時の効果的なOSカスタマイズ方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-host-list/">計算ノードのホスト名リスト作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/cluster-resize/">計算ノードの追加・削除・入れ替え方法</a></li></p>
        
      </ul>
    </li>
  </ul>
</nav>
    
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="GPUクラスタを構築する(基礎インフラ手動構築編)">
    <meta itemprop="description" content="GPUクラスタを構築してみましょう。このチュートリアルは、GPUクラスタのノード間接続に最適な高帯域・低遅延RDMA対応RoCEv2採用のクラスタ・ネットワークでベアメタルGPUインスタンスをノード間接続するGPUクラスタを、必要なリソースを順次コンソールから作成しながら構築します。">
    <meta itemprop="datePublished" content="2023-05-23T11:08:19+09:00">
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> 目次</h4></header>
              <ul class="toc__menu"><li><a href="#0-gpuクラスタ作成事前作業">0. GPUクラスタ作成事前作業</a><ul><li><a href="#0-0-概要">0-0. 概要</a></li><li><a href="#0-1-vcn作成">0-1. VCN作成</a></li><li><a href="#0-2-bastionノード作成">0-2. Bastionノード作成</a></li></ul></li><li><a href="#1-gpuクラスタ作成">1. GPUクラスタ作成</a><ul><li><a href="#1-0-概要">1-0. 概要</a></li><li><a href="#1-1-cloud-config作成">1-1. cloud-config作成</a></li><li><a href="#1-2-インスタンス構成作成">1-2. インスタンス構成作成</a></li><li><a href="#1-3-クラスタネットワーク作成">1-3. クラスタ・ネットワーク作成</a></li></ul></li><li><a href="#2-gpuノード確認">2. GPUノード確認</a><ul><li><a href="#21-gpuノードログイン">2.1. GPUノードログイン</a></li><li><a href="#22-cloud-init完了確認">2.2. cloud-init完了確認</a></li><li><a href="#23-gpuノードファイルシステム確認">2.3. GPUノードファイルシステム確認</a></li><li><a href="#24-dockerコンテナイメージ確認">2.4. Dockerコンテナイメージ確認</a></li></ul></li><li><a href="#3-horovodを使用するgpuクラスタ環境構築">3. Horovodを使用するGPUクラスタ環境構築</a><ul><li><a href="#3-1-dockerコンテナ環境構築">3-1. Dockerコンテナ環境構築</a><ul><li><a href="#3-1-0-概要">3-1-0. 概要</a></li><li><a href="#3-1-1-コンテナ間ssh接続環境構築">3-1-1. コンテナ間SSH接続環境構築</a></li><li><a href="#3-1-2-プライベートサブネットセキュリティリスト修正">3-1-2. プライベートサブネットセキュリティリスト修正</a></li><li><a href="#3-1-3-horovod用dockerコンテナ起動">3-1-3. Horovod用Dockerコンテナ起動</a></li></ul></li><li><a href="#3-2-nccl通信性能検証">3-2. NCCL通信性能検証</a><ul><li><a href="#3-2-0-概要">3-2-0. 概要</a></li><li><a href="#3-2-1-ncclアップデート">3-2-1. NCCLアップデート</a></li><li><a href="#3-2-2-nccl-testsビルド">3-2-2. NCCL Testsビルド</a></li><li><a href="#3-2-3-nccl-tests実行">3-2-3. NCCL Tests実行</a></li></ul></li><li><a href="#3-3-horovodサンプルプログラム実行">3-3. Horovodサンプルプログラム実行</a><ul><li><a href="#3-3-0-概要">3-3-0. 概要</a></li><li><a href="#3-3-1-horovodサンプルプログラム実行">3-3-1. Horovodサンプルプログラム実行</a></li></ul></li></ul></li><li><a href="#4-multiworkermirroredstrategyを使用するgpuクラスタ環境構築">4. MultiWorkerMirroredStrategyを使用するGPUクラスタ環境構築</a><ul><li><a href="#4-1-dockerコンテナ環境構築">4-1. Dockerコンテナ環境構築</a><ul><li><a href="#4-1-0-概要">4-1-0. 概要</a></li><li><a href="#4-1-1-コンテナ間ssh接続環境構築">4-1-1. コンテナ間SSH接続環境構築</a></li><li><a href="#4-1-2-プライベートサブネットセキュリティリスト修正">4-1-2. プライベートサブネットセキュリティリスト修正</a></li><li><a href="#4-1-3-dockerコンテナ起動">4-1-3. Dockerコンテナ起動</a></li><li><a href="#4-1-4-sshdインストール起動">4-1-4. sshdインストール・起動</a></li></ul></li><li><a href="#4-2-nccl通信性能検証">4-2. NCCL通信性能検証</a><ul><li><a href="#4-2-0-概要">4-2-0. 概要</a></li><li><a href="#4-2-1-nccl-testsビルド">4-2-1. NCCL Testsビルド</a></li><li><a href="#4-2-2-nccl-tests実行">4-2-2. NCCL Tests実行</a></li></ul></li><li><a href="#4-3-multiworkermirroredstrategyサンプルプログラム実行">4-3. MultiWorkerMirroredStrategyサンプルプログラム実行</a><ul><li><a href="#4-3-0-概要">4-3-0. 概要</a></li><li><a href="#4-3-1-multiworkermirroredstrategyサンプルプログラム作成">4-3-1. MultiWorkerMirroredStrategyサンプルプログラム作成</a></li><li><a href="#4-3-2-multiworkermirroredstrategyサンプルプログラム実行">4-3-2. MultiWorkerMirroredStrategyサンプルプログラム実行</a></li></ul></li></ul></li><li><a href="#5-gpuクラスタの削除">5. GPUクラスタの削除</a></li></ul>

            </nav>
          </aside>
        
        <p>このチュートリアルは、AIや機械学習ワークロードに最適なNVIDIA A100 40/80 GB 8枚と100 GbpsのRDMA対応ネットワークインタフェース16ポート搭載するGPUノード（ <strong><a href="https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-gpu">BM.GPU4.8/BM.GPU.GM4.8</a></strong> ）を <strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong> を使用してノード間接続し、1ノードでは搭載しきれないGPUを必要とする大規模なAI・機械学習ワークロードを実行するための分散機械学習に対応したDockerコンテナをGPUクラスタ上に構築、複数ノードに跨るGPU間の通信性能を <strong><a href="https://developer.nvidia.com/nccl">NCCL（NVIDIA Collective Communication Library）</a></strong> テストプログラム（ <strong><a href="https://github.com/nvidia/nccl-tests">NCCL Tests</a></strong> ）で検証後、分散機械学習のサンプルプログラムを実行、その性能を検証します。</p>

<p>このチュートリアルは、分散機械学習フレームワークに以下2種類を取り上げ、それぞれ3章と4章でこれらを解説しています。該当する章を参照することで、自身のワークロードに合わせた環境構築が可能です。</p>

<ul>
  <li>
    <p>Horovod（ <strong><a href="#3-horovodを使用するgpuクラスタ環境構築">3.Horovodを使用するGPUクラスタ環境構築</a></strong> ）</p>
  </li>
  <li>
    <p>MultiWorkerMirroredStrategy（ <strong><a href="#4-multiworkermirroredstrategyを使用するgpuクラスタ環境構築">4.MultiWorkerMirroredStrategyを使用するGPUクラスタ環境構築</a></strong> ）</p>
  </li>
</ul>

<p>よって本チュートリアルの進め方は、まず自身のワークロードに合わせて上記2種類からどちらを使用するか選択し、0章 → 1章 → 2章 → 3章 or 4章 → 5章と進めます。</p>

<p>このチュートリアルで作成する環境は、ユーザ管理、ホスト名管理、ファイル共有、プログラム開発環境、コンテナオーケストレーション等、必要なソフトウェア環境をこの上に整備し、ご自身の要件に沿ったGPUクラスタを構築する際の基礎インフラストラクチャとして利用することが可能です。<br />
なお、これらのクラスタ管理に必要なソフトウェアの導入までを自動化する <strong><a href="/ocitutorials/hpc/#5-10-hpcクラスタスタック">HPCクラスタスタック</a></strong> も利用可能で、詳細は <strong><a href="/ocitutorials/hpc/spinup-gpu-cluster-withstack/">GPUクラスタを構築する(スタティッククラスタ自動構築編)</a></strong> を参照ください。</p>

<p><img src="architecture_diagram.png" alt="システム構成図" /></p>

<p><strong>所要時間 :</strong> 約2時間</p>

<p><strong>前提条件 :</strong> GPUクラスタを収容するコンパートメント(ルート・コンパートメントでもOKです)の作成と、このコンパートメントに対する必要なリソース管理権限がユーザーに付与されていること。</p>

<p><strong>注意 :</strong> チュートリアル内の画面ショットについては、OCIの現在のコンソール画面と異なっている場合があります。</p>

<hr />
<h1 id="0-gpuクラスタ作成事前作業">0. GPUクラスタ作成事前作業</h1>

<h2 id="0-0-概要">0-0. 概要</h2>

<p>GPUクラスタを構成する <strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong> とGPUノードは、OCIコンソールからクラスタ・ネットワークを作成することで、GPUノードをクラスタ・ネットワークに接続したGPUクラスタとしてデプロイされます。</p>

<p>このため、このGPUノードをTCP接続するVCNと、インターネットから直接アクセス出来ないプライベートサブネットに通常接続されるGPUノードにログインする際の踏み台となるBastionノードを、GPUクラスタ作成前に予め用意する必要があります。</p>

<p>本章は、これらGPUクラスタ作成の前提となるリソースを作成します。</p>

<h2 id="0-1-vcn作成">0-1. VCN作成</h2>

<p>本章は、GPUノードをTCP接続するVCNを作成します。
VCNの作成は、以下チュートリアルページ <strong>クラウドに仮想ネットワーク(VCN)を作る</strong> の手順通りに実行し、</p>

<p><a href="https://oracle-japan.github.io/ocitutorials/beginners/creating-vcn">https://oracle-japan.github.io/ocitutorials/beginners/creating-vcn</a></p>

<p>以下のリソースを作成します。</p>

<ul>
  <li>VCN（10.0.0.0/16）</li>
  <li>パブリックサブネット（10.0.0.0/24）</li>
  <li>プライベートサブネット（10.0.1.0/24）</li>
  <li>インターネット・ゲートウェイ（パブリックサブネットにアタッチ）</li>
  <li>NATゲートウェイ（プライベートサブネットにアタッチ）</li>
  <li>サービス・ゲートウェイ（プライベートサブネットにアタッチ）</li>
  <li>ルート表 x 2（パブリックサブネットとプライベートサブネットにアタッチ）</li>
  <li>セキュリティリスト x 2（パブリックサブネットとプライベートサブネットにアタッチ）</li>
</ul>

<p>このVCNは、セキュリティリストで以下のアクセス制限が掛けられています。</p>

<ul>
  <li>インターネットからのアクセス：パブリックサブネットに接続されるインスタンスの22番ポート（SSH）に限定</li>
  <li>インターネットへのアクセス：インターネット上の任意のIPアドレス・ポートに制限なくアクセス可能</li>
</ul>

<h2 id="0-2-bastionノード作成">0-2. Bastionノード作成</h2>

<p>本章は、GPUノードにログインする際の踏み台となるBastionノードを作成します。
Bastionノードの作成は、以下チュートリアルページ <strong>インスタンスを作成する</strong> の手順を参考に、</p>

<p><a href="https://oracle-japan.github.io/ocitutorials/beginners/creating-compute-instance">https://oracle-japan.github.io/ocitutorials/beginners/creating-compute-instance</a></p>

<p>ご自身の要件に沿ったインスタンスを、先の手順で作成したVCNとパブリックサブネットを指定して作成します。本チュートリアルは、以下属性のインスタンスをBastionノードとして作成します。</p>

<ul>
  <li><strong>イメージ　:</strong> Oracle Linux 7.9</li>
  <li><strong>シェイプ　:</strong> VM.Optimized3.Flex（1 OCPU）</li>
  <li><strong>SSHキーの追加　:</strong> Bastionノードにログインする際使用するSSH秘密鍵に対応する公開鍵</li>
</ul>

<p>次に、このBastionノード上でSSHの鍵ペアを作成します。このSSH鍵は、BastionノードからGPUノードにログインする際に使用します。
先のチュートリアル <strong>インスタンスを作成する</strong> に記載のインスタンスへの接続方法に従いBastionノードにopcユーザでSSHログインし、以下のコマンドで全ての問いにエンターキーを入力（パスフレーズ無し・デフォルトの格納場所）してSSH鍵ペアを作成、作成された公開鍵を後のクラスタ・ネットワーク作成手順で指定します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> ssh-keygen
Generating public/private rsa key pair.
Enter file <span class="k">in </span>which to save the key <span class="o">(</span>/home/opc/.ssh/id_rsa<span class="o">)</span>: 
Enter passphrase <span class="o">(</span>empty <span class="k">for </span>no passphrase<span class="o">)</span>: 
Enter same passphrase again: 
Your identification has been saved <span class="k">in</span> /home/opc/.ssh/id_rsa.
Your public key has been saved <span class="k">in</span> /home/opc/.ssh/id_rsa.pub.
The key fingerprint is:
SHA256:Ska1aH1fQkN+Ahzi6xnqICSEHN8HUKDlujhEagu2Uc8 opc@bastion
The keys randomart image is:
+---[RSA 2048]----+
| <span class="nb">.</span> ++o  o.oo+    |
|o <span class="k">*</span> <span class="nb">.</span> .<span class="o">=</span> o.+ <span class="nb">.</span>   |
|.<span class="o">=</span> + .+.+ <span class="nb">.</span> + o  |
|+ o oo.  o <span class="nb">.</span> <span class="o">=</span>   |
|+<span class="k">*</span><span class="nb">.</span>  Eo S   <span class="nb">.</span>    |
|<span class="k">*</span><span class="nv">o</span><span class="o">=</span>  o + o       |
|o+. <span class="nb">.</span> o o        |
| <span class="nb">.</span> <span class="nb">.</span> o           |
|      <span class="nb">.</span>          |
+----[SHA256]-----+
<span class="o">&gt;</span> <span class="nb">cat</span> ~/.ssh/id_rsa.pub 
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Lna2m3TPiPKL/lHNK4GK2bkADRzm4674uwO9PHUqEPKVv+HBhTZ+zHOPSkYsOEubgeB9xpuKe+Z7ats0RbdXzT1bDWxcsvrMOdUVHQ9zv54eBSz+wEJO08zuxCjetQ2//6NRlYzoBs5/T1+DWg7lJuNadeyqXf1IaZGxRyfbCyXPzOnhL3TS/S7ydN0/313PsqAYj7PBNlx86WT/0qeNYsefjVmn54PKp1waNDQbOkiXi9Emx9uIKA1TCMCVSauZEI274P6orPvwggbX/HZ5Q8eRta2uw3LmzSRJUlrLBxi5xzhVOSNOXl29y2+U5+Q5/F2AxGSxUbW18AdOihuX opc@bastion
</code></pre></div></div>

<p>次に、以降作成するGPUノードの名前解決をインスタンス名で行うため、テクニカルTips <strong><a href="/ocitutorials/hpc/tech-knowhow/compute-name-resolution/">計算ノードの効果的な名前解決方法</a></strong> の手順を実施します。</p>

<hr />
<h1 id="1-gpuクラスタ作成">1. GPUクラスタ作成</h1>

<h2 id="1-0-概要">1-0. 概要</h2>

<p><strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong> は、作成時に指定する <strong><a href="/ocitutorials/hpc/#5-7-インスタンス構成">インスタンス構成</a></strong> に基づいて <strong><a href="/ocitutorials/hpc/#5-8-インスタンスプール">インスタンス・プール</a></strong> が作成時に指定するノード数のGPUノードをデプロイし、これをクラスタ・ネットワークに接続します。</p>

<p>クラスタ・ネットワークに接続するGPUノードは、OS起動時点でクラスタ・ネットワークに接続するネットワークインターフェースが作成されていないため、 <strong><a href="/ocitutorials/hpc/#5-11-cloud-init">cloud-init</a></strong> でこの作成を行います。また本チュートリアルは、GPUノードに装備されるNVMeローカルディスクのファイルシステム作成も、このcloud-initから行います。</p>

<p>以上より、GPUクラスタの作成は、以下の手順を経て行います。</p>

<ul>
  <li>cloud-init設定ファイル（cloud-config）作成</li>
  <li>インスタンス構成作成</li>
  <li>クラスタ・ネットワーク作成</li>
</ul>

<p>本チュートリアルは、2ノードのBM.GPU4.8を使用してGPUクラスタを構築しますが、 <strong><a href="https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-gpu">BM.GPU.GM4.8</a></strong> をGPUノードとするGPUクラスタを構築する際も、シェイプ指定の変更で対応可能です。</p>

<h2 id="1-1-cloud-config作成">1-1. cloud-config作成</h2>

<p>本章は、 <strong><a href="/ocitutorials/hpc/#5-11-cloud-init">cloud-init</a></strong> 設定ファイル（cloud-config）を作成します。</p>

<p>本チュートリアルは、このcloud-initを以下の目的で使用します。</p>

<ul>
  <li>Docker Community Editionインストール</li>
  <li>NVIDIA Container Toolkitインストール</li>
  <li>NVMeローカルディスクファイルシステム作成</li>
  <li>firewalld停止</li>
  <li><strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong> 接続用ネットワークインターフェース起動</li>
  <li>ルートファイルシステム拡張</li>
  <li>Dockerイメージプル</li>
</ul>

<p>以下は、本チュートリアルで使用するBM.GPU4.8用のcloud-configで、OCIコンソールを実行している端末上にテキストファイルで保存します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#cloud-config</span>
yum_repos:
<span class="c"># To install docker community edition</span>
  ol7_developer:
    name: Oracle Linux <span class="nv">$releasever</span> Development Packages <span class="o">(</span><span class="nv">$basearch</span><span class="o">)</span>
    baseurl: https://yum<span class="nv">$ociregion</span>.<span class="nv">$ocidomain</span>/repo/OracleLinux/OL7/developer/<span class="nv">$basearch</span>/
    enabled: <span class="nb">true
    </span>gpgcheck: <span class="nb">true
    </span>gpgkey: file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
  docker-ce-stable:
    name: Docker CE Stable - <span class="nv">$basearch</span>
    baseurl: https://download.docker.com/linux/centos/<span class="nv">$releasever</span>/<span class="nv">$basearch</span>/stable
    enabled: <span class="nb">true
    </span>gpgcheck: <span class="nb">true
    </span>gpgkey: https://download.docker.com/linux/centos/gpg
<span class="c"># To install NVIDIA container</span>
  libnvidia-container:
    name: libnvidia-container
    baseurl: https://nvidia.github.io/libnvidia-container/stable/centos7/<span class="nv">$basearch</span>
    enabled: <span class="nb">true
    </span>gpgcheck: <span class="nb">true
    </span>gpgkey: https://nvidia.github.io/libnvidia-container/gpgkey
  libnvidia-container-experimental:
    name: libnvidia-container-experimental
    baseurl: https://nvidia.github.io/libnvidia-container/experimental/centos7/<span class="nv">$basearch</span>
    enabled: <span class="nb">true
    </span>gpgcheck: <span class="nb">true
    </span>gpgkey: https://nvidia.github.io/libnvidia-container/gpgkey
packages:
<span class="c"># Install Docker community edition and NVIDIA container toolkit</span>
  - docker-ce
  - nvidia-container-toolkit
runcmd:
<span class="c"># NVMe local storage setting</span>
  - vgcreate nvme /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1
  - lvcreate <span class="nt">-l</span> 100%FREE nvme
  - mkfs.xfs <span class="nt">-L</span> localscratch /dev/nvme/lvol0
  - <span class="nb">mkdir</span> <span class="nt">-p</span> /mnt/localdisk
  - <span class="nb">echo</span> <span class="s2">"LABEL=localscratch /mnt/localdisk/ xfs defaults,noatime 0 0"</span> <span class="o">&gt;&gt;</span> /etc/fstab
  - mount /mnt/localdisk
<span class="c"># Stop firewalld</span>
  - systemctl stop firewalld
  - systemctl disable firewalld
<span class="c"># Set up cluster network interface</span>
  - systemctl start oci-rdma-configure
<span class="c"># Expand root file system to those set by instance configuration</span>
  - /usr/libexec/oci-growfs <span class="nt">-y</span>
<span class="c"># Pull Horovod/TensorFlow docker images</span>
  - systemctl start docker
  - systemctl <span class="nb">enable </span>docker
  - docker pull horovod/horovod:latest
  - docker pull nvcr.io/nvidia/tensorflow:22.11-tf2-py3
</code></pre></div></div>

<p>このcloud-configで行っているクラスタ・ネットワーク接続用ネットワークインターフェース起動は、クラスタ・ネットワーク対応OSイメージに含まれるsystemdのサービス <strong>oci-rdma-configure</strong> を使用しますが、この詳細はテクニカルTips <strong><a href="/ocitutorials/hpc/tech-knowhow/rdma-interface-configure/">クラスタ・ネットワーク接続用ネットワークインターフェース作成方法</a></strong> を参照ください。</p>

<h2 id="1-2-インスタンス構成作成">1-2. インスタンス構成作成</h2>

<p>本章は、 <strong><a href="/ocitutorials/hpc/#5-7-インスタンス構成">インスタンス構成</a></strong> を作成します。</p>

<ol>
  <li>
    <p>OCIコンソールにログインし、GPUクラスタをデプロイするリージョンを選択後、 <strong>コンピュート</strong> → <strong>インスタンス構成</strong> とメニューを辿ります。</p>
  </li>
  <li>
    <p>表示される以下画面で、<strong>インスタンス構成の作成</strong> ボタンをクリックします。</p>

    <p><img src="console_page01.png" alt="画面ショット" /></p>
  </li>
  <li>
    <p>表示される <strong>インスタンス構成の作成</strong> 画面で、以下の情報を入力し <strong>作成</strong> ボタンをクリックします。なお、ここに記載のないフィールドは、デフォルトのままとします。</p>

    <p>3.1 <strong>インスタンス構成情報</strong> フィールド</p>

    <ul>
      <li><strong>名前</strong> ：インスタンス構成に付与する名前</li>
      <li><strong>コンパートメントに作成</strong> ：インスタンス構成を作成するコンパートメント</li>
    </ul>

    <p><img src="console_page02.png" alt="画面ショット" /></p>

    <p>3.2 <strong>インスタンスの作成先のコンパートメント</strong> フィールド：インスタンスをデプロイするコンパートメント</p>

    <p><img src="console_page03.png" alt="画面ショット" /></p>

    <p>3.3 <strong>配置</strong> フィールド</p>
    <ul>
      <li><strong>可用性ドメイン</strong> ：インスタンスをデプロイする可用性ドメイン</li>
    </ul>

    <p><img src="console_page04.png" alt="画面ショット" /></p>

    <p>3.4 <strong>イメージとシェイプ</strong> フィールド</p>

    <p><img src="console_page05.png" alt="画面ショット" /></p>

    <ul>
      <li><strong>イメージ</strong> ：Oracle Linux 7 - GPU Cluster Networking Image (<strong>イメージの変更</strong> ボタンをクリックして表示される以下 <strong>イメージの選択</strong> サイドバーで <strong>Marketplace</strong> アイコンを選択し検索フィールドに <strong>gpu</strong> と入力して表示される <strong>Oracle Linux 7 - GPU Cluster Networking Image</strong> を選択し <strong>イメージ・ビルド</strong> フィールドで <strong>OracleLinux-7-RHCK-3.10.0-OFED-5.4-3.6.8.1-GPU-515-2023.01.10-0</strong> を選択し <strong>イメージの選択</strong> ボタンをクリック）</li>
    </ul>

    <p><img src="console_page06.png" alt="画面ショット" /></p>

    <ul>
      <li><strong>Shape</strong> ：BM.GPU4.8 (<strong>Change Shape</strong> ボタンをクリックして表示される以下 <strong>すべてのシェイプの参照</strong> サイドバーで <strong>ベア・メタル・マシン</strong> をクリックして表示される <strong>BM.GPU4.8</strong> を選択し <strong>次のドキュメントを確認した上でこれに同意します</strong> チェックボックスをチェックし <strong>シェイプの選択</strong> ボタンをクリック）</li>
    </ul>

    <p><img src="console_page07.png" alt="画面ショット" /></p>

    <p>3.5 <strong>ネットワーキング</strong> フィールド</p>
    <ul>
      <li><strong>プライマリ・ネットワーク</strong> ： 先に作成したVCNを選択</li>
      <li><strong>サブネット</strong> ：先に作成したプライベートサブネットを選択</li>
    </ul>

    <p><img src="console_page08.png" alt="画面ショット" /></p>

    <p>3.6 <strong>SSHキーの追加</strong> フィールド</p>
    <ul>
      <li><strong>SSHキー</strong> ：先にBastionノードで作成したSSH鍵の公開鍵（ 以下 <strong>公開キーの貼付け</strong> ラジオボタンを選択することで入力フィールドを表示）</li>
    </ul>

    <p><img src="console_page09.png" alt="画面ショット" /></p>

    <p>3.7 <strong>ブート・ボリューム</strong> フィールド</p>
    <ul>
      <li><strong>カスタム・ブート・ボリューム・サイズを指定します</strong> ： チェック</li>
      <li><strong>ブート・ボリューム・サイズ(GB)</strong> ：100</li>
    </ul>

    <p><img src="console_page27.png" alt="画面ショット" /></p>

    <p>3.8 <strong>管理</strong> フィールド（以下 <strong>拡張オプションの表示</strong> ボタンを選択して表示）</p>

    <p><img src="console_page10.png" alt="画面ショット" /></p>

    <ul>
      <li><strong>cloud-initスクリプト</strong> ：先に作成したcloud-init設定ファイル（cloud-config）を選択（ <strong>参照</strong> ボタンでファイルを選択）</li>
    </ul>

    <p><img src="console_page11.png" alt="画面ショット" /></p>
  </li>
</ol>

<h2 id="1-3-クラスタネットワーク作成">1-3. クラスタ・ネットワーク作成</h2>

<p>本章は、 <strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong> を作成します。</p>

<ol>
  <li>
    <p>OCIコンソールにログインし、GPUクラスタをデプロイするリージョンを選択後、 <strong>コンピュート</strong> → <strong>クラスタ・ネットワーク</strong> とメニューを辿ります。</p>
  </li>
  <li>
    <p>表示される以下画面で、<strong>クラスタ・ネットワークの作成</strong> ボタンをクリックします。</p>

    <p><img src="console_page12.png" alt="画面ショット" /></p>
  </li>
  <li>
    <p>表示される <strong>クラスタ・ネットワークの作成</strong> 画面で、以下の情報を入力し <strong>クラスタ・ネットワークの作成</strong> ボタンをクリックします。なお、ここに記載のないフィールドは、デフォルトのままとします。</p>

    <p>3.1 <strong>名前</strong> フィールド：クラスタ・ネットワークに付与する名前</p>

    <p><img src="console_page13.png" alt="画面ショット" /></p>

    <p>3.2 <strong>コンパートメントに作成</strong> フィールド：クラスタ・ネットワークをデプロイするコンパートメント</p>

    <p><img src="console_page14.png" alt="画面ショット" /></p>

    <p>3.2 <strong>可用性ドメイン</strong> フィールド：クラスタ・ネットワークをデプロイする可用性ドメイン</p>

    <p><img src="console_page15.png" alt="画面ショット" /></p>

    <p>3.3 <strong>ネットワーキングの構成</strong> フィールド</p>

    <ul>
      <li><strong>仮想クラウド・ネットワーク</strong> ：先に作成したVCNを選択</li>
      <li><strong>サブネット</strong> ：先に作成したプライベートサブネットを選択</li>
    </ul>

    <p><img src="console_page16.png" alt="画面ショット" /></p>

    <p>3.4 <strong>インスタンス・プールの構成</strong> フィールド</p>

    <ul>
      <li><strong>インスタンス・プール名</strong> ：作成されるインスタンス・プールに付与する名前</li>
      <li><strong>インスタンス数</strong> ：2（デプロイするGPUノードのノード数）</li>
      <li><strong>インスタンス構成</strong> ：先に作成したインスタンス構成</li>
    </ul>

    <p><img src="console_page17.png" alt="画面ショット" /></p>
  </li>
  <li>
    <p>表示される以下 <strong>クラスタ・ネットワーク作業リクエスト</strong> 画面で、左上のステータスが <strong>プロビジョニング中</strong> と表示されれば、クラスタ・ネットワークとGPUノードの作成が実施されています。</p>

    <p><img src="console_page18.png" alt="画面ショット" /></p>

    <p>ステータスが <strong>実行中</strong> となれば、クラスタ・ネットワークとGPUノードの作成が完了しています。</p>
  </li>
</ol>

<hr />
<h1 id="2-gpuノード確認">2. GPUノード確認</h1>

<p>本章は、デプロイされたGPUノードにログインし、環境を確認します。</p>

<h2 id="21-gpuノードログイン">2.1. GPUノードログイン</h2>

<p>GPUノードは、プライベートサブネットに接続されており、インターネットからログインすることが出来ないため、Bastionノードを経由してSSHログインします。BastionノードからGPUノードへのログインは、GPUノードのインスタンス名を使用します。</p>

<p>GPUノードのインスタンス名は、OCIコンソールでGPUノードをデプロイしたリージョンを選択後、 <strong>コンピュート</strong> → <strong>インスタンス</strong> とメニューを辿り、以下のインスタンス一覧からそのインスタンス名を確認します。</p>

<p>またこの画面は、GPUノードのIPアドレスも表示しており、これを使用してBastionノードからSSHログインすることも可能です。</p>

<p><img src="console_page19.png" alt="画面ショット" /></p>

<p>GPUノードへのログインは、以下のようにBastionノードからopcユーザでSSHログインします。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> ssh inst-d5ige-comp
The authenticity of host <span class="s1">'inst-d5ige-comp (10.0.2.67)'</span> cannot be established.
ECDSA key fingerprint is SHA256:Iro630ws/Fm3gbc3VNPp9BQ+AJnN9K5fcC2ZbzjJVXs.
ECDSA key fingerprint is MD5:2d:52:32:d9:43:24:5a:85:85:a9:aa:08:12:bf:fa:bb.
Are you sure you want to <span class="k">continue </span>connecting <span class="o">(</span><span class="nb">yes</span>/no<span class="o">)</span>? <span class="nb">yes
</span>Warning: Permanently added <span class="s1">'inst-d5ige-comp,10.0.2.67'</span> <span class="o">(</span>ECDSA<span class="o">)</span> to the list of known hosts.
</code></pre></div></div>

<h2 id="22-cloud-init完了確認">2.2. cloud-init完了確認</h2>

<p><strong><a href="/ocitutorials/hpc/#5-11-cloud-init">cloud-init</a></strong> は、GPUノードが起動してSSHログインできる状態であっても、その処理が継続している可能性があるため、以下コマンドでそのステータスを表示し、 <strong>done</strong> となっていることでcloud-initの処理完了を確認します。</p>

<p>ステータスが <strong>running</strong> の場合は、cloud-initの処理が継続中のため、処理が完了するまで待ちます。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">sudo </span>cloud-init status
status: <span class="k">done</span>
</code></pre></div></div>

<h2 id="23-gpuノードファイルシステム確認">2.3. GPUノードファイルシステム確認</h2>

<p>cloud-initが完了したGPUノードは、以下のようにNVMe領域が/mnt/localdiskにマウントされています。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">df</span> <span class="nt">-h</span> /mnt/localdisk
Filesystem              Size  Used Avail Use% Mounted on
/dev/mapper/nvme-lvol0   25T   34M   25T   1% /mnt/localdisk
</code></pre></div></div>

<h2 id="24-dockerコンテナイメージ確認">2.4. Dockerコンテナイメージ確認</h2>

<p>cloud-initが完了したGPUノードは、以下のように2種類のDockerコンテナイメージがプルされています。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">sudo </span>docker images
REPOSITORY                  TAG             IMAGE ID       CREATED        SIZE
nvcr.io/nvidia/tensorflow   22.11-tf2-py3   a88317ae0b1b   3 weeks ago    14.4GB
horovod/horovod             latest          f16647de3f02   2 months ago   14.2GB
</code></pre></div></div>

<hr />
<h1 id="3-horovodを使用するgpuクラスタ環境構築">3. Horovodを使用するGPUクラスタ環境構築</h1>

<h2 id="3-1-dockerコンテナ環境構築">3-1. Dockerコンテナ環境構築</h2>

<h3 id="3-1-0-概要">3-1-0. 概要</h3>

<p>本章は、後の章で実行するNCCL TestsとHorovodのサンプルプログラムを実行するHorovod用Dockerコンテナを起動するため、必要な環境構築作業を行います。</p>

<p>NCCL TestsとHorovodのサンプルプログラムは、コンテナを跨るプログラム実行のコントローラとしてMPIを使用します。ここで使用するMPIは、Horovod用Dockerコンテナに予め含まれる、OpenMPIです。</p>

<p>OpenMPIをコンテナ間で実行するためには、MPIプログラムをmpirun等で起動するコンテナ（いわゆるヘッドノード）からMPIプログラム実行に参加する他の全てのコンテナにパスフレーズ無しでSSH接続できる必要があります。</p>

<p>またOpenMPIの実行は、これを実行するコンテナ間で必要なポートにアクセス出来る必要があるため、GPUノードが接続されるプライベートサブネットのセキュリティリストを修正する必要があります。</p>

<p>以上より、本章で実施するDockerコンテナ環境構築は、以下の手順を経て行います。</p>

<ul>
  <li>コンテナ間SSH接続環境構築</li>
  <li>プライベートサブネットセキュリティリスト修正</li>
  <li>Horovod用Dockerコンテナ起動</li>
</ul>

<h3 id="3-1-1-コンテナ間ssh接続環境構築">3-1-1. コンテナ間SSH接続環境構築</h3>

<p>本章は、先にBastionノードで作成したSSH秘密鍵を含む.sshディレクトリをGPUノードにコピーし、後のコンテナ起動時にこのディレクトリをコンテナにマウントすることで、コンテナ間のパスフレーズ無しSSH接続環境を実現します。</p>

<p>まず初めに、テクニカルTips <strong><a href="/ocitutorials/hpc/tech-knowhow/compute-host-list/">計算ノードのホスト名リスト作成方法</a></strong> の手順を実施し、以下のように全てのGPUノードのホスト名を含むホスト名リストをBastionノード上にファイル名 <strong>hostlist.txt</strong> で作成します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>inst-d5ige-comp
inst-swgen-comp
</code></pre></div></div>

<p>次にこのホスト名リストを使用し、Bastionノードのopcユーザで以下コマンドを実行、全GPUノードのホストキーを含むknown_hostsファイルを作成します。この際、GPUノード毎に接続確認を求められるため、全てに <strong>yes</strong> を入力します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="k">for </span>hname <span class="k">in</span> <span class="sb">`</span><span class="nb">cat </span>hostlist.txt<span class="sb">`</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="nv">$hname</span><span class="p">;</span> ssh <span class="nv">$hname</span> <span class="nb">hostname</span><span class="p">;</span> <span class="k">done
</span>inst-d5ige-comp
inst-d5ige-comp
inst-swgen-comp
The authenticity of host <span class="s1">'inst-swgen-comp (10.0.2.242)'</span> cannot be established.
ECDSA key fingerprint is SHA256:koWs+bKLzf78RQsZ+mQhvwxEQTu/72St2DiVyVqmtl4.
ECDSA key fingerprint is MD5:68:b4:e0:37:f1:9a:36:c7:a5:3c:69:9f:91:d5:e0:34.
Are you sure you want to <span class="k">continue </span>connecting <span class="o">(</span><span class="nb">yes</span>/no<span class="o">)</span>? <span class="nb">yes
</span>Warning: Permanently added <span class="s1">'inst-swgen-comp,10.0.2.242'</span> <span class="o">(</span>ECDSA<span class="o">)</span> to the list of known hosts.
inst-swgen-comp
</code></pre></div></div>

<p>次に、Bastionノードのopcユーザで以下コマンドを実行、Bastionノードで作成した秘密鍵を使ったSSHログインを許可します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">cat</span> ~/.ssh/id_rsa.pub <span class="o">&gt;&gt;</span> ~/.ssh/authorized_keys
</code></pre></div></div>

<p>次に、Bastionノードのopcユーザで以下コマンドを実行、~opc/.sshディレクトリをアーカイブしてこれを全GPUノードにコピーします。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">cd</span> ~
<span class="o">&gt;</span> <span class="nb">tar</span> <span class="nt">-cvf</span> /tmp/ssh.tar ./.ssh
./.ssh/
./.ssh/id_rsa
./.ssh/known_hosts
./.ssh/id_rsa.pub
./.ssh/authorized_keys
<span class="o">&gt;</span> <span class="k">for </span>hname <span class="k">in</span> <span class="sb">`</span><span class="nb">cat </span>hostlist.txt<span class="sb">`</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="nv">$hname</span><span class="p">;</span> scp /tmp/ssh.tar <span class="nv">$hname</span>:/tmp/<span class="p">;</span> <span class="k">done
</span>inst-d5ige-comp
ssh.tar                                                                100%   10KB   9.0MB/s   00:00    
inst-swgen-comp
ssh.tar                                                                100%   10KB  11.5MB/s   00:00
</code></pre></div></div>

<p>次に、Bastionノードのopcユーザで以下コマンドを実行、先のアーカイブをGPUノードの/horovodディレクトリに展開します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="k">for </span>hname <span class="k">in</span> <span class="sb">`</span><span class="nb">cat </span>hostlist.txt<span class="sb">`</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="nv">$hname</span><span class="p">;</span> ssh <span class="nv">$hname</span> <span class="s2">"sudo mkdir /horovod"</span><span class="p">;</span> <span class="k">done
</span>inst-d5ige-comp
inst-swgen-comp
<span class="o">&gt;</span> <span class="k">for </span>hname <span class="k">in</span> <span class="sb">`</span><span class="nb">cat </span>hostlist.txt<span class="sb">`</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="nv">$hname</span><span class="p">;</span> ssh <span class="nv">$hname</span> <span class="s2">"sudo tar --no-same-owner -xvf /tmp/ssh.tar -C /horovod/"</span><span class="p">;</span> <span class="k">done
</span>inst-d5ige-comp
./.ssh/
./.ssh/id_rsa
./.ssh/id_rsa.pub
./.ssh/authorized_keys
./.ssh/known_hosts
inst-swgen-comp
./.ssh/
./.ssh/id_rsa
./.ssh/id_rsa.pub
./.ssh/authorized_keys
./.ssh/known_hosts
</code></pre></div></div>

<h3 id="3-1-2-プライベートサブネットセキュリティリスト修正">3-1-2. プライベートサブネットセキュリティリスト修正</h3>

<p>本章は、プライベートサブネットのセキュリティリストを以下の手順で修正します。</p>

<ol>
  <li>
    <p>OCIコンソールにログインし、GPUノードをデプロイしたリージョンを選択後、 <strong>ネットワーキング</strong> → <strong>仮想クラウド・ネットワーク</strong> とメニューを辿ります。</p>
  </li>
  <li>
    <p>表示される画面で、先に作成した仮想クラウド・ネットワークをクリックします。</p>
  </li>
  <li>
    <p>表示される以下 <strong>サブネット</strong> フィールドで、先に作成したプライベートサブネットをクリックします。</p>

    <p><img src="console_page21.png" alt="画面ショット" /></p>
  </li>
  <li>
    <p>表示される以下 <strong>セキュリティ・リスト</strong> フィールドで、プライベートサブネットに適用されているセキュリティリストをクリックします。</p>

    <p><img src="console_page22.png" alt="画面ショット" /></p>
  </li>
  <li>
    <p>表示される以下 <strong>イングレス・ルール</strong> フィールドで、SSHアクセスを許可しているルールの <strong>編集</strong> メニューをクリックします。</p>

    <p><img src="console_page23.png" alt="画面ショット" /></p>
  </li>
  <li>
    <p>表示される以下 <strong>イングレス・ルールの編集</strong> サイドバーで、 <strong>IPプロトコル</strong> フィールドを <strong>すべてのプロトコル</strong> に変更し、 <strong>変更の保存</strong> ボタンをクリックします。</p>

    <p><img src="console_page24.png" alt="画面ショット" /></p>
  </li>
  <li>
    <p>表示される以下 <strong>イングレス・ルール</strong> フィールドで、変更したルールの <strong>IPプロトコル</strong> が <strong>すべてのプロトコル</strong> に変更されたことを確認します。</p>

    <p><img src="console_page25.png" alt="画面ショット" /></p>
  </li>
</ol>

<h3 id="3-1-3-horovod用dockerコンテナ起動">3-1-3. Horovod用Dockerコンテナ起動</h3>

<p>本章は、2ノードのGPUノード（以降、このうち1台をマスターノード、残りの1台をスレーブノードと呼称。）でHorovod用Dockerコンテナを起動します。</p>

<p>以下コマンドをマスターノードのrootユーザで実行し、マスターノード上でHorovod用Dockerコンテナを起動します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> docker run <span class="nt">-it</span> <span class="nt">--privileged</span> <span class="nt">--rm</span> <span class="nt">--gpus</span> all <span class="nt">--network</span><span class="o">=</span>host <span class="nt">-v</span> /horovod:/root <span class="nt">-v</span> /mnt/localdisk:/scratch horovod/horovod:latest
</code></pre></div></div>

<p>次に、以下コマンドをスレーブノードのrootユーザで実行し、スレーブノード上でポート番号22222でSSH接続を受け付けるHorovod用Dockerコンテナを起動します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> docker run <span class="nt">-it</span> <span class="nt">--privileged</span> <span class="nt">--rm</span> <span class="nt">--gpus</span> all <span class="nt">--network</span><span class="o">=</span>host <span class="nt">-v</span> /horovod:/root <span class="nt">-v</span> /mnt/localdisk:/scratch horovod/horovod:latest bash <span class="nt">-c</span> <span class="s2">"/usr/sbin/sshd -p 22222; bash"</span>
</code></pre></div></div>

<h2 id="3-2-nccl通信性能検証">3-2. NCCL通信性能検証</h2>

<h3 id="3-2-0-概要">3-2-0. 概要</h3>

<p>本章は、NCCL Testsを使用し、GPUクラスタ内のNCCLによるGPU間通信性能を確認します。</p>

<p>ここで使用するNCCLは、Horovod用Dockerコンテナに予め含まれるものを本環境に合うバージョンにアップデートし、NCCL Testsはコンテナ内でソースコードからビルドします。</p>

<p>以上より、本章で実施するNCCL通信性能検証は、以下の手順を経て行います。</p>

<ul>
  <li>NCCLアップデート</li>
  <li>NCCL Testsビルド</li>
  <li>NCCL Tests実行</li>
</ul>

<p>本チュートリアルは、2ノードに跨る全16枚のGPUで全16ポートのRDMAインタフェースを使用したNCCLのAll Reduce通信性能をコンテナ環境から計測し、以下性能が出ています。</p>

<ul>
  <li>帯域（busbw）：約 221 GB/s</li>
</ul>

<h3 id="3-2-1-ncclアップデート">3-2-1. NCCLアップデート</h3>

<p>本章は、Horovod用Dockerコンテナに含まれるNCCLをアップデートします。</p>

<p>マスターノードとスレーブノードのそれぞれで、起動したコンテナ上のrootユーザで、以下のコマンドを実行します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> apt update
<span class="o">&gt;</span> apt <span class="nb">install</span> <span class="nt">-y</span> <span class="nt">--allow-change-held-packages</span> <span class="nv">libnccl2</span><span class="o">=</span>2.15.5-1+cuda11.8 libnccl-dev<span class="o">=</span>2.15.5-1+cuda11.8
</code></pre></div></div>

<h3 id="3-2-2-nccl-testsビルド">3-2-2. NCCL Testsビルド</h3>

<p>本章は、NCCL TestsプログラムをGitHubからダウンロード、ビルドします。</p>

<p>マスターノードとスレーブノードのそれぞれで、起動したコンテナ上のrootユーザで、以下のコマンドを実行します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">cd</span> /root
<span class="o">&gt;</span> git clone https://github.com/NVIDIA/nccl-tests.git
<span class="o">&gt;</span> <span class="nb">cd </span>nccl-tests
<span class="o">&gt;</span> make <span class="nv">MPI</span><span class="o">=</span>1 <span class="nv">MPI_HOME</span><span class="o">=</span>/usr/local <span class="nv">CUDA_HOME</span><span class="o">=</span>/usr/local/cuda <span class="nv">NCCL_HOME</span><span class="o">=</span>/usr/lib/x86_64-linux-gnu
</code></pre></div></div>

<h3 id="3-2-3-nccl-tests実行">3-2-3. NCCL Tests実行</h3>

<p>本章は、NCCL Testsプログラムを実行します。</p>

<p>マスターノードで起動したコンテナ上のrootユーザで以下のコマンドを実行し、マスターノードの8枚のGPUを使用したNCCLのall reduce通信性能を計測します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> ./build/all_reduce_perf <span class="nt">-b</span> 10G <span class="nt">-e</span> 10G <span class="nt">-f</span> 2 <span class="nt">-t</span> 1 <span class="nt">-g</span> 8
<span class="c"># nThread 1 nGpus 8 minBytes 10737418240 maxBytes 10737418240 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0</span>
<span class="c">#</span>
<span class="c"># Using devices</span>
<span class="c">#  Rank  0 Group  0 Pid   1277 on inst-d5ige-comp device  0 [0x0f] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  1 Group  0 Pid   1277 on inst-d5ige-comp device  1 [0x15] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  2 Group  0 Pid   1277 on inst-d5ige-comp device  2 [0x51] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  3 Group  0 Pid   1277 on inst-d5ige-comp device  3 [0x54] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  4 Group  0 Pid   1277 on inst-d5ige-comp device  4 [0x8d] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  5 Group  0 Pid   1277 on inst-d5ige-comp device  5 [0x92] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  6 Group  0 Pid   1277 on inst-d5ige-comp device  6 [0xd6] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  7 Group  0 Pid   1277 on inst-d5ige-comp device  7 [0xda] NVIDIA A100-SXM4-40GB</span>
<span class="c">#</span>
<span class="c">#                                                              out-of-place                       in-place          </span>
<span class="c">#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong</span>
<span class="c">#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       </span>
 10737418240    2684354560     float     <span class="nb">sum</span>      <span class="nt">-1</span>    79751  134.64  235.61      0    79750  134.64  235.62      0
<span class="c"># Out of bounds values : 0 OK</span>
<span class="c"># Avg bus bandwidth    : 235.616 </span>
<span class="c">#</span>
</code></pre></div></div>

<p>次に、マスターノードで起動したコンテナ上のrootユーザで以下のコマンドを実行し、マスターノードとスレーブノードの全16枚のGPUと全16ポートのRDMAインタフェースを使用した、2ノードのGPUノードに跨るNCCLのall reduce通信性能を計測します。ここで、”-H”オプションに指定するマスターノード（inst-d5ige-comp）とスレーブノード（inst-swgen-comp）のホスト名は、自身の環境に合わせて修正します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> mpirun <span class="nt">--allow-run-as-root</span> <span class="nt">-np</span> 16 <span class="nt">-H</span> inst-d5ige-comp:8,inst-swgen-comp:8 <span class="nt">-mca</span> plm_rsh_args <span class="s2">"-p 22222"</span> <span class="nt">--mca</span> btl_tcp_if_exclude docker0,lo <span class="nt">-x</span> <span class="nv">NCCL_IB_QPS_PER_CONNECTION</span><span class="o">=</span>4 <span class="nt">-x</span> <span class="nv">NCCL_IB_GID_INDEX</span><span class="o">=</span>3 <span class="nt">-x</span> <span class="nv">UCX_NET_DEVICES</span><span class="o">=</span>enp45s0f0 <span class="nt">-x</span> <span class="nv">NCCL_IB_HCA</span><span class="o">=</span><span class="s2">"mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_6,mlx5_7,mlx5_8,mlx5_9,mlx5_10,mlx5_11,mlx5_12,mlx5_13,mlx5_14,mlx5_15,mlx5_16,mlx5_17"</span> ./build/all_reduce_perf <span class="nt">-b</span> 10G <span class="nt">-e</span> 10G <span class="nt">-f</span> 2 <span class="nt">-t</span> 1 <span class="nt">-g</span> 1
<span class="c"># nThread 1 nGpus 1 minBytes 10737418240 maxBytes 10737418240 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0</span>
<span class="c">#</span>
<span class="c"># Using devices</span>
<span class="c">#  Rank  0 Group  0 Pid    417 on inst-d5ige-comp device  0 [0x0f] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  1 Group  0 Pid    418 on inst-d5ige-comp device  1 [0x15] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  2 Group  0 Pid    419 on inst-d5ige-comp device  2 [0x51] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  3 Group  0 Pid    420 on inst-d5ige-comp device  3 [0x54] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  4 Group  0 Pid    421 on inst-d5ige-comp device  4 [0x8d] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  5 Group  0 Pid    422 on inst-d5ige-comp device  5 [0x92] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  6 Group  0 Pid    425 on inst-d5ige-comp device  6 [0xd6] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  7 Group  0 Pid    429 on inst-d5ige-comp device  7 [0xda] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  8 Group  0 Pid    371 on inst-swgen-comp device  0 [0x0f] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  9 Group  0 Pid    372 on inst-swgen-comp device  1 [0x15] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 10 Group  0 Pid    373 on inst-swgen-comp device  2 [0x51] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 11 Group  0 Pid    374 on inst-swgen-comp device  3 [0x54] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 12 Group  0 Pid    375 on inst-swgen-comp device  4 [0x8d] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 13 Group  0 Pid    376 on inst-swgen-comp device  5 [0x92] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 14 Group  0 Pid    377 on inst-swgen-comp device  6 [0xd6] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 15 Group  0 Pid    380 on inst-swgen-comp device  7 [0xda] NVIDIA A100-SXM4-40GB</span>
<span class="c">#</span>
<span class="c">#                                                              out-of-place                       in-place          </span>
<span class="c">#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong</span>
<span class="c">#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       </span>
 10737418240    2684354560     float     <span class="nb">sum</span>      <span class="nt">-1</span>    90330  118.87  222.88      0    91945  116.78  218.96      0
<span class="c"># Out of bounds values : 0 OK</span>
<span class="c"># Avg bus bandwidth    : 220.921 </span>
<span class="c">#</span>
</code></pre></div></div>

<h2 id="3-3-horovodサンプルプログラム実行">3-3. Horovodサンプルプログラム実行</h2>

<h3 id="3-3-0-概要">3-3-0. 概要</h3>

<p>本章は、Horovodサンプルプログラムを使用し、構築したGPUクラスタで分散機械学習プログラムを実行します。</p>

<p>ここで使用するHorovodサンプルプログラムは、Horovod用Dockerコンテナに予め含まれる、TensorFlow 2でダミーデータを用いてResNet-50モデルを訓練するベンチマークプログラムです。</p>

<h3 id="3-3-1-horovodサンプルプログラム実行">3-3-1. Horovodサンプルプログラム実行</h3>

<p>本章は、Horovodサンプルプログラムを実行します。</p>

<p>マスターノードで起動したコンテナ上のrootユーザで以下のコマンドを実行し、マスターノードの8枚のGPUを使用してHorovodサンプルプログラムを実行します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">cd</span> /horovod/examples/
<span class="o">&gt;</span> horovodrun <span class="nt">-np</span> 8 <span class="nt">-H</span> localhost:8 python tensorflow2/tensorflow2_synthetic_benchmark.py
   :
<span class="o">[</span>1,0]&lt;stdout&gt;:Model: ResNet50
<span class="o">[</span>1,0]&lt;stdout&gt;:Batch size: 32
<span class="o">[</span>1,0]&lt;stdout&gt;:Number of GPUs: 8
<span class="o">[</span>1,0]&lt;stdout&gt;:Running warmup...
   :
<span class="o">[</span>1,0]&lt;stdout&gt;:Running benchmark...
<span class="o">[</span>1,0]&lt;stdout&gt;:Iter <span class="c">#0: 592.3 img/sec per GPU</span>
<span class="o">[</span>1,0]&lt;stdout&gt;:Iter <span class="c">#1: 599.6 img/sec per GPU</span>
<span class="o">[</span>1,0]&lt;stdout&gt;:Iter <span class="c">#2: 600.7 img/sec per GPU</span>
<span class="o">[</span>1,0]&lt;stdout&gt;:Iter <span class="c">#3: 600.2 img/sec per GPU</span>
<span class="o">[</span>1,0]&lt;stdout&gt;:Iter <span class="c">#4: 601.0 img/sec per GPU</span>
<span class="o">[</span>1,0]&lt;stdout&gt;:Iter <span class="c">#5: 601.3 img/sec per GPU</span>
<span class="o">[</span>1,0]&lt;stdout&gt;:Iter <span class="c">#6: 601.8 img/sec per GPU</span>
<span class="o">[</span>1,0]&lt;stdout&gt;:Iter <span class="c">#7: 601.0 img/sec per GPU</span>
<span class="o">[</span>1,0]&lt;stdout&gt;:Iter <span class="c">#8: 602.2 img/sec per GPU</span>
<span class="o">[</span>1,0]&lt;stdout&gt;:Iter <span class="c">#9: 601.7 img/sec per GPU</span>
<span class="o">[</span>1,0]&lt;stdout&gt;:Img/sec per GPU: 600.2 +-5.3
<span class="o">[</span>1,0]&lt;stdout&gt;:Total img/sec on 8 GPU<span class="o">(</span>s<span class="o">)</span>: 4801.4 +-42.7
</code></pre></div></div>

<p>最後の行に出力される実行結果から、8枚のGPUを使用した実行時のスコアが4,800程度であることを確認します。</p>

<p>次に、マスターノードで起動したコンテナ上のrootユーザで以下のコマンドを実行し、マスターノードとスレーブノードの全16枚のGPUを使用して、2ノードのGPUノードに跨ってHorovodサンプルプログラムを実行します。ここで、”-H”オプションに指定するマスターノード（inst-d5ige-comp）とスレーブノード（inst-swgen-comp）のホスト名は、自身の環境に合わせて修正します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> mpirun <span class="nt">--allow-run-as-root</span> <span class="nt">-np</span> 16 <span class="nt">-H</span> inst-d5ige-comp:8,inst-swgen-comp:8 <span class="nt">-mca</span> plm_rsh_args <span class="s2">"-p 22222"</span> <span class="nt">--mca</span> btl_tcp_if_exclude docker0,lo <span class="nt">-x</span> <span class="nv">NCCL_IB_QPS_PER_CONNECTION</span><span class="o">=</span>4 <span class="nt">-x</span> <span class="nv">NCCL_IB_GID_INDEX</span><span class="o">=</span>3 <span class="nt">-x</span> <span class="nv">UCX_NET_DEVICES</span><span class="o">=</span>enp45s0f0 <span class="nt">-x</span> <span class="nv">NCCL_IB_HCA</span><span class="o">=</span><span class="s2">"mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_6,mlx5_7,mlx5_8,mlx5_9,mlx5_10,mlx5_11,mlx5_12,mlx5_13,mlx5_14,mlx5_15,mlx5_16,mlx5_17"</span> python tensorflow2/tensorflow2_synthetic_benchmark.py
   :
Model: ResNet50
Batch size: 32
Number of GPUs: 16
Running warmup...
   :
Running benchmark...
Iter <span class="c">#0: 577.9 img/sec per GPU</span>
Iter <span class="c">#1: 578.8 img/sec per GPU</span>
Iter <span class="c">#2: 577.8 img/sec per GPU</span>
Iter <span class="c">#3: 580.1 img/sec per GPU</span>
Iter <span class="c">#4: 579.2 img/sec per GPU</span>
Iter <span class="c">#5: 580.7 img/sec per GPU</span>
Iter <span class="c">#6: 581.0 img/sec per GPU</span>
Iter <span class="c">#7: 581.6 img/sec per GPU</span>
Iter <span class="c">#8: 581.2 img/sec per GPU</span>
Iter <span class="c">#9: 581.6 img/sec per GPU</span>
Img/sec per GPU: 580.0 +-2.8
Total img/sec on 16 GPU<span class="o">(</span>s<span class="o">)</span>: 9279.7 +-44.2
</code></pre></div></div>

<p>最後の行に出力される実行結果から、2ノード16枚のGPUを使用した実行時のスコアが9,300程度で、先の1ノード8枚のGPUで実行したスコアからほぼリニアにスケールしていることを確認します。</p>

<hr />
<h1 id="4-multiworkermirroredstrategyを使用するgpuクラスタ環境構築">4. MultiWorkerMirroredStrategyを使用するGPUクラスタ環境構築</h1>

<h2 id="4-1-dockerコンテナ環境構築">4-1. Dockerコンテナ環境構築</h2>

<h3 id="4-1-0-概要">4-1-0. 概要</h3>

<p>本章は、後の章で実行するNCCL TestsとMultiWorkerMirroredStrategyのサンプルプログラムを実行するDockerコンテナを起動するため、必要な環境構築作業を行います。</p>

<p>NCCL Testsは、コンテナを跨るプログラム実行のコントローラとしてMPIを使用します。ここで使用するMPIは、Dockerコンテナに予め含まれる、OpenMPIです。</p>

<p>OpenMPIをコンテナ間で実行するためには、MPIプログラムをmpirun等で起動するコンテナ（いわゆるヘッドノード）からMPIプログラム実行に参加する他の全てのコンテナにパスフレーズ無しでSSH接続できる必要があります。</p>

<p>またここで使用するDockerコンテナは、sshdがインストールされていないため、ヘッドノード以外のコンテナでこれをインストールする必要があります。</p>

<p>またOpenMPIの実行は、これを実行するコンテナ間で必要なポートにアクセス出来る必要があるため、GPUノードが接続されるプライベートサブネットのセキュリティリストを修正する必要があります。</p>

<p>以上より、本章で実施するDockerコンテナ環境構築は、以下の手順を経て行います。</p>

<ul>
  <li>コンテナ間SSH接続環境構築</li>
  <li>プライベートサブネットセキュリティリスト修正</li>
  <li>Dockerコンテナ起動</li>
  <li>sshdインストール・起動</li>
</ul>

<h3 id="4-1-1-コンテナ間ssh接続環境構築">4-1-1. コンテナ間SSH接続環境構築</h3>

<p>本章は、先にBastionノードで作成したSSH秘密鍵を含む.sshディレクトリをGPUノードにコピーし、後のコンテナ起動時にこのディレクトリをコンテナにマウントすることで、コンテナ間のパスフレーズ無しSSH接続環境を実現します。</p>

<p>まず初めに、テクニカルTips <strong><a href="/ocitutorials/hpc/tech-knowhow/compute-host-list/">計算ノードのホスト名リスト作成方法</a></strong> の手順を実施し、以下のように全てのGPUノードのホスト名を含むホスト名リストをBastionノード上にファイル名 <strong>hostlist.txt</strong> で作成します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>inst-d5ige-comp
inst-swgen-comp
</code></pre></div></div>

<p>次にこのホスト名リストを使用し、Bastionノードのopcユーザで以下コマンドを実行、全GPUノードのホストキーを含むknown_hostsファイルを作成します。この際、GPUノード毎に接続確認を求められるため、全てに <strong>yes</strong> を入力します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="k">for </span>hname <span class="k">in</span> <span class="sb">`</span><span class="nb">cat </span>hostlist.txt<span class="sb">`</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="nv">$hname</span><span class="p">;</span> ssh <span class="nv">$hname</span> <span class="nb">hostname</span><span class="p">;</span> <span class="k">done
</span>inst-d5ige-comp
inst-d5ige-comp
inst-swgen-comp
The authenticity of host <span class="s1">'inst-swgen-comp (10.0.2.242)'</span> cannot be established.
ECDSA key fingerprint is SHA256:koWs+bKLzf78RQsZ+mQhvwxEQTu/72St2DiVyVqmtl4.
ECDSA key fingerprint is MD5:68:b4:e0:37:f1:9a:36:c7:a5:3c:69:9f:91:d5:e0:34.
Are you sure you want to <span class="k">continue </span>connecting <span class="o">(</span><span class="nb">yes</span>/no<span class="o">)</span>? <span class="nb">yes
</span>Warning: Permanently added <span class="s1">'inst-swgen-comp,10.0.2.242'</span> <span class="o">(</span>ECDSA<span class="o">)</span> to the list of known hosts.
inst-swgen-comp
</code></pre></div></div>

<p>次に、Bastionノードのopcユーザで以下コマンドを実行、Bastionノードで作成した秘密鍵を使ったSSHログインを許可します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">cat</span> ~/.ssh/id_rsa.pub <span class="o">&gt;&gt;</span> ~/.ssh/authorized_keys
</code></pre></div></div>

<p>次に、Bastionノードのopcユーザで以下コマンドを実行、~opc/.sshディレクトリをアーカイブしてこれを全GPUノードにコピーします。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">cd</span> ~
<span class="o">&gt;</span> <span class="nb">tar</span> <span class="nt">-cvf</span> /tmp/ssh.tar ./.ssh
./.ssh/
./.ssh/id_rsa
./.ssh/known_hosts
./.ssh/id_rsa.pub
./.ssh/authorized_keys
<span class="o">&gt;</span> <span class="k">for </span>hname <span class="k">in</span> <span class="sb">`</span><span class="nb">cat </span>hostlist.txt<span class="sb">`</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="nv">$hname</span><span class="p">;</span> scp /tmp/ssh.tar <span class="nv">$hname</span>:/tmp/<span class="p">;</span> <span class="k">done
</span>inst-d5ige-comp
ssh.tar                                                                100%   10KB   9.0MB/s   00:00    
inst-swgen-comp
ssh.tar                                                                100%   10KB  11.5MB/s   00:00
</code></pre></div></div>

<p>次に、Bastionノードのopcユーザで以下コマンドを実行、先のアーカイブをGPUノードの/TFディレクトリに展開します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="k">for </span>hname <span class="k">in</span> <span class="sb">`</span><span class="nb">cat </span>hostlist.txt<span class="sb">`</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="nv">$hname</span><span class="p">;</span> ssh <span class="nv">$hname</span> <span class="s2">"sudo mkdir /TF"</span><span class="p">;</span> <span class="k">done
</span>inst-d5ige-comp
inst-swgen-comp
<span class="o">&gt;</span> <span class="k">for </span>hname <span class="k">in</span> <span class="sb">`</span><span class="nb">cat </span>hostlist.txt<span class="sb">`</span><span class="p">;</span> <span class="k">do </span><span class="nb">echo</span> <span class="nv">$hname</span><span class="p">;</span> ssh <span class="nv">$hname</span> <span class="s2">"sudo tar --no-same-owner -xvf /tmp/ssh.tar -C /TF/"</span><span class="p">;</span> <span class="k">done
</span>inst-d5ige-comp
./.ssh/
./.ssh/id_rsa
./.ssh/id_rsa.pub
./.ssh/authorized_keys
./.ssh/known_hosts
inst-swgen-comp
./.ssh/
./.ssh/id_rsa
./.ssh/id_rsa.pub
./.ssh/authorized_keys
./.ssh/known_hosts
</code></pre></div></div>

<h3 id="4-1-2-プライベートサブネットセキュリティリスト修正">4-1-2. プライベートサブネットセキュリティリスト修正</h3>

<p>本章は、プライベートサブネットのセキュリティリストを以下の手順で修正します。</p>

<ol>
  <li>
    <p>OCIコンソールにログインし、GPUノードをデプロイしたリージョンを選択後、 <strong>ネットワーキング</strong> → <strong>仮想クラウド・ネットワーク</strong> とメニューを辿ります。</p>
  </li>
  <li>
    <p>表示される画面で、先に作成した仮想クラウド・ネットワークをクリックします。</p>
  </li>
  <li>
    <p>表示される以下 <strong>サブネット</strong> フィールドで、先に作成したプライベートサブネットをクリックします。</p>

    <p><img src="console_page21.png" alt="画面ショット" /></p>
  </li>
  <li>
    <p>表示される以下 <strong>セキュリティ・リスト</strong> フィールドで、プライベートサブネットに適用されているセキュリティリストをクリックします。</p>

    <p><img src="console_page22.png" alt="画面ショット" /></p>
  </li>
  <li>
    <p>表示される以下 <strong>イングレス・ルール</strong> フィールドで、SSHアクセスを許可しているルールの <strong>編集</strong> メニューをクリックします。</p>

    <p><img src="console_page23.png" alt="画面ショット" /></p>
  </li>
  <li>
    <p>表示される以下 <strong>イングレス・ルールの編集</strong> サイドバーで、 <strong>IPプロトコル</strong> フィールドを <strong>すべてのプロトコル</strong> に変更し、 <strong>変更の保存</strong> ボタンをクリックします。</p>

    <p><img src="console_page24.png" alt="画面ショット" /></p>
  </li>
  <li>
    <p>表示される以下 <strong>イングレス・ルール</strong> フィールドで、変更したルールの <strong>IPプロトコル</strong> が <strong>すべてのプロトコル</strong> に変更されたことを確認します。</p>

    <p><img src="console_page25.png" alt="画面ショット" /></p>
  </li>
</ol>

<h3 id="4-1-3-dockerコンテナ起動">4-1-3. Dockerコンテナ起動</h3>

<p>本章は、2ノードのGPUノード（以降、このうち1台をマスターノード、残りの1台をスレーブノードと呼称。）でDockerコンテナを起動します。</p>

<p>以下コマンドをマスターノードとスレーブノードのrootユーザでそれぞれ実行し、Dockerコンテナを起動します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> docker run <span class="nt">-it</span> <span class="nt">--privileged</span> <span class="nt">--rm</span> <span class="nt">--gpus</span> all <span class="nt">--network</span><span class="o">=</span>host <span class="nt">--ipc</span><span class="o">=</span>host <span class="nt">--ulimit</span> <span class="nv">memlock</span><span class="o">=</span><span class="nt">-1</span> <span class="nt">--ulimit</span> <span class="nv">stack</span><span class="o">=</span>67108864 <span class="nt">-v</span> /TF:/root <span class="nt">-v</span> /mnt/localdisk:/scratch nvcr.io/nvidia/tensorflow:22.11-tf2-py3
</code></pre></div></div>

<h3 id="4-1-4-sshdインストール起動">4-1-4. sshdインストール・起動</h3>

<p>本章は、マスターノードで実行するmpirunによるSSH接続をスレーブノードで受け付けるため、スレーブノードでsshdをインストールし、このsshdをポート番号22222で起動します。</p>

<p>以下コマンドをスレーブノードで起動したコンテナ上のrootユーザで実行し、sshdをインストール・起動します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> apt update
<span class="o">&gt;</span> apt <span class="nb">install</span> <span class="nt">-y</span> openssh-server
<span class="o">&gt;</span> <span class="nb">mkdir</span> /run/sshd
<span class="o">&gt;</span> /usr/sbin/sshd <span class="nt">-p</span> 22222
</code></pre></div></div>

<p>次に、以下コマンドをマスターノードで起動したコンテナ上のrootユーザで実行し、スレーブノードにSSH接続できることを確認します。この際、接続を継続するかどうかの問いに”yes”と入力し、known_hostsにスレーブノードのホストキーを登録します。これは、この後のmpirunを実行するために必要です。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> ssh <span class="nt">-p</span> 22222 inst-swgen-comp <span class="nb">hostname
</span>The authenticity of host <span class="s1">'[inst-swgen-comp]:22222 ([10.0.2.242]:22222)'</span> cannot be established.
ECDSA key fingerprint is SHA256:KiuvF9QMILkeDOJnr3RFcteAGbTVCkxBZ4gBEJgNqYE.
Are you sure you want to <span class="k">continue </span>connecting <span class="o">(</span><span class="nb">yes</span>/no/[fingerprint]<span class="o">)</span>? <span class="nb">yes
</span>Warning: Permanently added <span class="s1">'[inst-swgen-comp]:22222,[10.0.2.242]:22222'</span> <span class="o">(</span>ECDSA<span class="o">)</span> to the list of known hosts.
inst-swgen-comp
</code></pre></div></div>

<h2 id="4-2-nccl通信性能検証">4-2. NCCL通信性能検証</h2>

<h3 id="4-2-0-概要">4-2-0. 概要</h3>

<p>本章は、NCCL Testsを使用し、GPUクラスタ内のNCCLによるGPU間通信性能を確認します。</p>

<p>ここで使用するNCCLは、起動したDockerコンテナに予め含まれますが、NCCL Testsはコンテナ内でソースコードからビルドします。</p>

<p>以上より、本章で実施するNCCL通信性能検証は、以下の手順を経て行います。</p>

<ul>
  <li>NCCL Testsビルド</li>
  <li>NCCL Tests実行</li>
</ul>

<p>本チュートリアルは、2ノードに跨る全16枚のGPUで全16ポートのRDMAインタフェースを使用したNCCLのAll Reduce通信性能をコンテナ環境から計測し、以下性能が出ています。</p>

<ul>
  <li>帯域（busbw）：約 172 GB/s</li>
</ul>

<h3 id="4-2-1-nccl-testsビルド">4-2-1. NCCL Testsビルド</h3>

<p>本章は、NCCL TestsプログラムをGitHubからダウンロード、ビルドします。</p>

<p>マスターノードとスレーブノードのそれぞれで、起動したコンテナ上のrootユーザで、以下のコマンドを実行します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">cd</span> /root
<span class="o">&gt;</span> git clone https://github.com/NVIDIA/nccl-tests.git
<span class="o">&gt;</span> <span class="nb">cd </span>nccl-tests
<span class="o">&gt;</span> make <span class="nv">MPI</span><span class="o">=</span>1 <span class="nv">MPI_HOME</span><span class="o">=</span>/usr/local/mpi <span class="nv">CUDA_HOME</span><span class="o">=</span>/usr/local/cuda <span class="nv">NCCL_HOME</span><span class="o">=</span>/usr/lib/x86_64-linux-gnu
</code></pre></div></div>

<h3 id="4-2-2-nccl-tests実行">4-2-2. NCCL Tests実行</h3>

<p>本章は、NCCL Testsプログラムを実行します。</p>

<p>マスターノードで起動したコンテナ上のrootユーザで以下のコマンドを実行し、マスターノードの8枚のGPUを使用したNCCLのall reduce通信性能を計測します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> ./build/all_reduce_perf <span class="nt">-b</span> 10G <span class="nt">-e</span> 10G <span class="nt">-f</span> 2 <span class="nt">-t</span> 1 <span class="nt">-g</span> 8
<span class="c"># nThread 1 nGpus 8 minBytes 10737418240 maxBytes 10737418240 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0</span>
<span class="c">#</span>
<span class="c"># Using devices</span>
<span class="c">#  Rank  0 Group  0 Pid   1179 on inst-d5ige-comp device  0 [0x0f] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  1 Group  0 Pid   1179 on inst-d5ige-comp device  1 [0x15] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  2 Group  0 Pid   1179 on inst-d5ige-comp device  2 [0x51] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  3 Group  0 Pid   1179 on inst-d5ige-comp device  3 [0x54] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  4 Group  0 Pid   1179 on inst-d5ige-comp device  4 [0x8d] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  5 Group  0 Pid   1179 on inst-d5ige-comp device  5 [0x92] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  6 Group  0 Pid   1179 on inst-d5ige-comp device  6 [0xd6] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  7 Group  0 Pid   1179 on inst-d5ige-comp device  7 [0xda] NVIDIA A100-SXM4-40GB</span>
<span class="c">#</span>
<span class="c">#                                                              out-of-place                       in-place          </span>
<span class="c">#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong</span>
<span class="c">#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       </span>
 10737418240    2684354560     float     <span class="nb">sum</span>      <span class="nt">-1</span>    79783  134.58  235.52      0    79754  134.63  235.61      0
<span class="c"># Out of bounds values : 0 OK</span>
<span class="c"># Avg bus bandwidth    : 235.563 </span>
<span class="c">#</span>
</code></pre></div></div>

<p>次に、マスターノードで起動したコンテナ上のrootユーザで以下のコマンドを実行し、マスターノードとスレーブノードの全16枚のGPUと全16ポートのRDMAインタフェースを使用した、2ノードのGPUノードに跨るNCCLのall reduce通信性能を計測します。ここで、”-H”オプションに指定するマスターノード（inst-d5ige-comp）とスレーブノード（inst-swgen-comp）のホスト名は、自身の環境に合わせて修正します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> mpirun <span class="nt">--allow-run-as-root</span> <span class="nt">-np</span> 16 <span class="nt">-H</span> inst-d5ige-comp:8,inst-swgen-comp:8 <span class="nt">-mca</span> plm_rsh_args <span class="s2">"-p 22222"</span> <span class="nt">--mca</span> btl_tcp_if_exclude docker0,lo <span class="nt">-x</span> <span class="nv">NCCL_IB_QPS_PER_CONNECTION</span><span class="o">=</span>4 <span class="nt">-x</span> <span class="nv">NCCL_IB_GID_INDEX</span><span class="o">=</span>3 <span class="nt">-x</span> <span class="nv">UCX_NET_DEVICES</span><span class="o">=</span>enp45s0f0 <span class="nt">-x</span> <span class="nv">NCCL_IB_HCA</span><span class="o">=</span><span class="s2">"mlx5_0,mlx5_1,mlx5_2,mlx5_3,mlx5_6,mlx5_7,mlx5_8,mlx5_9,mlx5_10,mlx5_11,mlx5_12,mlx5_13,mlx5_14,mlx5_15,mlx5_16,mlx5_17"</span> ./build/all_reduce_perf <span class="nt">-b</span> 10G <span class="nt">-e</span> 10G <span class="nt">-f</span> 2 <span class="nt">-t</span> 1 <span class="nt">-g</span> 1
<span class="c"># nThread 1 nGpus 1 minBytes 10737418240 maxBytes 10737418240 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0</span>
<span class="c">#</span>
<span class="c"># Using devices</span>
<span class="c">#  Rank  0 Group  0 Pid    417 on inst-d5ige-comp device  0 [0x0f] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  1 Group  0 Pid    418 on inst-d5ige-comp device  1 [0x15] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  2 Group  0 Pid    419 on inst-d5ige-comp device  2 [0x51] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  3 Group  0 Pid    420 on inst-d5ige-comp device  3 [0x54] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  4 Group  0 Pid    421 on inst-d5ige-comp device  4 [0x8d] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  5 Group  0 Pid    422 on inst-d5ige-comp device  5 [0x92] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  6 Group  0 Pid    425 on inst-d5ige-comp device  6 [0xd6] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  7 Group  0 Pid    429 on inst-d5ige-comp device  7 [0xda] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  8 Group  0 Pid    371 on inst-swgen-comp device  0 [0x0f] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank  9 Group  0 Pid    372 on inst-swgen-comp device  1 [0x15] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 10 Group  0 Pid    373 on inst-swgen-comp device  2 [0x51] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 11 Group  0 Pid    374 on inst-swgen-comp device  3 [0x54] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 12 Group  0 Pid    375 on inst-swgen-comp device  4 [0x8d] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 13 Group  0 Pid    376 on inst-swgen-comp device  5 [0x92] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 14 Group  0 Pid    377 on inst-swgen-comp device  6 [0xd6] NVIDIA A100-SXM4-40GB</span>
<span class="c">#  Rank 15 Group  0 Pid    380 on inst-swgen-comp device  7 [0xda] NVIDIA A100-SXM4-40GB</span>
<span class="c">#</span>
<span class="c">#                                                              out-of-place                       in-place          </span>
<span class="c">#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong</span>
<span class="c">#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       </span>
 10737418240    2684354560     float     <span class="nb">sum</span>      <span class="nt">-1</span>   118569   90.56  169.80      0   115501   92.96  174.31      0
<span class="c"># Out of bounds values : 0 OK</span>
<span class="c"># Avg bus bandwidth    : 172.052 </span>
<span class="c">#</span>
</code></pre></div></div>

<h2 id="4-3-multiworkermirroredstrategyサンプルプログラム実行">4-3. MultiWorkerMirroredStrategyサンプルプログラム実行</h2>

<h3 id="4-3-0-概要">4-3-0. 概要</h3>

<p>本章は、MultiWorkerMirroredStrategyサンプルプログラムを使用し、構築したGPUクラスタで分散機械学習プログラムを実行します。</p>

<p>ここで使用するMultiWorkerMirroredStrategyサンプルプログラムは、以下TensorFlow公式ドキュメントページのチュートリアルで使用されている、MNISTデータセットを使用した訓練を行うプログラムです。</p>

<p><a href="https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras">https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras</a></p>

<h3 id="4-3-1-multiworkermirroredstrategyサンプルプログラム作成">4-3-1. MultiWorkerMirroredStrategyサンプルプログラム作成</h3>

<p>本章は、MultiWorkerMirroredStrategyサンプルプログラムを作成します。</p>

<p>マスターノードとスレーブノードで起動した双方のコンテナ上のrootユーザで、以下のプログラムを作成します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">cd</span> /root
<span class="o">&gt;</span> <span class="nb">cat </span>mnist.py
import os
import json
import tensorflow as tf
import numpy as np

def mnist_dataset<span class="o">(</span>batch_size<span class="o">)</span>:
  <span class="o">(</span>x_train, y_train<span class="o">)</span>, _ <span class="o">=</span> tf.keras.datasets.mnist.load_data<span class="o">()</span>
  x_train <span class="o">=</span> x_train / np.float32<span class="o">(</span>255<span class="o">)</span>
  y_train <span class="o">=</span> y_train.astype<span class="o">(</span>np.int64<span class="o">)</span>
  train_dataset <span class="o">=</span> tf.data.Dataset.from_tensor_slices<span class="o">(</span>
      <span class="o">(</span>x_train, y_train<span class="o">))</span>.shuffle<span class="o">(</span>60000<span class="o">)</span>.repeat<span class="o">()</span>.batch<span class="o">(</span>batch_size<span class="o">)</span>
  <span class="k">return </span>train_dataset

def build_and_compile_cnn_model<span class="o">()</span>:
  model <span class="o">=</span> tf.keras.Sequential<span class="o">([</span>
      tf.keras.layers.InputLayer<span class="o">(</span><span class="nv">input_shape</span><span class="o">=(</span>28, 28<span class="o">))</span>,
      tf.keras.layers.Reshape<span class="o">(</span><span class="nv">target_shape</span><span class="o">=(</span>28, 28, 1<span class="o">))</span>,
      tf.keras.layers.Conv2D<span class="o">(</span>32, 3, <span class="nv">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="o">)</span>,
      tf.keras.layers.Flatten<span class="o">()</span>,
      tf.keras.layers.Dense<span class="o">(</span>128, <span class="nv">activation</span><span class="o">=</span><span class="s1">'relu'</span><span class="o">)</span>,
      tf.keras.layers.Dense<span class="o">(</span>10<span class="o">)</span>
  <span class="o">])</span>
  model.compile<span class="o">(</span>
      <span class="nv">loss</span><span class="o">=</span>tf.keras.losses.SparseCategoricalCrossentropy<span class="o">(</span><span class="nv">from_logits</span><span class="o">=</span>True<span class="o">)</span>,
      <span class="nv">optimizer</span><span class="o">=</span>tf.keras.optimizers.SGD<span class="o">(</span><span class="nv">learning_rate</span><span class="o">=</span>0.001<span class="o">)</span>,
      <span class="nv">metrics</span><span class="o">=[</span><span class="s1">'accuracy'</span><span class="o">])</span>
  <span class="k">return </span>model

per_worker_batch_size <span class="o">=</span> 64
tf_config <span class="o">=</span> json.loads<span class="o">(</span>os.environ[<span class="s1">'TF_CONFIG'</span><span class="o">])</span>
num_workers <span class="o">=</span> len<span class="o">(</span>tf_config[<span class="s1">'cluster'</span><span class="o">][</span><span class="s1">'worker'</span><span class="o">])</span>

strategy <span class="o">=</span> tf.distribute.MultiWorkerMirroredStrategy<span class="o">()</span>

global_batch_size <span class="o">=</span> per_worker_batch_size <span class="k">*</span> num_workers
multi_worker_dataset <span class="o">=</span> mnist_dataset<span class="o">(</span>global_batch_size<span class="o">)</span>

with strategy.scope<span class="o">()</span>:
  multi_worker_model <span class="o">=</span> build_and_compile_cnn_model<span class="o">()</span>

multi_worker_model.fit<span class="o">(</span>multi_worker_dataset, <span class="nv">epochs</span><span class="o">=</span>3, <span class="nv">steps_per_epoch</span><span class="o">=</span>70<span class="o">)</span>
</code></pre></div></div>

<h3 id="4-3-2-multiworkermirroredstrategyサンプルプログラム実行">4-3-2. MultiWorkerMirroredStrategyサンプルプログラム実行</h3>

<p>本章は、MultiWorkerMirroredStrategyサンプルプログラムを実行します。</p>

<p>マスターノードで起動したコンテナ上のrootユーザで、以下コマンドを実行しTF_CONFIG環境変数を設定・確認します。ここで、”worker”セクションに指定するマスターノード（inst-d5ige-comp）とスレーブノード（inst-swgen-comp）のホスト名は、自身の環境に合わせて修正します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">export </span><span class="nv">TF_CONFIG</span><span class="o">=</span><span class="s2">"{</span><span class="se">\"</span><span class="s2">cluster</span><span class="se">\"</span><span class="s2">: {</span><span class="se">\"</span><span class="s2">worker</span><span class="se">\"</span><span class="s2">: [</span><span class="se">\"</span><span class="s2">inst-d5ige-comp:12345</span><span class="se">\"</span><span class="s2">, </span><span class="se">\"</span><span class="s2">inst-swgen-comp:23456</span><span class="se">\"</span><span class="s2">]}, </span><span class="se">\"</span><span class="s2">task</span><span class="se">\"</span><span class="s2">: {</span><span class="se">\"</span><span class="s2">type</span><span class="se">\"</span><span class="s2">: </span><span class="se">\"</span><span class="s2">worker</span><span class="se">\"</span><span class="s2">, </span><span class="se">\"</span><span class="s2">index</span><span class="se">\"</span><span class="s2">: 0}}"</span>
<span class="o">&gt;</span> <span class="nb">echo</span> <span class="nv">$TF_CONFIG</span>
<span class="o">{</span><span class="s2">"cluster"</span>: <span class="o">{</span><span class="s2">"worker"</span>: <span class="o">[</span><span class="s2">"inst-d5ige-comp:12345"</span>, <span class="s2">"inst-swgen-comp:23456"</span><span class="o">]}</span>, <span class="s2">"task"</span>: <span class="o">{</span><span class="s2">"type"</span>: <span class="s2">"worker"</span>, <span class="s2">"index"</span>: 0<span class="o">}}</span>
</code></pre></div></div>

<p>次に、マスターノードで起動したコンテナ上のrootユーザで、以下コマンドを実行しプログラムを実行します。この時点では、スレーブノードの実行を待っている状態で、以下の出力で停止します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> python mnist.py
   :
2022-12-15 08:48:49.404772: I tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:281] Coordination agent has successfully connected.
</code></pre></div></div>

<p>次に、スレーブノードで起動したコンテナ上のrootユーザで、以下のコマンドを実行しTF_CONFIG環境変数を設定・確認します。ここで、”worker”セクションに指定するマスターノード（inst-d5ige-comp）とスレーブノード（inst-swgen-comp）のホスト名は、自身の環境に合わせて修正します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="nb">export </span><span class="nv">TF_CONFIG</span><span class="o">=</span><span class="s2">"{</span><span class="se">\"</span><span class="s2">cluster</span><span class="se">\"</span><span class="s2">: {</span><span class="se">\"</span><span class="s2">worker</span><span class="se">\"</span><span class="s2">: [</span><span class="se">\"</span><span class="s2">inst-d5ige-comp:12345</span><span class="se">\"</span><span class="s2">, </span><span class="se">\"</span><span class="s2">inst-swgen-comp:23456</span><span class="se">\"</span><span class="s2">]}, </span><span class="se">\"</span><span class="s2">task</span><span class="se">\"</span><span class="s2">: {</span><span class="se">\"</span><span class="s2">type</span><span class="se">\"</span><span class="s2">: </span><span class="se">\"</span><span class="s2">worker</span><span class="se">\"</span><span class="s2">, </span><span class="se">\"</span><span class="s2">index</span><span class="se">\"</span><span class="s2">: 1}}"</span>
<span class="o">&gt;</span> <span class="nb">echo</span> <span class="nv">$TF_CONFIG</span>
<span class="o">{</span><span class="s2">"cluster"</span>: <span class="o">{</span><span class="s2">"worker"</span>: <span class="o">[</span><span class="s2">"inst-d5ige-comp:12345"</span>, <span class="s2">"inst-swgen-comp:23456"</span><span class="o">]}</span>, <span class="s2">"task"</span>: <span class="o">{</span><span class="s2">"type"</span>: <span class="s2">"worker"</span>, <span class="s2">"index"</span>: 1<span class="o">}}</span>
</code></pre></div></div>

<p>次に、スレーブノードで起動したコンテナ上のrootユーザで、以下のコマンドを実行します。これにより、待機していたマスターノードのワーカーとスレーブノードのワーカーが処理を開始、MultiWorkerMirroredStrategyサンプルプログラムが全16枚のGPUを使用してプログラムを実行します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> python mnist.py
2022-12-19 09:38:23.383751: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library <span class="o">(</span>oneDNN<span class="o">)</span> to use the following CPU instructions <span class="k">in </span>performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To <span class="nb">enable </span>them <span class="k">in </span>other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-19 09:38:26.455413: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library <span class="o">(</span>oneDNN<span class="o">)</span> to use the following CPU instructions <span class="k">in </span>performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To <span class="nb">enable </span>them <span class="k">in </span>other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-12-19 09:38:27.647393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38139 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus <span class="nb">id</span>: 0000:0f:00.0, compute capability: 8.0
2022-12-19 09:38:27.649632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38139 MB memory:  -&gt; device: 1, name: NVIDIA A100-SXM4-40GB, pci bus <span class="nb">id</span>: 0000:15:00.0, compute capability: 8.0
2022-12-19 09:38:27.651996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38139 MB memory:  -&gt; device: 2, name: NVIDIA A100-SXM4-40GB, pci bus <span class="nb">id</span>: 0000:51:00.0, compute capability: 8.0
2022-12-19 09:38:27.654473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38139 MB memory:  -&gt; device: 3, name: NVIDIA A100-SXM4-40GB, pci bus <span class="nb">id</span>: 0000:54:00.0, compute capability: 8.0
2022-12-19 09:38:27.656586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 38139 MB memory:  -&gt; device: 4, name: NVIDIA A100-SXM4-40GB, pci bus <span class="nb">id</span>: 0000:8d:00.0, compute capability: 8.0
2022-12-19 09:38:27.658573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 38139 MB memory:  -&gt; device: 5, name: NVIDIA A100-SXM4-40GB, pci bus <span class="nb">id</span>: 0000:92:00.0, compute capability: 8.0
2022-12-19 09:38:27.660481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 38139 MB memory:  -&gt; device: 6, name: NVIDIA A100-SXM4-40GB, pci bus <span class="nb">id</span>: 0000:d6:00.0, compute capability: 8.0
2022-12-19 09:38:27.662428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 38139 MB memory:  -&gt; device: 7, name: NVIDIA A100-SXM4-40GB, pci bus <span class="nb">id</span>: 0000:da:00.0, compute capability: 8.0
   :
Epoch 1/3
   :
70/70 <span class="o">[==============================]</span> - 7s 30ms/step - loss: 2.2694 - accuracy: 0.1422
Epoch 2/3
70/70 <span class="o">[==============================]</span> - 2s 29ms/step - loss: 2.2036 - accuracy: 0.3489
Epoch 3/3
70/70 <span class="o">[==============================]</span> - 2s 28ms/step - loss: 2.1268 - accuracy: 0.5488
</code></pre></div></div>

<hr />
<h1 id="5-gpuクラスタの削除">5. GPUクラスタの削除</h1>

<p>本章は、 <strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong> を終了することで、作成したクラスタ・ネットワークとGPUノードを削除します。</p>

<ol>
  <li>
    <p>OCIコンソールメニューから <strong>コンピュート</strong> → <strong>クラスタ・ネットワーク</strong> を選択し、表示される以下画面で作成したクラスタ・ネットワークの <strong>終了</strong> メニューをクリックします。</p>

    <p><img src="console_page26.png" alt="画面ショット" /></p>
  </li>
</ol>

<p>クラスタ・ネットワークの <strong>状態</strong> が <strong>終了済</strong> となれば、削除が完了しています。</p>

<p>以上で、本チュートリアルは終了です。</p>

        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 更新日時:</strong> <time datetime="2023-05-23T11:08:19+09:00">May 23, 2023</time></p>


      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">共有</h4>
  

  <a href="https://twitter.com/intent/tweet?text=GPU%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%82%92%E6%A7%8B%E7%AF%89%E3%81%99%E3%82%8B%28%E5%9F%BA%E7%A4%8E%E3%82%A4%E3%83%B3%E3%83%95%E3%83%A9%E6%89%8B%E5%8B%95%E6%A7%8B%E7%AF%89%E7%B7%A8%29%20https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fspinup-gpu-cluster%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fspinup-gpu-cluster%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fspinup-gpu-cluster%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ocitutorials/hpc/spinup-ml-instance/" class="pagination--pager" title="GPUインスタンスで機械学習にトライ
">前へ</a>
    
    
      <a href="/ocitutorials/hpc/spinup-gpu-cluster-withstack/" class="pagination--pager" title="GPUクラスタを構築する(スタティッククラスタ自動構築編)
">次へ</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">関連記事</h4>
      <div class="grid__wrapper">
        
      </div>
    </div>
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="検索キーワードを入力してください..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>フォロー</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/#ocijp" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/oracle-japan" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/ocitutorials/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 Oracle Cloud Infrastructure チュートリアル. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/ocitutorials/assets/js/main.min.js"></script>




<script src="/ocitutorials/assets/js/lunr/lunr.min.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-store.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6W7FEC5CEH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6W7FEC5CEH', { 'anonymize_ip': false});
</script>







  
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
  
    <script src="/ocitutorials/assets/js/clipboardrouge.js"></script>
  
    <script src="/ocitutorials/assets/js/tabs.js"></script>
  
    <script src="/ocitutorials/assets/js/sidebar.js"></script>
  



  </body>
</html>
