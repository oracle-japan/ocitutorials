<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="ja-JP" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Intel MPI Benchmarks実行方法 | Oracle Cloud Infrastructure チュートリアル</title>
<meta name="description" content="本ドキュメントは、HPCワークロードの実行に最適な、高帯域・低遅延RDMA対応RoCEv2採用のクラスタ・ネットワークでHPCワークロード向けベアメタルインスタンスをノード間接続するHPCクラスタで、標準ベンチマークのIntel MPI Benchmarksを実行する方法を解説します。">


  <meta name="author" content="Oracle Japan Solution Engineers">
  
  <meta property="article:author" content="Oracle Japan Solution Engineers">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ja_JP">
<meta property="og:site_name" content="Oracle Cloud Infrastructure チュートリアル">
<meta property="og:title" content="Intel MPI Benchmarks実行方法">
<meta property="og:url" content="https://oracle-japan.github.io/ocitutorials/hpc/benchmark/run-imb/">


  <meta property="og:description" content="本ドキュメントは、HPCワークロードの実行に最適な、高帯域・低遅延RDMA対応RoCEv2採用のクラスタ・ネットワークでHPCワークロード向けベアメタルインスタンスをノード間接続するHPCクラスタで、標準ベンチマークのIntel MPI Benchmarksを実行する方法を解説します。">



  <meta property="og:image" content="https://oracle-japan.github.io/ocitutorials/assets/images/rh01-cloud-home-pine-background.jpg">





  <meta property="article:published_time" content="2025-03-11T16:30:36+09:00">






<link rel="canonical" href="https://oracle-japan.github.io/ocitutorials/hpc/benchmark/run-imb/">












<!-- end _includes/seo.html -->



  <link href="/ocitutorials/feed.xml" type="application/atom+xml" rel="alternate" title="Oracle Cloud Infrastructure チュートリアル Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script type="text/javascript">
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/ocitutorials/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-32.png" sizes="32x32">
<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-128.png" sizes="128x128">
<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-192.png" sizes="192x192">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-120.png" sizes="120x120">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-152.png" sizes="152x152">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-180.png" sizes="180x180">
<link rel="shortcut icon" type="image/x-icon" href="/ocitutorials/assets/favicon/favicon.ico">
<link rel="manifest" href="/ocitutorials/assets/favicon/site.webmanifest">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/ocitutorials/"><img src="/ocitutorials/assets/images/social-og-oracle-badge.jpg" alt="OCI チュートリアル"></a>
        
        <a class="site-title" href="/ocitutorials/">
          OCI チュートリアル
          <span class="site-subtitle">Oracle Cloud Infrastructure を使ってみよう</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/ocitutorials/#%E3%83%81%E3%83%A5%E3%83%BC%E3%83%88%E3%83%AA%E3%82%A2%E3%83%AB%E3%82%B3%E3%83%B3%E3%83%86%E3%83%B3%E3%83%84%E4%B8%80%E8%A6%A7"
                
                
              >チュートリアル一覧</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ocitutorials/about/"
                
                
              >このサイトについて</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">メニュー</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style=" background-image: linear-gradient(rgba(34, 66, 55, 0.7), rgba(34, 66, 55, 0.7)), url('/ocitutorials/assets/images/rh01-cloud-home-pine-background.jpg');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Intel MPI Benchmarks実行方法

        
      </h1>
      
        <p class="page__lead">本ドキュメントは、HPCワークロードの実行に最適な、高帯域・低遅延RDMA対応RoCEv2採用のクラスタ・ネットワークでHPCワークロード向けベアメタルインスタンスをノード間接続するHPCクラスタで、標準ベンチマークのIntel MPI Benchmarksを実行する方法を解説します。
</p>
      
      


      
    </div>
  
  
</div>






  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/" itemprop="item"><span itemprop="name">ホーム</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">></span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/hpc" itemprop="item"><span itemprop="name">Hpc</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">></span>
      
    
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/benchmark" itemprop="item"><span itemprop="name">Benchmark</span></a>
          <meta itemprop="position" content="3" />
        </li>
        <span class="sep">></span>
      
    
      
      
        <li class="current">Intel MPI Benchmarks実行方法</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">メニュー</label>
  <ul class="nav__items">
    <li>
      
      <a href=""><span class="nav__sub-title">HPC編</span></a>
      <ul>
        
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-cluster-network/">HPCクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster-withterraform/">HPCクラスタを構築する(基礎インフラ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster/">HPCクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster-withautoscaling/">HPCクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-ml-instance/">GPUインスタンスで機械学習にトライ</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster/">GPUクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withterraform/">GPUクラスタを構築する(基礎インフラ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withstack/">GPUクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withautoscaling/">GPUクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withubuntu/">GPUクラスタを構築する(Ubuntu OS編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server-fss/">ファイル・ストレージでファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server/">ブロック・ボリュームでファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server-nvme/">短期保存データ用高速ファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-backup-server/">ベア・メタル・インスタンスNFSサーバ向けバックアップサーバを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-base/">ブロック・ボリュームNFSサーバと基礎インフラ編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-stack/">ブロック・ボリュームNFSサーバと自動構築編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl/">HPL実行方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl-e5/">HPL実行方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream/">STREAM実行方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream-e5/">STREAM実行方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-imb/" class="active">Intel MPI Benchmarks実行方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-nccltests/">NCCL Tests実行方法（BM.GPU4.8/BM.GPU.A100-v2.8編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-nccltests-h100/">NCCL Tests実行方法（BM.GPU.H100.8編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/bios-setting/">パフォーマンスに関連するベアメタルインスタンスのBIOS設定方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/stop-unused-service/">不要サービス停止によるパフォーマンスチューニング方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/topology-aware-cn-tuning/">クラスタ・ネットワークのトポロジーを考慮したノード間通信最適化方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openfoam-tuning/">CFD解析フローのコストパフォーマンを向上させるOpenFOAM関連Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/">OpenMPIのMPI通信性能に影響するパラメータとその関連Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/cpu-binding/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftune/">OpenMPIのMPI集合通信チューニング方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/papi-profiling/">PAPIでHPCアプリケーションをプロファイリング</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/scorep-profiling/">Score-P・Scalasca・CubeGUIで並列アプリケーションをプロファイリング</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-connect-clusternetwork/">クラスタネットワーキングイメージを使ったクラスタ・ネットワーク接続方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/rdma-interface-configure/">クラスタ・ネットワーク接続用ネットワークインターフェース作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/">クラスタネットワーキングイメージの選び方</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-create-cnenabled-osimage/">クラスタ・ネットワーク未対応OSを使ったクラスタ・ネットワーク接続方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/determine-cnrelated-issue/">クラスタ・ネットワークに接続する計算/GPUノード作成時の問題判別方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-get-cnrelated-statistics/">クラスタ・ネットワーク統計情報の取得方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/nvme-filesystem/">ベアメタルインスタンスのNVMe SSDローカルディスク領域ファイルシステム作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-configure-sharedstorage/">HPC/GPUクラスタ向けファイル共有ストレージの最適な構築手法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/bv-sharedstorage-recovery/">ブロック・ボリュームを使用するNFSサーバのインスタンス障害からの復旧方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/boot-volume-extension/">計算/GPUノードのブート・ボリューム動的拡張方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-choose-osbackuptool/">ファイル共有ストレージ向けバックアップ環境の最適な構築手法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-name-resolution/">計算/GPUノードの効果的な名前解決方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-os-customization/">計算/GPUノードデプロイ時の効果的なOSカスタマイズ方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-host-list/">計算/GPUノードのホスト名リスト作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/cluster-resize/">計算/GPUノードの追加・削除・入れ替え方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/cluster-with-pdsh/">pdshで効率的にクラスタ管理オペレーションを実行</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/instance-principal-auth/">オンデマンドクラスタ実現のためのインスタンス・プリンシパル認証設定方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/log-monitoring/">OCIロギングとGrafanaを使用したHPC/GPUクラスタのログ監視方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/metric-monitoring/">OCIモニタリングとGrafanaを使用したHPC/GPUクラスタのメトリック監視方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/gpu-with-ubuntu/">UbuntuをOSとする機械学習ワークロード向けGPUノード構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするUCX通信フレームワークベースのOpenMPI構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/setup-slurm-cluster/">Slurmによるリソース管理・ジョブ管理システム構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/install-blas/">線形代数演算ライブラリインストール・利用方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/install-openfoam/">OpenFOAMインストール・利用方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/slurm-tips/">Slurmによるリソース管理・ジョブ管理システム運用Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/kdump-on-baremetal/">ベアメタル・インスタンスのカーネルダンプ取得方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/site-to-site-vpn/">サイト間VPNによるOCIとの拠点間接続方法</a></li></p>
        
      </ul>
    </li>
  </ul>
</nav>
    
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Intel MPI Benchmarks実行方法">
    <meta itemprop="description" content="本ドキュメントは、HPCワークロードの実行に最適な、高帯域・低遅延RDMA対応RoCEv2採用のクラスタ・ネットワークでHPCワークロード向けベアメタルインスタンスをノード間接続するHPCクラスタで、標準ベンチマークのIntel MPI Benchmarksを実行する方法を解説します。">
    <meta itemprop="datePublished" content="2025-03-11T16:30:36+09:00">
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> 目次</h4></header>
              <ul class="toc__menu"><li><a href="#0-概要">0. 概要</a></li><li><a href="#1-openmpiでintel-mpi-benchmarksを実行する場合">1. OpenMPIでIntel MPI Benchmarksを実行する場合</a><ul><li><a href="#1-0-概要">1-0. 概要</a></li><li><a href="#1-1-openmpiインストール">1-1. OpenMPIインストール</a></li><li><a href="#1-2-intel-mpi-benchmarksインストール">1-2. Intel MPI Benchmarksインストール</a></li><li><a href="#1-3-intel-mpi-benchmarks実行">1-3. Intel MPI Benchmarks実行</a></li><li><a href="#1-3-0-概要">1-3-0. 概要</a></li><li><a href="#1-3-1-1ノード内全コアを使用するalltoall">1-3-1. 1ノード内全コアを使用するAlltoall</a></li><li><a href="#1-3-2-2ノード間のpingpong">1-3-2. 2ノード間のPingPong</a></li><li><a href="#1-3-3-4ノード間のallreduce">1-3-3. 4ノード間のAllreduce</a></li></ul></li><li><a href="#2-intel-mpi-libraryでintel-mpi-benchmarksを実行する場合">2. Intel MPI LibraryでIntel MPI Benchmarksを実行する場合</a><ul><li><a href="#2-0-概要">2-0. 概要</a></li><li><a href="#2-1-intel-oneapi-hpc-toolkitインストール">2-1. Intel oneAPI HPC Toolkitインストール</a></li><li><a href="#2-2-ホストリストファイル作成">2-2. ホストリストファイル作成</a></li><li><a href="#2-3-intel-mpi-benchmarks実行">2-3. Intel MPI Benchmarks実行</a></li></ul></li></ul>
            </nav>
          </aside>
        
        <hr />
<h1 id="0-概要">0. 概要</h1>

<p>本ドキュメントで解説する <strong><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-mpi-benchmarks.html">Intel MPI Benchmarks</a></strong> の実行は、 <strong>GitHub</strong> から提供される <strong>Intel MPI Benchmarks</strong> を <strong><a href="https://www.open-mpi.org/">OpenMPI</a></strong> で実行する方法と、 <strong><a href="https://www.xlsoft.com/jp/products/intel/oneapi/hpc/index.html">Intel oneAPI HPC Toolkit</a></strong> に含まれる <strong>Intel MPI Benchmarks</strong> と <strong><a href="https://www.xlsoft.com/jp/products/intel/cluster/mpi/index.html">Intel MPI Library</a></strong> を使用する方法を解説します。</p>

<p><strong>Intel MPI Benchmarks</strong> の実行は、以下3種類を解説します。</p>

<ol>
  <li>1ノード内全コアを使用するAlltoall</li>
  <li>2ノード間のPingPong</li>
  <li>4ノード間のAllreduce</li>
</ol>

<p>本ドキュメントで <strong>Intel MPI Benchmarks</strong> を実行するHPCクラスタは、HPCワークロード向けベアメタルシェイプ <strong><a href="https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized">BM.Optimized3.36</a></strong> 4インスタンスを <strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong> で接続した構成とし、 <strong><a href="/ocitutorials/hpc/#1-oci-hpcチュートリアル集">OCI HPCチュートリアル集</a></strong> のカテゴリ <strong><a href="/ocitutorials/hpc/#1-1-hpcクラスタ">HPCクラスタ</a></strong> のチュートリアルの手順に従う等により、ノード間でMPIが実行できるよう予め構築しておきます。</p>

<p>本ドキュメントは、以下の環境で <strong>Intel MPI Benchmarks</strong> PingPongを実行し、以下の性能が出ています。</p>

<p>[実行環境]</p>
<ul>
  <li>シェイプ : <strong>BM.Optimized3.36</strong> （搭載コア数36）</li>
  <li>OS ： <strong>Oracle Linux</strong> 8.10ベースのHPC <strong><a href="/ocitutorials/hpc/#5-13-クラスタネットワーキングイメージ">クラスタネットワーキングイメージ</a></strong> （※1）</li>
  <li><strong>OpenMPI</strong> ： 5.0.6（※2）</li>
  <li><strong>Intel MPI Library</strong> ： 2021.3.0</li>
  <li><strong>Intel MPI Benchmarks</strong> ： 2021.7</li>
</ul>

<p>※1）<strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/">クラスタネットワーキングイメージの選び方</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/#1-クラスタネットワーキングイメージ一覧">1. クラスタネットワーキングイメージ一覧</a></strong> のイメージ <strong>No.12</strong> です。<br />
※2） <strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするUCX通信フレームワークベースのOpenMPI構築方法</a></strong> に従って構築された <strong>OpenMPI</strong> です。</p>

<p>[実行結果（ <strong>OpenMPI</strong> ）]</p>
<ul>
  <li>レイテンシ: 1.66 usec</li>
  <li>帯域幅（256 MiBメッセージサイズ）: 12,225 MB/s</li>
</ul>

<p>[実行結果（ <strong>Intel MPI Library</strong> ）]</p>
<ul>
  <li>レイテンシ: 1.60 usec</li>
  <li>帯域幅（256 MiBメッセージサイズ）: 12,277 MB/s</li>
</ul>

<hr />
<h1 id="1-openmpiでintel-mpi-benchmarksを実行する場合">1. OpenMPIでIntel MPI Benchmarksを実行する場合</h1>

<h2 id="1-0-概要">1-0. 概要</h2>

<p>本章は、 <strong>OpenMPI</strong> を使用して <strong>Intel MPI Benchmarks</strong> を実行する方法を解説します。<br />
具体的には、以下の作業を実施します。</p>

<ol>
  <li><strong>OpenMPI</strong> インストール</li>
  <li><strong>Intel MPI Benchmarks</strong> インストール</li>
  <li><strong>Intel MPI Benchmarks</strong> 実行</li>
</ol>

<h2 id="1-1-openmpiインストール">1-1. OpenMPIインストール</h2>

<p><strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするUCX通信フレームワークベースのOpenMPI構築方法</a></strong> に従い、  <strong>Intel MPI Benchmarks</strong> を実行する全てのノードに <strong>OpenMPI</strong> をインストールします。</p>

<h2 id="1-2-intel-mpi-benchmarksインストール">1-2. Intel MPI Benchmarksインストール</h2>

<p>以下コマンドを <strong>Intel MPI Benchmarks</strong> を実行する全てのノードのopcユーザで実行し、 <strong>Intel MPI Benchmarks</strong> をインストールします。<br />
なお、makeコマンドの並列数は当該ノードのコア数に合わせて調整します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cd</span> ~<span class="p">;</span> wget https://github.com/intel/mpi-benchmarks/archive/refs/tags/IMB-v2021.7.tar.gz
<span class="nv">$ </span><span class="nb">tar</span> <span class="nt">-xvf</span> ./IMB-v2021.7.tar.gz
<span class="nv">$ </span><span class="nb">cd </span>mpi-benchmarks-IMB-v2021.7<span class="p">;</span> <span class="nb">export </span><span class="nv">CXX</span><span class="o">=</span>/opt/openmpi-5.0.6/bin/mpicxx<span class="p">;</span> <span class="nb">export </span><span class="nv">CC</span><span class="o">=</span>/opt/openmpi-5.0.6/bin/mpicc<span class="p">;</span> make <span class="nt">-j</span> 36 all
<span class="nv">$ </span><span class="nb">sudo mkdir</span> <span class="nt">-p</span> /opt/openmpi-5.0.6/tests/imb
<span class="nv">$ </span><span class="nb">sudo cp</span> ./IMB<span class="k">*</span> /opt/openmpi-5.0.6/tests/imb/
</code></pre></div></div>

<h2 id="1-3-intel-mpi-benchmarks実行">1-3. Intel MPI Benchmarks実行</h2>

<h2 id="1-3-0-概要">1-3-0. 概要</h2>

<p>本章は、以下3種類の <strong>Intel MPI Benchmarks</strong> 実行方法を解説します。</p>

<ol>
  <li>1ノード内全コアを使用するAlltoall</li>
  <li>2ノード間のPingPong</li>
  <li>4ノード間のAllreduce</li>
</ol>

<h2 id="1-3-1-1ノード内全コアを使用するalltoall">1-3-1. 1ノード内全コアを使用するAlltoall</h2>

<p>以下コマンドを対象ノードで <strong>Intel MPI Benchmarks</strong> 実行ユーザで実行します。<br />
ここでは、1ノード36プロセスのAlltoall所要時間をメッセージサイズ32 MiBで計測しています。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>mpirun <span class="nt">-n</span> 36 /opt/openmpi-5.0.6/tests/imb/IMB-MPI1 <span class="nt">-msglog</span> 25:25 <span class="nt">-mem</span> 2.3G <span class="nt">-off_cache</span> 39,64 <span class="nt">-npmin</span> 36 alltoall
<span class="c">#----------------------------------------------------------------</span>
<span class="c">#    Intel(R) MPI Benchmarks 2021.7, MPI-1 part</span>
<span class="c">#----------------------------------------------------------------</span>
<span class="c"># Date                  : Fri Dec 13 16:41:35 2024</span>
<span class="c"># Machine               : x86_64</span>
<span class="c"># System                : Linux</span>
<span class="c"># Release               : 4.18.0-553.8.1.el8_10.x86_64</span>
<span class="c"># Version               : #1 SMP Tue Jul 2 05:18:08 PDT 2024</span>
<span class="c"># MPI Version           : 3.1</span>
<span class="c"># MPI Thread Environment: </span>


<span class="c"># Calling sequence was: </span>

<span class="c"># /opt/openmpi-5.0.6/tests/imb/IMB-MPI1 -msglog 25:25 -mem 2.3G -off_cache 39,64 -npmin 36 alltoall </span>

<span class="c"># Minimum message length in bytes:   0</span>
<span class="c"># Maximum message length in bytes:   33554432</span>
<span class="c">#</span>
<span class="c"># MPI_Datatype                   :   MPI_BYTE </span>
<span class="c"># MPI_Datatype for reductions    :   MPI_FLOAT </span>
<span class="c"># MPI_Op                         :   MPI_SUM  </span>
<span class="c"># </span>
<span class="c"># </span>

<span class="c"># List of Benchmarks to run:</span>

<span class="c"># Alltoall</span>

<span class="c">#----------------------------------------------------------------</span>
<span class="c"># Benchmarking Alltoall </span>
<span class="c"># #processes = 36 </span>
<span class="c">#----------------------------------------------------------------</span>
       <span class="c">#bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]</span>
            0         1000         0.03         0.05         0.03
     33554432            1    368774.69    377145.91    373427.99


<span class="c"># All processes entering MPI_Finalize</span>

<span class="err">$</span>
</code></pre></div></div>

<h2 id="1-3-2-2ノード間のpingpong">1-3-2. 2ノード間のPingPong</h2>

<p>以下コマンドを <strong>Intel MPI Benchmarks</strong> を実行するユーザで何れか1ノードで実行します。<br />
ここでは、2ノードを使用したPingPongをメッセージサイズ0バイトと256 MiBで計測し、レイテンシは0バイトメッセージの所要時間（ここでは <strong>1.66 usec</strong> ）、帯域幅は256 MiBメッセージの帯域幅（ <strong>12,225.03 MB/s</strong> ）を以ってその結果とします。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>mpirun <span class="nt">-n</span> 2 <span class="nt">-N</span> 1 <span class="nt">-hostfile</span> ~/hostlist.txt <span class="nt">-x</span> <span class="nv">UCX_NET_DEVICES</span><span class="o">=</span>mlx5_2:1 /opt/openmpi-5.0.6/tests/imb/IMB-MPI1 <span class="nt">-msglog</span> 28:28 pingpong
<span class="c">#----------------------------------------------------------------</span>
<span class="c">#    Intel(R) MPI Benchmarks 2021.7, MPI-1 part</span>
<span class="c">#----------------------------------------------------------------</span>
<span class="c"># Date                  : Thu Jun 20 11:35:05 2024</span>
<span class="c"># Machine               : x86_64</span>
<span class="c"># System                : Linux</span>
<span class="c"># Release               : 4.18.0-513.11.0.1.el8_9.x86_64</span>
<span class="c"># Version               : #1 SMP Thu Jan 11 11:30:45 PST 2024</span>
<span class="c"># MPI Version           : 3.1</span>
<span class="c"># MPI Thread Environment: </span>


<span class="c"># Calling sequence was: </span>

<span class="c"># /opt/openmpi-5.0.3/tests/imb/IMB-MPI1 -msglog 28:28 pingpong </span>

<span class="c"># Minimum message length in bytes:   0</span>
<span class="c"># Maximum message length in bytes:   268435456</span>
<span class="c">#</span>
<span class="c"># MPI_Datatype                   :   MPI_BYTE </span>
<span class="c"># MPI_Datatype for reductions    :   MPI_FLOAT </span>
<span class="c"># MPI_Op                         :   MPI_SUM  </span>
<span class="c"># </span>
<span class="c"># </span>

<span class="c"># List of Benchmarks to run:</span>

<span class="c"># PingPong</span>

<span class="c">#---------------------------------------------------</span>
<span class="c"># Benchmarking PingPong </span>
<span class="c"># #processes = 2 </span>
<span class="c">#---------------------------------------------------</span>
       <span class="c">#bytes #repetitions      t[usec]   Mbytes/sec</span>
            0         1000         1.66         0.00
    268435456            1     21957.85     12225.03


<span class="c"># All processes entering MPI_Finalize</span>

<span class="err">$</span>
</code></pre></div></div>

<h2 id="1-3-3-4ノード間のallreduce">1-3-3. 4ノード間のAllreduce</h2>

<p>以下コマンドを <strong>Intel MPI Benchmarks</strong> を実行するユーザで何れか1ノードで実行します。<br />
ここでは、4ノード144プロセス（ノードあたり36プロセス）を使用したAllreduceの所要時間をメッセージサイズ256 MiBで計測しています。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>mpirun <span class="nt">-n</span> 144 <span class="nt">-N</span> 36 <span class="nt">-hostfile</span> ~/hostlist.txt <span class="nt">-x</span> <span class="nv">UCX_NET_DEVICES</span><span class="o">=</span>mlx5_2:1 /opt/openmpi-5.0.6/tests/imb/IMB-MPI1 <span class="nt">-msglog</span> 28:28 <span class="nt">-npmin</span> 144 allreduce
<span class="c">#----------------------------------------------------------------</span>
<span class="c">#    Intel(R) MPI Benchmarks 2021.7, MPI-1 part</span>
<span class="c">#----------------------------------------------------------------</span>
<span class="c"># Date                  : Thu Jun 20 11:40:32 2024</span>
<span class="c"># Machine               : x86_64</span>
<span class="c"># System                : Linux</span>
<span class="c"># Release               : 4.18.0-513.11.0.1.el8_9.x86_64</span>
<span class="c"># Version               : #1 SMP Thu Jan 11 11:30:45 PST 2024</span>
<span class="c"># MPI Version           : 3.1</span>
<span class="c"># MPI Thread Environment: </span>


<span class="c"># Calling sequence was: </span>

<span class="c"># /opt/openmpi-5.0.3/tests/imb/IMB-MPI1 -msglog 28:28 -npmin 144 allreduce </span>

<span class="c"># Minimum message length in bytes:   0</span>
<span class="c"># Maximum message length in bytes:   268435456</span>
<span class="c">#</span>
<span class="c"># MPI_Datatype                   :   MPI_BYTE </span>
<span class="c"># MPI_Datatype for reductions    :   MPI_FLOAT </span>
<span class="c"># MPI_Op                         :   MPI_SUM  </span>
<span class="c"># </span>
<span class="c"># </span>

<span class="c"># List of Benchmarks to run:</span>

<span class="c"># Allreduce</span>

<span class="c">#----------------------------------------------------------------</span>
<span class="c"># Benchmarking Allreduce </span>
<span class="c"># #processes = 144 </span>
<span class="c">#----------------------------------------------------------------</span>
       <span class="c">#bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]</span>
            0         1000         0.03         0.04         0.03
    268435456            1    307115.36    326712.51    319229.75


<span class="c"># All processes entering MPI_Finalize</span>

<span class="err">$</span>
</code></pre></div></div>

<hr />
<h1 id="2-intel-mpi-libraryでintel-mpi-benchmarksを実行する場合">2. Intel MPI LibraryでIntel MPI Benchmarksを実行する場合</h1>

<h2 id="2-0-概要">2-0. 概要</h2>

<p>本章は、 <strong>Intel MPI Library</strong> を使用して <strong>Intel MPI Benchmarks</strong> を実行する方法を解説します。<br />
具体的には、以下の作業を実施します。</p>

<ol>
  <li><strong>Intel oneAPI HPC Toolkit</strong> インストール</li>
  <li>ホストリストファイル作成</li>
  <li><strong>Intel MPI Benchmarks</strong> 実行</li>
</ol>

<h2 id="2-1-intel-oneapi-hpc-toolkitインストール">2-1. Intel oneAPI HPC Toolkitインストール</h2>

<p>以下コマンドを <strong>Intel MPI Benchmarks</strong> を実行する全てのノードのopcユーザで実行し、 <strong>Intel oneAPI HPC Toolkit</strong> をインストールします。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>yum-config-manager <span class="nt">--add-repo</span> https://yum.repos.intel.com/oneapi
<span class="nv">$ </span><span class="nb">sudo </span>rpm <span class="nt">--import</span> https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB
<span class="nv">$ </span><span class="nb">sudo </span>dnf <span class="nb">install</span> <span class="nt">-y</span> intel-basekit-2021.3.0 intel-hpckit-2021.3.0
</code></pre></div></div>

<h2 id="2-2-ホストリストファイル作成">2-2. ホストリストファイル作成</h2>

<p><strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/compute-host-list/">計算/GPUノードのホスト名リスト作成方法</a></strong> の手順に従い、 <strong>Intel MPI Benchmarks</strong> を実行する全てのノードのホスト名を記載したホストリストファイルを <strong>Intel MPI Benchmarks</strong> を実行するユーザのホームディレクトリ直下に <strong>hostlist.txt</strong> として作成します。</p>

<h2 id="2-3-intel-mpi-benchmarks実行">2-3. Intel MPI Benchmarks実行</h2>

<p>以下コマンドを <strong>Intel MPI Benchmarks</strong> を実行するユーザで何れか1ノードで実行します。<br />
ここでは、2ノードを使用したPingPongをメッセージサイズ0バイトと256 MiBで計測し、レイテンシは0バイトメッセージの所要時間（ここでは1.60 usec）、帯域幅は256 MiBメッセージの帯域幅（12,277.42 MB/s）を以ってその結果とします。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">source</span> /opt/intel/oneapi/setvars.sh
 
:: initializing oneAPI environment ...
   <span class="nt">-bash</span>: BASH_VERSION <span class="o">=</span> 4.4.20<span class="o">(</span>1<span class="o">)</span><span class="nt">-release</span>
   args: Using <span class="s2">"</span><span class="nv">$@</span><span class="s2">"</span> <span class="k">for </span>setvars.sh arguments: 
:: advisor <span class="nt">--</span> latest
:: ccl <span class="nt">--</span> latest
:: clck <span class="nt">--</span> latest
:: compiler <span class="nt">--</span> latest
:: dal <span class="nt">--</span> latest
:: debugger <span class="nt">--</span> latest
:: dev-utilities <span class="nt">--</span> latest
:: dnnl <span class="nt">--</span> latest
:: dpcpp-ct <span class="nt">--</span> latest
:: dpl <span class="nt">--</span> latest
:: inspector <span class="nt">--</span> latest
:: intelpython <span class="nt">--</span> latest
:: ipp <span class="nt">--</span> latest
:: ippcp <span class="nt">--</span> latest
:: itac <span class="nt">--</span> latest
:: mkl <span class="nt">--</span> latest
:: mpi <span class="nt">--</span> latest
:: tbb <span class="nt">--</span> latest
:: vpl <span class="nt">--</span> latest
:: vtune <span class="nt">--</span> latest
:: oneAPI environment initialized ::
 
<span class="nv">$ </span>mpirun <span class="nt">-n</span> 2 <span class="nt">-ppn</span> 1 <span class="nt">-hostfile</span> ~/hostlist.txt <span class="nt">-genv</span> <span class="nv">UCX_NET_DEVICES</span><span class="o">=</span>mlx5_2:1 IMB-MPI1 <span class="nt">-msglog</span> 27:28 pingpong
<span class="c">#----------------------------------------------------------------</span>
<span class="c">#    Intel(R) MPI Benchmarks 2021.2, MPI-1 part</span>
<span class="c">#----------------------------------------------------------------</span>
<span class="c"># Date                  : Thu Jun 20 16:45:47 2024</span>
<span class="c"># Machine               : x86_64</span>
<span class="c"># System                : Linux</span>
<span class="c"># Release               : 4.18.0-513.11.0.1.el8_9.x86_64</span>
<span class="c"># Version               : #1 SMP Thu Jan 11 11:30:45 PST 2024</span>
<span class="c"># MPI Version           : 3.1</span>
<span class="c"># MPI Thread Environment: </span>


<span class="c"># Calling sequence was: </span>

<span class="c"># IMB-MPI1 -msglog 27:28 pingpong </span>

<span class="c"># Minimum message length in bytes:   0</span>
<span class="c"># Maximum message length in bytes:   268435456</span>
<span class="c">#</span>
<span class="c"># MPI_Datatype                   :   MPI_BYTE </span>
<span class="c"># MPI_Datatype for reductions    :   MPI_FLOAT </span>
<span class="c"># MPI_Op                         :   MPI_SUM  </span>
<span class="c"># </span>
<span class="c"># </span>

<span class="c"># List of Benchmarks to run:</span>

<span class="c"># PingPong</span>

<span class="c">#---------------------------------------------------</span>
<span class="c"># Benchmarking PingPong </span>
<span class="c"># #processes = 2 </span>
<span class="c">#---------------------------------------------------</span>
       <span class="c">#bytes #repetitions      t[usec]   Mbytes/sec</span>
            0         1000         1.60         0.00
    134217728            1     10935.56     12273.51
    268435456            1     21864.16     12277.42


<span class="c"># All processes entering MPI_Finalize</span>

<span class="err">$</span>
</code></pre></div></div>

<p>次に、以下コマンドを <strong>Intel MPI Benchmarks</strong> を実行するユーザで何れか1ノードで実行します。<br />
ここでは、4ノード144プロセス（ノードあたり36プロセス）を使用したAllreduceの所要時間をメッセージサイズ256 MiBで計測しています。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>mpirun <span class="nt">-n</span> 144 <span class="nt">-ppn</span> 36 <span class="nt">-hostfile</span> ~/hostlist.txt <span class="nt">-genv</span> <span class="nv">UCX_NET_DEVICES</span><span class="o">=</span>mlx5_2:1 IMB-MPI1 <span class="nt">-msglog</span> 27:28 <span class="nt">-npmin</span> 144 allreduce
<span class="c">#----------------------------------------------------------------</span>
<span class="c">#    Intel(R) MPI Benchmarks 2021.2, MPI-1 part</span>
<span class="c">#----------------------------------------------------------------</span>
<span class="c"># Date                  : Thu Jun 20 16:46:03 2024</span>
<span class="c"># Machine               : x86_64</span>
<span class="c"># System                : Linux</span>
<span class="c"># Release               : 4.18.0-513.11.0.1.el8_9.x86_64</span>
<span class="c"># Version               : #1 SMP Thu Jan 11 11:30:45 PST 2024</span>
<span class="c"># MPI Version           : 3.1</span>
<span class="c"># MPI Thread Environment: </span>


<span class="c"># Calling sequence was: </span>

<span class="c"># IMB-MPI1 -msglog 27:28 -npmin 144 allreduce </span>

<span class="c"># Minimum message length in bytes:   0</span>
<span class="c"># Maximum message length in bytes:   268435456</span>
<span class="c">#</span>
<span class="c"># MPI_Datatype                   :   MPI_BYTE </span>
<span class="c"># MPI_Datatype for reductions    :   MPI_FLOAT </span>
<span class="c"># MPI_Op                         :   MPI_SUM  </span>
<span class="c"># </span>
<span class="c"># </span>

<span class="c"># List of Benchmarks to run:</span>

<span class="c"># Allreduce</span>

<span class="c">#----------------------------------------------------------------</span>
<span class="c"># Benchmarking Allreduce </span>
<span class="c"># #processes = 144 </span>
<span class="c">#----------------------------------------------------------------</span>
       <span class="c">#bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]</span>
            0         1000         0.03         0.03         0.03
    134217728            1    105362.17    106363.65    105716.44
    268435456            1    205579.37    206503.46    205992.71


<span class="c"># All processes entering MPI_Finalize</span>

<span class="err">$</span>
</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 更新日時:</strong> <time class="dt-published" datetime="2025-03-11T16:30:36+09:00">March 11, 2025</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">共有</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Intel+MPI+Benchmarks%E5%AE%9F%E8%A1%8C%E6%96%B9%E6%B3%95%20https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fbenchmark%2Frun-imb%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fbenchmark%2Frun-imb%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://oracle-japan.github.io/ocitutorials/hpc/benchmark/run-imb/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ocitutorials/hpc/benchmark/run-stream-e5/" class="pagination--pager" title="STREAM実行方法（BM.Standard.E5.192編）
">前へ</a>
    
    
      <a href="/ocitutorials/hpc/benchmark/run-nccltests/" class="pagination--pager" title="NCCL Tests実行方法（BM.GPU4.8/BM.GPU.A100-v2.8編）
">次へ</a>
    
  </nav>

    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">関連記事</h2>
  <div class="grid__wrapper">
    
  </div>
</div>

  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="検索キーワードを入力してください..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>フォロー</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/#ocijp" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/oracle-japan" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/ocitutorials/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 <a href="https://oracle-japan.github.io">Oracle Cloud Infrastructure チュートリアル</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/ocitutorials/assets/js/main.min.js"></script>




<script src="/ocitutorials/assets/js/lunr/lunr.min.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-store.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6W7FEC5CEH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6W7FEC5CEH', { 'anonymize_ip': false});
</script>







  
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
  
    <script src="/ocitutorials/assets/js/clipboardrouge.js"></script>
  
    <script src="/ocitutorials/assets/js/tabs.js"></script>
  
    <script src="/ocitutorials/assets/js/sidebar.js"></script>
  


  </body>
</html>
