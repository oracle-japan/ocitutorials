<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="ja-JP" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>プロファイリング情報に基づく並列アプリケーションチューニング方法 | Oracle Cloud Infrastructure チュートリアル</title>
<meta name="description" content="並列アプリケーションは、実行時のプロセス間通信に要する時間が性能に悪影響を及ぼしますが、プロファイリングツールから収集する情報で最も時間を要している通信を特定し、この通信パターンに応じたチューニングを適用することで、その性能を改善できる場合があります。本パフォーマンス・プロファイリング関連Tipsは、MPI並列アプリケーションをオープンソースのプロファイリングツールでプロファイリングし、収集した情報からMPI通信パターンに応じたチューニングを適用、その性能を改善する方法を解説します。">


  <meta name="author" content="Oracle Japan Solution Engineers">
  
  <meta property="article:author" content="Oracle Japan Solution Engineers">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ja_JP">
<meta property="og:site_name" content="Oracle Cloud Infrastructure チュートリアル">
<meta property="og:title" content="プロファイリング情報に基づく並列アプリケーションチューニング方法">
<meta property="og:url" content="https://oracle-japan.github.io/ocitutorials/hpc/benchmark/profiling-tuning/">


  <meta property="og:description" content="並列アプリケーションは、実行時のプロセス間通信に要する時間が性能に悪影響を及ぼしますが、プロファイリングツールから収集する情報で最も時間を要している通信を特定し、この通信パターンに応じたチューニングを適用することで、その性能を改善できる場合があります。本パフォーマンス・プロファイリング関連Tipsは、MPI並列アプリケーションをオープンソースのプロファイリングツールでプロファイリングし、収集した情報からMPI通信パターンに応じたチューニングを適用、その性能を改善する方法を解説します。">



  <meta property="og:image" content="https://oracle-japan.github.io/ocitutorials/assets/images/rh01-cloud-home-pine-background.jpg">





  <meta property="article:published_time" content="2025-08-13T08:16:11+09:00">






<link rel="canonical" href="https://oracle-japan.github.io/ocitutorials/hpc/benchmark/profiling-tuning/">












<!-- end _includes/seo.html -->



  <link href="/ocitutorials/feed.xml" type="application/atom+xml" rel="alternate" title="Oracle Cloud Infrastructure チュートリアル Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/ocitutorials/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-32.png" sizes="32x32">
<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-128.png" sizes="128x128">
<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-192.png" sizes="192x192">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-120.png" sizes="120x120">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-152.png" sizes="152x152">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-180.png" sizes="180x180">
<link rel="shortcut icon" type="image/x-icon" href="/ocitutorials/assets/favicon/favicon.ico">
<link rel="manifest" href="/ocitutorials/assets/favicon/site.webmanifest">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/ocitutorials/"><img src="/ocitutorials/assets/images/social-og-oracle-badge.jpg" alt="OCI チュートリアル"></a>
        
        <a class="site-title" href="/ocitutorials/">
          OCI チュートリアル
          <span class="site-subtitle">Oracle Cloud Infrastructure を使ってみよう</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/ocitutorials/#%E3%83%81%E3%83%A5%E3%83%BC%E3%83%88%E3%83%AA%E3%82%A2%E3%83%AB%E3%82%B3%E3%83%B3%E3%83%86%E3%83%B3%E3%83%84%E4%B8%80%E8%A6%A7"
                
                
              >チュートリアル一覧</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ocitutorials/about/"
                
                
              >このサイトについて</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">メニュー</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style=" background-image: linear-gradient(rgba(34, 66, 55, 0.7), rgba(34, 66, 55, 0.7)), url('/ocitutorials/assets/images/rh01-cloud-home-pine-background.jpg');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          プロファイリング情報に基づく並列アプリケーションチューニング方法

        
      </h1>
      
        <p class="page__lead">並列アプリケーションは、実行時のプロセス間通信に要する時間が性能に悪影響を及ぼしますが、プロファイリングツールから収集する情報で最も時間を要している通信を特定し、この通信パターンに応じたチューニングを適用することで、その性能を改善できる場合があります。本パフォーマンス・プロファイリング関連Tipsは、MPI並列アプリケーションをオープンソースのプロファイリングツールでプロファイリングし、収集した情報からMPI通信パターンに応じたチューニングを適用、その性能を改善する方法を解説します。
</p>
      
      


      
    </div>
  
  
</div>






  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/" itemprop="item"><span itemprop="name">ホーム</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">></span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/hpc" itemprop="item"><span itemprop="name">Hpc</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">></span>
      
    
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/benchmark" itemprop="item"><span itemprop="name">Benchmark</span></a>
          <meta itemprop="position" content="3" />
        </li>
        <span class="sep">></span>
      
    
      
      
        <li class="current">プロファイリング情報に基づく並列アプリケーションチューニング方法</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">メニュー</label>
  <ul class="nav__items">
    <li>
      
      <a href=""><span class="nav__sub-title">HPC編</span></a>
      <ul>
        
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-cluster-network/">HPCクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster-withterraform/">HPCクラスタを構築する(基礎インフラ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster/">HPCクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster-withautoscaling/">HPCクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-ml-instance/">GPUインスタンスで機械学習にトライ</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-ml-instance-cntnd/">GPUインスタンスで分散機械学習環境を構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster/">GPUクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withterraform/">GPUクラスタを構築する(基礎インフラ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withstack/">GPUクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withautoscaling/">GPUクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withubuntu/">GPUクラスタを構築する(Ubuntu OS編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server-fss/">ファイル・ストレージでファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-lustre-server-fswl/">File Storage with Lustreでファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server/">ブロック・ボリュームでファイル共有ストレージを構築する（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server-e6/">ブロック・ボリュームでファイル共有ストレージを構築する（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server-nvme/">短期保存データ用高速ファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-backup-server/">ベア・メタル・インスタンスNFSサーバ向けバックアップサーバを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-base/">ブロック・ボリュームNFSサーバと基礎インフラ編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-stack/">ブロック・ボリュームNFSサーバと自動構築編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl/">HPL実行方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl-e5/">HPL実行方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl-e6/">HPL実行方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream/">STREAM実行方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream-e5/">STREAM実行方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream-e6/">STREAM実行方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-imb/">Intel MPI Benchmarks実行方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-nccltests/">NCCL Tests実行方法（BM.GPU4.8/BM.GPU.A100-v2.8編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-nccltests-h100/">NCCL Tests実行方法（BM.GPU.H100.8編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/bios-setting/">パフォーマンスに関連するベアメタルインスタンスのBIOS設定方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/stop-unused-service/">不要サービス停止によるパフォーマンスチューニング方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/topology-aware-cn-tuning/">クラスタ・ネットワークのトポロジーを考慮したノード間通信最適化方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openfoam-tuning/">CFD解析フローのコストパフォーマンを向上させるOpenFOAM関連Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/">OpenMPIのMPI通信性能に影響するパラメータとその関連Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/cpu-binding/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/cpu-binding-e5/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/cpu-binding-e6/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftune/">OpenMPIのMPI集合通信チューニング方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftune-e5/">OpenMPIのMPI集合通信チューニング方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftune-e6/">OpenMPIのMPI集合通信チューニング方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/papi-profiling/">PAPIでHPCアプリケーションをプロファイリング</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/scorep-profiling/">Score-P・Scalasca・CubeGUIで並列アプリケーションをプロファイリング</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/profiling-tuning/" class="active">プロファイリング情報に基づく並列アプリケーションチューニング方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-connect-clusternetwork/">クラスタネットワーキングイメージを使ったクラスタ・ネットワーク接続方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/rdma-interface-configure/">クラスタ・ネットワーク接続用ネットワークインターフェース作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/">クラスタネットワーキングイメージの選び方</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-create-cnenabled-osimage/">クラスタ・ネットワーク未対応OSを使ったクラスタ・ネットワーク接続方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/determine-cnrelated-issue/">クラスタ・ネットワークに接続する計算/GPUノード作成時の問題判別方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-get-cnrelated-statistics/">クラスタ・ネットワーク統計情報の取得方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/nvme-filesystem/">ベアメタルインスタンスのNVMe SSDローカルディスク領域ファイルシステム作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-configure-sharedstorage/">HPC/GPUクラスタ向けファイル共有ストレージの最適な構築手法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/bv-sharedstorage-recovery/">ブロック・ボリュームを使用するNFSサーバのインスタンス障害からの復旧方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/boot-volume-extension/">計算/GPUノードのブート・ボリューム動的拡張方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-choose-osbackuptool/">ファイル共有ストレージ向けバックアップ環境の最適な構築手法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-name-resolution/">計算/GPUノードの効果的な名前解決方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-os-customization/">計算/GPUノードデプロイ時の効果的なOSカスタマイズ方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-host-list/">計算/GPUノードのホスト名リスト作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/cluster-resize/">計算/GPUノードの追加・削除・入れ替え方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/cluster-with-pdsh/">pdshで効率的にクラスタ管理オペレーションを実行</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/instance-principal-auth/">オンデマンドクラスタ実現のためのインスタンス・プリンシパル認証設定方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/log-monitoring/">OCIロギングとGrafanaを使用したHPC/GPUクラスタのログ監視方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/metric-monitoring/">OCIモニタリングとGrafanaを使用したHPC/GPUクラスタのメトリック監視方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/gpu-with-ubuntu/">UbuntuをOSとする機械学習ワークロード向けGPUノード構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするUCX通信フレームワークベースのOpenMPI構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/setup-slurm-cluster/">Slurmによるリソース管理・ジョブ管理システム構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/install-blas/">線形代数演算ライブラリインストール・利用方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/install-openfoam/">OpenFOAMインストール・利用方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/slurm-tips/">Slurmによるリソース管理・ジョブ管理システム運用Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/build-oraclelinux-hpcenv/">Oracle Linuxプラットフォーム・イメージベースのHPCワークロード実行環境構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/kdump-on-baremetal/">ベアメタルインスタンスのカーネルダンプ取得方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/site-to-site-vpn/">サイト間VPNによるOCIとの拠点間接続方法</a></li></p>
        
      </ul>
    </li>
  </ul>
</nav>
    
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="プロファイリング情報に基づく並列アプリケーションチューニング方法">
    <meta itemprop="description" content="並列アプリケーションは、実行時のプロセス間通信に要する時間が性能に悪影響を及ぼしますが、プロファイリングツールから収集する情報で最も時間を要している通信を特定し、この通信パターンに応じたチューニングを適用することで、その性能を改善できる場合があります。本パフォーマンス・プロファイリング関連Tipsは、MPI並列アプリケーションをオープンソースのプロファイリングツールでプロファイリングし、収集した情報からMPI通信パターンに応じたチューニングを適用、その性能を改善する方法を解説します。">
    <meta itemprop="datePublished" content="2025-08-13T08:16:11+09:00">
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> 目次</h4></header>
              <ul class="toc__menu"><li><a href="#0-概要">0. 概要</a></li><li><a href="#1-プロファイリングチューニング環境構築">1. プロファイリング・チューニング環境構築</a></li><li><a href="#2-プロファイリング">2. プロファイリング</a><ul><li><a href="#2-0-概要">2-0. 概要</a></li><li><a href="#2-1-プロファイリング手順">2-1. プロファイリング手順</a></li></ul></li><li><a href="#3-チューニング">3. チューニング</a><ul><li><a href="#3-0-概要">3-0. 概要</a></li><li><a href="#3-1-チューニング手順">3-1. チューニング手順</a></li></ul></li></ul>
            </nav>
          </aside>
        
        <hr />
<h1 id="0-概要">0. 概要</h1>

<p>並列アプリケーションのチューニングは、以下のステップを経て行われることが一般的です。</p>

<ul>
  <li>プロファイリング
    <ul>
      <li><strong>asis</strong> （※1）の所要時間計測</li>
      <li>プロファイリング取得時の所要時間計測</li>
      <li>両者に差がある場合プロファイリングオーバーヘッドの原因を特定</li>
      <li>プロファイリングオーバーヘッド対象個所を除外してプロファイリング取得</li>
      <li>プロファイリング情報から <strong>ホットスポット</strong> （※2）を特定</li>
    </ul>
  </li>
  <li>チューニング
    <ul>
      <li><strong>ホットスポット</strong> に対するチューニング手法検討</li>
      <li>チューニング適用時のプロファイリング取得</li>
      <li>プロファイリング情報から <strong>ホットスポット</strong> に対するチューニングの効果を確認</li>
      <li>チューニング適用時の所要時間計測</li>
      <li><strong>asis</strong> とチューニング適用時の所要時間比較・チューニング効果確認</li>
    </ul>
  </li>
</ul>

<p>※1）本パフォーマンス・プロファイリング関連Tipsでは、チューニング適用前のアプリケーションの状態を <strong>asis</strong> と呼称します。<br />
※2）本パフォーマンス・プロファイリング関連Tipsでは、所要時間のうち上位を占めるプログラム単位（サブルーチン・関数、MPI通信関数、IO等）を <strong>ホットスポット</strong> と呼称します。</p>

<p>以上のステップでチューニングの効果が得られた場合は、チューニング適用状態を <strong>asis</strong> として上記ステップを繰り返し、更なる性能向上を図ります。</p>

<p>以上を踏まえて本パフォーマンス・プロファイリング関連Tipsは、プロファイリングツールに以下のオープンソースソフトウェアを使用し、</p>

<ul>
  <li><strong><a href="https://www.vi-hps.org/projects/score-p/">Score-P</a></strong></li>
  <li><strong><a href="https://www.scalasca.org/">Scalasca</a></strong></li>
  <li><strong><a href="https://www.scalasca.org/scalasca/software/cube-4.x/download.html">CubeGUI</a></strong></li>
</ul>

<p><strong><a href="https://www.open-mpi.org/">OpenMPI</a></strong> を使用する並列アプリケーションを <strong>ホットスポット</strong> となるMPI通信関数にフォーカスしてチューニングし、その性能を向上させる手順を解説します。</p>

<p>本手順は、 <strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong> で相互接続する <strong><a href="https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized">BM.Optimized3.36</a></strong> を計算ノードとするHPCクラスタ環境で実行することを前提とし、 <strong><a href="https://www.nas.nasa.gov/software/npb.html">NAS Parallel Benchmarks</a></strong> （以降 <strong>NPB</strong> と呼称します。）の <strong>FT Class D</strong> をプロファイリング・チューニング対象の並列アプリケーションに使用します。</p>

<p>またMPI通信関数のチューニング手法は、 <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftune/">OpenMPIのMPI集合通信チューニング方法（BM.Optimized3.36編）</a></strong> で得られた結果を元に検討します。</p>

<p>以降では、以下の順に解説します。</p>

<ol>
  <li><strong><a href="#1-プロファイリングチューニング環境構築">プロファイリング・チューニング環境構築</a></strong></li>
  <li><strong><a href="#2-プロファイリング">プロファイリング</a></strong></li>
  <li><strong><a href="#3-チューニング">チューニング</a></strong></li>
</ol>

<hr />
<h1 id="1-プロファイリングチューニング環境構築">1. プロファイリング・チューニング環境構築</h1>

<p>本章は、本プロファイリング・チューニング関連Tipsで使用する環境を構築します。</p>

<p>この構築は、 <strong><a href="/ocitutorials/hpc/#2-3-プロファイリング関連tips集">OCI HPCプロファイリング関連Tips集</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/scorep-profiling/">Score-P・Scalasca・CubeGUIで並列アプリケーションをプロファイリング</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/scorep-profiling/#1-プロファイリング環境構築">1. プロファイリング環境構築</a></strong> の手順に従い実施します。</p>

<p>本プロファイリング・チューニング関連Tipsは、 <strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong> の同一リーフスイッチに接続する8ノードの計算ノードを使用しているため、その他の構成では以降のプロファイリング・チューニングの結果が異なります。（※3）</p>

<p>※3）<strong>クラスタ・ネットワーク</strong> の同一リーフスイッチに接続するインスタンス間のノード間接続に於ける効果は、 <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/topology-aware-cn-tuning/">クラスタ・ネットワークのトポロジーを考慮したノード間通信最適化方法</a></strong> を参照してください。</p>

<hr />
<h1 id="2-プロファイリング">2. プロファイリング</h1>

<h2 id="2-0-概要">2-0. 概要</h2>

<p>本章は、 <strong>NPB</strong> の <strong>FT Class D</strong> を使用し、 <strong>Scalasca</strong> から起動する <strong>Score-P</strong> でプロファイリング手法によるプロファイリングを実施します。<br />
ここでMPIの実行は、8ノードの計算ノードにノード当たり32個のMPIプロセスを配置する256並列のフラットMPIとし、プロセス分割方法にブロック分割を使用（※4）します。<br />
この際、プロファイリングによるオーバーヘッドを考慮した精度の良いプロファイリングを実施するため、以下の手順で実施します。</p>

<ul>
  <li><strong>NPB</strong> バイナリの作成</li>
  <li><strong>asis</strong> の  <strong>所要時間</strong> （※5）計測</li>
  <li>プロファイリング取得時の  <strong>所要時間</strong> 計測</li>
  <li>両者に差がある場合プロファイリングオーバーヘッドの原因を特定</li>
  <li>プロファイリングオーバーヘッド対象個所を除外してプロファイリング取得</li>
  <li>プロファイリング情報から <strong>ホットスポット</strong> を特定</li>
</ul>

<p>※4）プロセス分割方法にブロック分割を使用する場合の実行方法は、 <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/cpu-binding/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Optimized3.36編）</a></strong> を参照してください。<br />
※5）以降では、 <strong>NPB</strong> の <strong>FT Class D</strong> の出力の <strong>Time in seconds</strong> を  <strong>所要時間</strong> として扱います。</p>

<h2 id="2-1-プロファイリング手順">2-1. プロファイリング手順</h2>

<p>以下コマンドを計算ノードのプロファイリング利用ユーザで実行し、 <strong>NPB</strong> の <strong>FT Class D</strong> のプロファイリング未取得用バイナリ（<strong>ft.D.x_wo_scorep</strong>）とプロファイリング取得用バイナリ（<strong>ft.D.x_wi_scorep</strong>）を作成します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mkdir</span> ~/<span class="sb">`</span><span class="nb">hostname</span><span class="sb">`</span> <span class="o">&amp;&amp;</span> <span class="nb">cd</span> ~/<span class="sb">`</span><span class="nb">hostname</span><span class="sb">`</span> <span class="o">&amp;&amp;</span> wget https://www.nas.nasa.gov/assets/npb/NPB3.4.3.tar.gz
<span class="nv">$ </span><span class="nb">tar</span> <span class="nt">-xvf</span> ./NPB3.4.3.tar.gz
<span class="nv">$ </span><span class="nb">cd </span>NPB3.4.3/NPB3.4-MPI
<span class="nv">$ </span><span class="nb">cp </span>config/make.def.template config/make.def
<span class="nv">$ </span>make ft <span class="nv">CLASS</span><span class="o">=</span>D
<span class="nv">$ </span><span class="nb">mv </span>bin/ft.D.x bin/ft.D.x_wo_scorep
<span class="nv">$ </span><span class="nb">sed</span> <span class="nt">-i</span> <span class="s1">'s/^MPIFC = mpif90/MPIFC = scorep-mpif90/g'</span> config/make.def
<span class="nv">$ </span>diff config/make.def.template config/make.def
32c32
&lt; MPIFC <span class="o">=</span> mpif90
<span class="nt">---</span>
<span class="o">&gt;</span> MPIFC <span class="o">=</span> scorep-mpif90
<span class="nv">$ </span>make clean
<span class="nv">$ </span>make ft <span class="nv">CLASS</span><span class="o">=</span>D
<span class="nv">$ </span><span class="nb">mv </span>bin/ft.D.x bin/ft.D.x_wi_scorep
</code></pre></div></div>

<p>次に、以下コマンドを計算ノードのプロファイリング利用ユーザで実行し、 <strong>asis</strong> の  <strong>所要時間</strong> を計測します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>mpirun <span class="nt">-n</span> 256 <span class="nt">--hostfile</span> ~/hostlist.txt <span class="nt">--bind-to</span> core <span class="nt">--map-by</span> ppr:16:package <span class="nt">--rank-by</span> fill <span class="nt">-x</span> <span class="nv">UCX_NET_DEVICES</span><span class="o">=</span>mlx5_2:1 ./bin/ft.D.x_wo_scorep 2&gt;&amp;1 | <span class="nb">grep</span> <span class="s2">"Time in seconds ="</span>
 Time <span class="k">in </span>seconds <span class="o">=</span>                    32.41
<span class="err">$</span>
</code></pre></div></div>

<p>次に、以下コマンドを計算ノードのプロファイリング利用ユーザで実行し、プロファイリング取得時の  <strong>所要時間</strong> を計測します。<br />
この実行により、カレントディレクトリにディレクトリ <strong>scorep_ft_256_sum</strong> が作成され、ここに取得したプロファイリングデータが格納されます。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>scalasca <span class="nt">-analyze</span> mpirun <span class="nt">-n</span> 256 <span class="nt">-machinefile</span> ~/hostlist.txt <span class="s2">"--bind-to core"</span> <span class="s2">"--map-by ppr:16:package"</span> <span class="s2">"--rank-by fill"</span> <span class="s2">"-x UCX_NET_DEVICES=mlx5_2:1"</span> ./bin/ft.D.x_wi_scorep 2&gt;&amp;1 | <span class="nb">grep</span> <span class="s2">"Time in seconds ="</span>
 Time <span class="k">in </span>seconds <span class="o">=</span>                    32.74
<span class="err">$</span>
</code></pre></div></div>

<p>以上より、 <strong>asis</strong> とプロファイリング取得時の  <strong>所要時間</strong> に大きな差が無い（32.41秒と32.74秒）ことが確認できました。<br />
もし両者に大きな差がある場合は、プロファイリングのオーバーヘッドの原因を調査し、プロファイリング対象を限定するフィルタを作成し、これを使用してプロファイリングのオーバーヘッドを排除します。<br />
この手順は、 <strong><a href="/ocitutorials/hpc/#2-3-プロファイリング関連tips集">OCI HPCプロファイリング関連Tips集</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/scorep-profiling/">Score-P・Scalasca・CubeGUIで並列アプリケーションをプロファイリング</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/scorep-profiling/#2-1-事前準備">2-1. 事前準備</a></strong> を参照してください。</p>

<p>次に、以下コマンドを計算ノードのプロファイリング利用ユーザで実行し、トータル時間を評価指標としたプロファイリング情報を作成します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>scalasca <span class="nt">-examine</span> <span class="nt">-s</span> <span class="nt">-x</span> <span class="s2">"-s totaltime"</span> scorep_ft_256_sum/
</code></pre></div></div>

<p>次に、以下コマンドを計算ノードのプロファイリング利用ユーザで実行し、作成したプロファイリング情報を表示します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">head</span> <span class="nt">-n</span> 35 scorep_ft_256_sum/scorep.score

Estimated aggregate size of event trace:                   2462MB
Estimated requirements <span class="k">for </span>largest trace buffer <span class="o">(</span>max_buf<span class="o">)</span>: 10MB
Estimated memory requirements <span class="o">(</span>SCOREP_TOTAL_MEMORY<span class="o">)</span>:       12MB
<span class="o">(</span>hint: When tracing <span class="nb">set </span><span class="nv">SCOREP_TOTAL_MEMORY</span><span class="o">=</span>12MB to avoid intermediate flushes
 or reduce requirements using USR regions filters.<span class="o">)</span>

flt     <span class="nb">type </span>max_buf[B]     visits <span class="nb">time</span><span class="o">[</span>s] <span class="nb">time</span><span class="o">[</span>%] <span class="nb">time</span>/visit[us]  region
         ALL 10,082,337 99,243,224 9134.33   100.0          92.04  ALL
         USR 10,075,130 99,198,680 4654.88    51.0          46.92  USR
         MPI      4,332     16,384 4479.29    49.0      273394.14  MPI
         COM      2,834     27,904    0.15     0.0           5.43  COM
      SCOREP         41        256    0.01     0.0          40.52  SCOREP

         MPI      1,836      6,912 3631.26    39.8      525355.87  MPI_Alltoall
         USR  9,165,312 90,243,072 2181.55    23.9          24.17  fftz2
         USR        702      6,912  717.82     7.9      103850.86  transpose2_local
         MPI      1,700      6,400  587.28     6.4       91762.80  MPI_Reduce
         USR        650      6,400  448.36     4.9       70056.11  evolve
         USR      1,404     13,824  386.17     4.2       27935.09  cffts1
         USR        702      6,912  385.43     4.2       55762.38  cffts2
         USR        702      6,912  258.85     2.8       37449.58  transpose2_finish
         MPI         84        256  248.91     2.7      972291.31  MPI_Init
         USR         26        256  164.06     1.8      640862.56  free_space
         USR         52        512   47.06     0.5       91917.14  compute_indexmap
         USR    898,560  8,847,360   43.95     0.5           4.97  cfftz
         USR        208      2,048   21.56     0.2       10525.81  vranlc
         MPI         68        256    7.20     0.1       28111.36  MPI_Barrier
         MPI         84        256    4.43     0.0       17290.94  MPI_Finalize
         MPI        168        512    0.16     0.0         319.79  MPI_Comm_split
         MPI        340      1,280    0.05     0.0          42.35  MPI_Bcast
         COM        650      6,400    0.04     0.0           6.43  checksum
         COM        702      6,912    0.03     0.0           4.26  transpose2_global
         COM        650      6,400    0.03     0.0           4.51  transpose_x_yz
         COM         26        256    0.03     0.0         100.69  ft


<span class="err">$</span>
</code></pre></div></div>

<p>この出力から、MPI通信関数にフォーカスすると、以下のことがわかります。</p>

<ul>
  <li><strong>MPI_Alltoall</strong> に3,631秒を要しトータル時間の約 <strong>40%</strong> を占めている</li>
  <li><strong>MPI_Reduce</strong> に587秒を要しトータル時間の約 <strong>6%</strong> を占めている</li>
  <li><strong>MPI_Init</strong> に249秒を要しトータル時間の約 <strong>3%</strong> を占めている</li>
</ul>

<p>次に、以下コマンドを計算ノードのプロファイリング利用ユーザで実行し、プロファイリングデータ格納ディレクトリを次の実行に備えて別名に変更します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mv </span>scorep_ft_256_sum scorep_ft_256_sum_def
</code></pre></div></div>

<p>以降は、Bastionノードの <strong>CubeGUI</strong> でプロファイリングを継続するため、計算ノードのディレクトリ <strong>scorep_ft_256_sum_def</strong> をBastionノードにコピーします。</p>

<p>次に、以下コマンドをBastionノードのプロファイリング利用ユーザで実行し、 <strong>CubeGUI</strong> を起動します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>cube path_to_dir/scorep_ft_256_sum_def/profile.cubex
</code></pre></div></div>

<p>次に、評価指標軸の <strong>Time</strong> をクリックします。</p>

<p><img src="cubegui_page01.png" alt="画面ショット" /></p>

<p>次に、コールツリー軸領域の任意の箇所をクリックしたのちに <strong>Ctrl-F</strong> キーを入力し、表示される検索フィールドに <strong>Alltoall</strong> と入力します。</p>

<p><img src="cubegui_page02.png" alt="画面ショット" /></p>

<p>次に、表示された <strong>MPI_Alltoall(2)</strong> プルダウンメニューを選択します。</p>

<p><img src="cubegui_page03.png" alt="画面ショット" /></p>

<p>次に、コールツリー軸の表示から <strong>MPI_Alltoall</strong> がサブルーチン/関数の <strong>transpose_xy_z</strong> と <strong>transpose_x_yz</strong> の2か所から呼ばれており、後者のほうが圧倒的に時間を要していることがわかります。</p>

<p><img src="cubegui_page04.png" alt="画面ショット" /></p>

<p>次に、評価指標軸の <strong>bytes_sent</strong> をクリックします。</p>

<p><img src="cubegui_page05.png" alt="画面ショット" /></p>

<p>次に、コールツリー軸の <strong>MPI_Alltoall</strong> の時間を要している側をクリックします。</p>

<p><img src="cubegui_page06.png" alt="画面ショット" /></p>

<p>次に、システム位置軸の <strong>machine Linux</strong> をクリックします。</p>

<p><img src="cubegui_page07.png" alt="画面ショット" /></p>

<p>次に、システム位置軸の8ノードの計算ノードのうち1ノードをクリックします。</p>

<p><img src="cubegui_page08.png" alt="画面ショット" /></p>

<p>次に、選択した計算ノード配下に32個のMPIプロセスが表示され、各MPIプロセスが3.36 GBのデータを <strong>MPI_Alltoall</strong> で送信していることを突き止めます。</p>

<p><img src="cubegui_page09.png" alt="画面ショット" /></p>

<p>次に、他の計算ノードを選択し、同様に計算ノード配下の32個のMPIプロセスが同サイズのデータを <strong>MPI_Alltoall</strong> で送信していることを確認します。</p>

<p><img src="cubegui_page10.png" alt="画面ショット" /></p>

<p>次に、評価指標軸の <strong>Visits</strong> をクリックします。</p>

<p><img src="cubegui_page11.png" alt="画面ショット" /></p>

<p>次に、コールツリー軸の <strong>MPI_Alltoall</strong> の時間を要している側をクリックし、システム位置軸の表示から計算ノード配下の32個の全てのMPIプロセスが時間を要している側の <strong>MPI_Alltoall</strong> を25回呼び出していることを突き止めます。</p>

<p><img src="cubegui_page12.png" alt="画面ショット" /></p>

<hr />
<h1 id="3-チューニング">3. チューニング</h1>

<h2 id="3-0-概要">3-0. 概要</h2>

<p>本章は、先に取得したプロファイリング情報を元に、以下の手順でチューニングを実施します。</p>

<ul>
  <li><strong>ホットスポット</strong> に対するチューニング手法検討</li>
  <li>チューニング適用時のプロファイリング取得</li>
  <li>プロファイリング情報から <strong>ホットスポット</strong> に対するチューニングの効果を確認</li>
  <li>チューニング適用時の  <strong>所要時間</strong> 計測</li>
  <li><strong>asis</strong> とチューニング適用時の  <strong>所要時間</strong> 比較・チューニング効果確認</li>
</ul>

<h2 id="3-1-チューニング手順">3-1. チューニング手順</h2>

<p>先の <strong><a href="#2-プロファイリング">2. プロファイリング</a></strong> の結果から、以下のことが判明しました。</p>

<ul>
  <li>最も時間を要しているMPI関数は <strong>MPI_Alltoall</strong> で2か所から呼び出されている</li>
  <li><strong>transpose_x_yz</strong> から呼ばれる <strong>MPI_Alltoall</strong> が殆どの時間を占めこれを <strong>ホットスポット</strong> と特定</li>
  <li><strong>ホットスポット</strong> の <strong>MPI_Alltoall</strong> は以下の特性を有する
    <ul>
      <li>256個のMPIプロセスから均等に総量3.36 GBのデータが送信されている</li>
      <li>256個のMPIプロセスから均等に25回コールされている</li>
    </ul>
  </li>
</ul>

<p>ここで、 <strong>ホットスポット</strong> の <strong>MPI_Alltoall</strong> が各回とも同一メッセージサイズであると仮定し、このメッセージサイズを以下の計算式から求めます。</p>

<p>3.36 (GB) / 25 (回) / 256 (MPIプロセス) = <strong>525 KB</strong></p>

<p>以上の情報から、 <strong>OpenMPI</strong> の以下MPI通信をターゲットにチューニング手法を検討します。</p>

<ul>
  <li>MPI関数： <strong>MPI_Alltoall</strong></li>
  <li>ノード数： 8ノード</li>
  <li>ノード当たりプロセス数： 32</li>
  <li>メッセージサイズ： 525 KB</li>
</ul>

<p>ここで、 <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftune/">OpenMPIのMPI集合通信チューニング方法（BM.Optimized3.36編）</a></strong> の当該箇所である <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftune/#4-3-1-alltoall">4-3-1. Alltoall</a></strong> に於いて、最後に記載されている以下グラフの512 KBメッセージサイズ部分を確認し、</p>

<p><img src="ata_08_32_step3.png" alt="Alltoall 8 node 32 ppn" /></p>

<p>最も所要時間の短い黄色のグラフである以下のパラメータ設定が適していると判断、これをチューニング手法として採用します。</p>

<ul>
  <li>UCX_TLS： self,sm,ud</li>
  <li>UCX_RNDV_THRESH： intra:16kb,inter:128kb</li>
  <li>UCX_ZCOPY_THRESH： 128kb</li>
  <li><strong>NPS</strong>： 1</li>
  <li>プロセス配置： サイクリック分割（※5）</li>
  <li>coll_hcoll_enable： 0</li>
</ul>

<p>※5）プロセス分割方法にサイクリック分割を使用する場合の実行方法は、 <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/cpu-binding/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Optimized3.36編）</a></strong> を参照してください。</p>

<p>次に、以下コマンドを計算ノードのプロファイリング利用ユーザで実行し、チューニング手法適用時のプロファイリングを取得します。<br />
この実行により、カレントディレクトリにディレクトリ <strong>scorep_ft_256_sum</strong> が作成され、ここに取得したプロファイリングデータが格納されます。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>scalasca <span class="nt">-analyze</span> mpirun <span class="nt">-n</span> 256 <span class="nt">-machinefile</span> ~/hostlist.txt <span class="s2">"--map-by pe-list=</span><span class="sb">`</span><span class="k">for </span>i <span class="k">in</span> <span class="se">\`</span><span class="nb">seq </span>0 15<span class="se">\`</span><span class="p">;</span> <span class="k">do </span><span class="nb">seq</span> <span class="nt">-s</span>, <span class="nv">$i</span> 18 35 | <span class="nb">tr</span> <span class="s1">'\n'</span> <span class="s1">','</span><span class="p">;</span> <span class="k">done</span> | <span class="nb">sed</span> <span class="s1">'s/,$//g'</span><span class="sb">`</span><span class="s2">:ordered"</span> <span class="s2">"--mca coll_hcoll_enable 0"</span> <span class="s2">"-x UCX_NET_DEVICES=mlx5_2:1"</span> <span class="s2">"-x UCX_TLS=self,sm,ud"</span> <span class="s2">"-x UCX_RNDV_THRESH=intra:16kb,inter:128kb"</span> <span class="s2">"-x UCX_ZCOPY_THRESH=128kb"</span> ./bin/ft.D.x_wi_scorep
</code></pre></div></div>

<p>次に、以下コマンドを計算ノードのプロファイリング利用ユーザで実行し、トータル時間を評価指標としたプロファイリング情報を作成します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>scalasca <span class="nt">-examine</span> <span class="nt">-s</span> <span class="nt">-x</span> <span class="s2">"-s totaltime"</span> scorep_ft_256_sum/
</code></pre></div></div>

<p>次に、以下コマンドを計算ノードのプロファイリング利用ユーザで実行し、作成したプロファイリング情報を表示します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">head</span> <span class="nt">-n</span> 35 scorep_ft_256_sum/scorep.score

Estimated aggregate size of event trace:                   2462MB
Estimated requirements <span class="k">for </span>largest trace buffer <span class="o">(</span>max_buf<span class="o">)</span>: 10MB
Estimated memory requirements <span class="o">(</span>SCOREP_TOTAL_MEMORY<span class="o">)</span>:       12MB
<span class="o">(</span>hint: When tracing <span class="nb">set </span><span class="nv">SCOREP_TOTAL_MEMORY</span><span class="o">=</span>12MB to avoid intermediate flushes
 or reduce requirements using USR regions filters.<span class="o">)</span>

flt     <span class="nb">type </span>max_buf[B]     visits <span class="nb">time</span><span class="o">[</span>s] <span class="nb">time</span><span class="o">[</span>%] <span class="nb">time</span>/visit[us]  region
         ALL 10,082,337 99,243,224 8211.33   100.0          82.74  ALL
         USR 10,075,130 99,198,680 4596.88    56.0          46.34  USR
         MPI      4,332     16,384 3614.27    44.0      220597.31  MPI
         COM      2,834     27,904    0.17     0.0           6.00  COM
      SCOREP         41        256    0.01     0.0          34.58  SCOREP

         MPI      1,836      6,912 3378.02    41.1      488718.85  MPI_Alltoall
         USR  9,165,312 90,243,072 2181.79    26.6          24.18  fftz2
         USR        702      6,912  723.68     8.8      104699.28  transpose2_local
         USR        702      6,912  412.58     5.0       59690.86  cffts2
         USR      1,404     13,824  396.78     4.8       28702.14  cffts1
         USR        650      6,400  394.26     4.8       61603.52  evolve
         USR        702      6,912  278.05     3.4       40226.67  transpose2_finish
         MPI         84        256  186.26     2.3      727571.60  MPI_Init
         USR         26        256   97.82     1.2      382113.92  free_space
         USR         52        512   47.86     0.6       93468.91  compute_indexmap
         USR    898,560  8,847,360   43.55     0.5           4.92  cfftz
         MPI      1,700      6,400   34.49     0.4        5389.22  MPI_Reduce
         USR        208      2,048   20.45     0.2        9986.40  vranlc
         MPI         84        256   10.93     0.1       42714.15  MPI_Finalize
         MPI         68        256    3.03     0.0       11818.25  MPI_Barrier
         MPI        340      1,280    0.91     0.0         709.50  MPI_Bcast
         MPI        168        512    0.62     0.0        1217.71  MPI_Comm_split
         COM        650      6,400    0.05     0.0           8.34  checksum
         COM        702      6,912    0.03     0.0           4.46  transpose2_global
         COM        650      6,400    0.03     0.0           4.73  transpose_x_yz
         COM         26        256    0.03     0.0         103.13  ft
<span class="err">$</span>
</code></pre></div></div>

<p>この出力から、 <strong>MPI_Alltoall</strong> の時間が3,631秒から3,378秒に減少しており、チューニング
の効果が確認できます。<br />
またチューニング適用の副次的な効果として、 <strong>MPI_Reduce</strong> の時間が587秒から34秒に減少していることも確認できます。</p>

<p>次に、以下コマンドを計算ノードのプロファイリング利用ユーザで実行し、チューニング適用時の  <strong>所要時間</strong> を計測します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>mpirun <span class="nt">-n</span> 256 <span class="nt">--hostfile</span> ~/hostlist.txt <span class="nt">--map-by</span> pe-list<span class="o">=</span><span class="sb">`</span><span class="k">for </span>i <span class="k">in</span> <span class="se">\`</span><span class="nb">seq </span>0 15<span class="se">\`</span><span class="p">;</span> <span class="k">do </span><span class="nb">seq</span> <span class="nt">-s</span>, <span class="nv">$i</span> 18 35 | <span class="nb">tr</span> <span class="s1">'\n'</span> <span class="s1">','</span><span class="p">;</span> <span class="k">done</span> | <span class="nb">sed</span> <span class="s1">'s/,$//g'</span><span class="sb">`</span>:ordered <span class="nt">--mca</span> coll_hcoll_enable 0 <span class="nt">-x</span> <span class="nv">UCX_NET_DEVICES</span><span class="o">=</span>mlx5_2:1 <span class="nt">-x</span> <span class="nv">UCX_TLS</span><span class="o">=</span>self,sm,ud <span class="nt">-x</span> <span class="nv">UCX_RNDV_THRESH</span><span class="o">=</span>intra:16kb,inter:128kb <span class="nt">-x</span> <span class="nv">UCX_ZCOPY_THRESH</span><span class="o">=</span>128kb ./bin/ft.D.x_wo_scorep 2&gt;&amp;1 | <span class="nb">grep</span> <span class="s2">"Time in seconds ="</span>
 Time <span class="k">in </span>seconds <span class="o">=</span>                    28.97
<span class="err">$</span>
</code></pre></div></div>

<p>この結果から、 <strong>asis</strong> とチューニング適用時の  <strong>所要時間</strong> を比較し、チューニングの効果を確認します。</p>

<p>以下は、本プロファイリング・チューニング関連Tips環境で  <strong>所要時間</strong> を計測した結果です。<br />
この計測結果は、 <strong>asis</strong> とチューニング適用時をそれぞれ5回計測し、最大値と最小値を除く3回の算術平均です。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">asis</th>
      <th style="text-align: center">チューニング適用時</th>
      <th style="text-align: center">性能向上比</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">32.6秒</td>
      <td style="text-align: center">29.1秒</td>
      <td style="text-align: center"><strong>12.0％</strong></td>
    </tr>
  </tbody>
</table>

        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 更新日時:</strong> <time class="dt-published" datetime="2025-08-13T08:16:11+09:00">August 13, 2025</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">共有</h4>

  <a href="https://x.com/intent/tweet?text=%E3%83%97%E3%83%AD%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AA%E3%83%B3%E3%82%B0%E6%83%85%E5%A0%B1%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E4%B8%A6%E5%88%97%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E6%96%B9%E6%B3%95%20https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fbenchmark%2Fprofiling-tuning%2F" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fbenchmark%2Fprofiling-tuning%2F" class="btn btn--facebook" aria-label="Share on Facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Facebook">
    <i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://oracle-japan.github.io/ocitutorials/hpc/benchmark/profiling-tuning/" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

  <a href="https://bsky.app/intent/compose?text=%E3%83%97%E3%83%AD%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AA%E3%83%B3%E3%82%B0%E6%83%85%E5%A0%B1%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E4%B8%A6%E5%88%97%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E6%96%B9%E6%B3%95%20https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fbenchmark%2Fprofiling-tuning%2F" class="btn btn--bluesky" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Bluesky">
    <i class="fab fa-fw fa-bluesky" aria-hidden="true"></i><span> Bluesky</span>
  </a>
</section>


      
  <nav class="pagination">
    
      <a href="/ocitutorials/hpc/benchmark/scorep-profiling/" class="pagination--pager" title="Score-P・Scalasca・CubeGUIで並列アプリケーションをプロファイリング">前へ</a>
    
    
      <a href="/ocitutorials/hpc/tech-knowhow/howto-connect-clusternetwork/" class="pagination--pager" title="クラスタネットワーキングイメージを使ったクラスタ・ネットワーク接続方法">次へ</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">関連記事</h2>
  <div class="grid__wrapper">
    
  </div>
</div>

  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="検索キーワードを入力してください..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>フォロー</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/#ocijp" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/oracle-japan" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/ocitutorials/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2025 <a href="https://oracle-japan.github.io">Oracle Cloud Infrastructure チュートリアル</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/ocitutorials/assets/js/main.min.js"></script>




<script src="/ocitutorials/assets/js/lunr/lunr.min.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-store.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6W7FEC5CEH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6W7FEC5CEH', { 'anonymize_ip': false});
</script>







  
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
  
    <script src="/ocitutorials/assets/js/clipboardrouge.js"></script>
  
    <script src="/ocitutorials/assets/js/tabs.js"></script>
  
    <script src="/ocitutorials/assets/js/sidebar.js"></script>
  


  </body>
</html>
