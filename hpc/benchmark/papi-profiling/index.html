<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.2 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="ja-JP" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>PAPIでHPCアプリケーションをプロファイリング | Oracle Cloud Infrastructure チュートリアル</title>
<meta name="description" content="HPCワークロードの実行に最適なベアメタル・インスタンスでアプリケーションを実行する場合、高価な計算資源を有効活用出来ているかを検証するため、アプリケーションのプロファイリングを実施することが一般的です。PAPIは、HPCワークロード向けベ アメタル・シェイプ に採用されているIntel Ice LakeやAMD EPYC 9004シリーズのCPUが持つハードウェアカウンタから浮動小数点演算数やキャッシュヒット数といったプロファイリングに有益な情報を取得するAPIを提供し、HPCアプリケーションのプロファイリングに欠かせないツールとなっています。本プロファイリング関連Tipsは、ベアメタル・インスタンス上で実行するHPCアプリケーションをPAPIを使ってプロファイリングする方法を解説します。">


  <meta name="author" content="Oracle Japan Solution Engineers">
  
  <meta property="article:author" content="Oracle Japan Solution Engineers">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ja_JP">
<meta property="og:site_name" content="Oracle Cloud Infrastructure チュートリアル">
<meta property="og:title" content="PAPIでHPCアプリケーションをプロファイリング">
<meta property="og:url" content="https://oracle-japan.github.io/ocitutorials/hpc/benchmark/papi-profiling/">


  <meta property="og:description" content="HPCワークロードの実行に最適なベアメタル・インスタンスでアプリケーションを実行する場合、高価な計算資源を有効活用出来ているかを検証するため、アプリケーションのプロファイリングを実施することが一般的です。PAPIは、HPCワークロード向けベ アメタル・シェイプ に採用されているIntel Ice LakeやAMD EPYC 9004シリーズのCPUが持つハードウェアカウンタから浮動小数点演算数やキャッシュヒット数といったプロファイリングに有益な情報を取得するAPIを提供し、HPCアプリケーションのプロファイリングに欠かせないツールとなっています。本プロファイリング関連Tipsは、ベアメタル・インスタンス上で実行するHPCアプリケーションをPAPIを使ってプロファイリングする方法を解説します。">



  <meta property="og:image" content="https://oracle-japan.github.io/ocitutorials/assets/images/rh01-cloud-home-pine-background.jpg">





  <meta property="article:published_time" content="2025-07-26T13:05:45+09:00">






<link rel="canonical" href="https://oracle-japan.github.io/ocitutorials/hpc/benchmark/papi-profiling/">












<!-- end _includes/seo.html -->



  <link href="/ocitutorials/feed.xml" type="application/atom+xml" rel="alternate" title="Oracle Cloud Infrastructure チュートリアル Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/ocitutorials/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-32.png" sizes="32x32">
<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-128.png" sizes="128x128">
<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-192.png" sizes="192x192">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-120.png" sizes="120x120">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-152.png" sizes="152x152">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-180.png" sizes="180x180">
<link rel="shortcut icon" type="image/x-icon" href="/ocitutorials/assets/favicon/favicon.ico">
<link rel="manifest" href="/ocitutorials/assets/favicon/site.webmanifest">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/ocitutorials/"><img src="/ocitutorials/assets/images/social-og-oracle-badge.jpg" alt="OCI チュートリアル"></a>
        
        <a class="site-title" href="/ocitutorials/">
          OCI チュートリアル
          <span class="site-subtitle">Oracle Cloud Infrastructure を使ってみよう</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/ocitutorials/#%E3%83%81%E3%83%A5%E3%83%BC%E3%83%88%E3%83%AA%E3%82%A2%E3%83%AB%E3%82%B3%E3%83%B3%E3%83%86%E3%83%B3%E3%83%84%E4%B8%80%E8%A6%A7"
                
                
              >チュートリアル一覧</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ocitutorials/about/"
                
                
              >このサイトについて</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">メニュー</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style=" background-image: linear-gradient(rgba(34, 66, 55, 0.7), rgba(34, 66, 55, 0.7)), url('/ocitutorials/assets/images/rh01-cloud-home-pine-background.jpg');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          PAPIでHPCアプリケーションをプロファイリング

        
      </h1>
      
        <p class="page__lead">HPCワークロードの実行に最適なベアメタル・インスタンスでアプリケーションを実行する場合、高価な計算資源を有効活用出来ているかを検証するため、アプリケーションのプロファイリングを実施することが一般的です。PAPIは、HPCワークロード向けベ <strong>アメタル・シェイプ</strong> に採用されているIntel Ice LakeやAMD EPYC 9004シリーズのCPUが持つハードウェアカウンタから浮動小数点演算数やキャッシュヒット数といったプロファイリングに有益な情報を取得するAPIを提供し、HPCアプリケーションのプロファイリングに欠かせないツールとなっています。本プロファイリング関連Tipsは、ベアメタル・インスタンス上で実行するHPCアプリケーションをPAPIを使ってプロファイリングする方法を解説します。
</p>
      
      


      
    </div>
  
  
</div>






  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/" itemprop="item"><span itemprop="name">ホーム</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">></span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/hpc" itemprop="item"><span itemprop="name">Hpc</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">></span>
      
    
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/benchmark" itemprop="item"><span itemprop="name">Benchmark</span></a>
          <meta itemprop="position" content="3" />
        </li>
        <span class="sep">></span>
      
    
      
      
        <li class="current">PAPIでHPCアプリケーションをプロファイリング</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">メニュー</label>
  <ul class="nav__items">
    <li>
      
      <a href=""><span class="nav__sub-title">HPC編</span></a>
      <ul>
        
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-cluster-network/">HPCクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster-withterraform/">HPCクラスタを構築する(基礎インフラ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster/">HPCクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster-withautoscaling/">HPCクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-ml-instance/">GPUインスタンスで機械学習にトライ</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-ml-instance-cntnd/">GPUインスタンスで分散機械学習環境を構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster/">GPUクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withterraform/">GPUクラスタを構築する(基礎インフラ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withstack/">GPUクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withautoscaling/">GPUクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withubuntu/">GPUクラスタを構築する(Ubuntu OS編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server-fss/">ファイル・ストレージでファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-lustre-server-fswl/">File Storage with Lustreでファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server/">ブロック・ボリュームでファイル共有ストレージを構築する（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server-e6/">ブロック・ボリュームでファイル共有ストレージを構築する（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server-nvme/">短期保存データ用高速ファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-backup-server/">ベア・メタル・インスタンスNFSサーバ向けバックアップサーバを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-base/">ブロック・ボリュームNFSサーバと基礎インフラ編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-stack/">ブロック・ボリュームNFSサーバと自動構築編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl/">HPL実行方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl-e5/">HPL実行方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl-e6/">HPL実行方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream/">STREAM実行方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream-e5/">STREAM実行方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream-e6/">STREAM実行方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-imb/">Intel MPI Benchmarks実行方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-nccltests/">NCCL Tests実行方法（BM.GPU4.8/BM.GPU.A100-v2.8編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-nccltests-h100/">NCCL Tests実行方法（BM.GPU.H100.8編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/bios-setting/">パフォーマンスに関連するベアメタルインスタンスのBIOS設定方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/stop-unused-service/">不要サービス停止によるパフォーマンスチューニング方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/topology-aware-cn-tuning/">クラスタ・ネットワークのトポロジーを考慮したノード間通信最適化方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openfoam-tuning/">CFD解析フローのコストパフォーマンを向上させるOpenFOAM関連Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/">OpenMPIのMPI通信性能に影響するパラメータとその関連Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/cpu-binding/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/cpu-binding-e5/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/cpu-binding-e6/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftune/">OpenMPIのMPI集合通信チューニング方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftune-e5/">OpenMPIのMPI集合通信チューニング方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftune-e6/">OpenMPIのMPI集合通信チューニング方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/papi-profiling/" class="active">PAPIでHPCアプリケーションをプロファイリング</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/scorep-profiling/">Score-P・Scalasca・CubeGUIで並列アプリケーションをプロファイリング</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/profiling-tuning/">プロファイリング情報に基づく並列アプリケーションチューニング方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-connect-clusternetwork/">クラスタネットワーキングイメージを使ったクラスタ・ネットワーク接続方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/rdma-interface-configure/">クラスタ・ネットワーク接続用ネットワークインターフェース作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/">クラスタネットワーキングイメージの選び方</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-create-cnenabled-osimage/">クラスタ・ネットワーク未対応OSを使ったクラスタ・ネットワーク接続方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/determine-cnrelated-issue/">クラスタ・ネットワークに接続する計算/GPUノード作成時の問題判別方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-get-cnrelated-statistics/">クラスタ・ネットワーク統計情報の取得方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/nvme-filesystem/">ベアメタルインスタンスのNVMe SSDローカルディスク領域ファイルシステム作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-configure-sharedstorage/">HPC/GPUクラスタ向けファイル共有ストレージの最適な構築手法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/bv-sharedstorage-recovery/">ブロック・ボリュームを使用するNFSサーバのインスタンス障害からの復旧方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/boot-volume-extension/">計算/GPUノードのブート・ボリューム動的拡張方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-choose-osbackuptool/">ファイル共有ストレージ向けバックアップ環境の最適な構築手法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-name-resolution/">計算/GPUノードの効果的な名前解決方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-os-customization/">計算/GPUノードデプロイ時の効果的なOSカスタマイズ方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-host-list/">計算/GPUノードのホスト名リスト作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/cluster-resize/">計算/GPUノードの追加・削除・入れ替え方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/cluster-with-pdsh/">pdshで効率的にクラスタ管理オペレーションを実行</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/instance-principal-auth/">オンデマンドクラスタ実現のためのインスタンス・プリンシパル認証設定方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/log-monitoring/">OCIロギングとGrafanaを使用したHPC/GPUクラスタのログ監視方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/metric-monitoring/">OCIモニタリングとGrafanaを使用したHPC/GPUクラスタのメトリック監視方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/gpu-with-ubuntu/">UbuntuをOSとする機械学習ワークロード向けGPUノード構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするUCX通信フレームワークベースのOpenMPI構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/setup-slurm-cluster/">Slurmによるリソース管理・ジョブ管理システム構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/install-blas/">線形代数演算ライブラリインストール・利用方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/install-openfoam/">OpenFOAMインストール・利用方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/slurm-tips/">Slurmによるリソース管理・ジョブ管理システム運用Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/build-oraclelinux-hpcenv/">Oracle Linuxプラットフォーム・イメージベースのHPCワークロード実行環境構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/kdump-on-baremetal/">ベアメタルインスタンスのカーネルダンプ取得方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/site-to-site-vpn/">サイト間VPNによるOCIとの拠点間接続方法</a></li></p>
        
      </ul>
    </li>
  </ul>
</nav>
    
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="PAPIでHPCアプリケーションをプロファイリング">
    <meta itemprop="description" content="HPCワークロードの実行に最適なベアメタル・インスタンスでアプリケーションを実行する場合、高価な計算資源を有効活用出来ているかを検証するため、アプリケーションのプロファイリングを実施することが一般的です。PAPIは、HPCワークロード向けベ アメタル・シェイプ に採用されているIntel Ice LakeやAMD EPYC 9004シリーズのCPUが持つハードウェアカウンタから浮動小数点演算数やキャッシュヒット数といったプロファイリングに有益な情報を取得するAPIを提供し、HPCアプリケーションのプロファイリングに欠かせないツールとなっています。本プロファイリング関連Tipsは、ベアメタル・インスタンス上で実行するHPCアプリケーションをPAPIを使ってプロファイリングする方法を解説します。">
    <meta itemprop="datePublished" content="2025-07-26T13:05:45+09:00">
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> 目次</h4></header>
              <ul class="toc__menu"><li><a href="#0-概要">0. 概要</a></li><li><a href="#1-前提条件ソフトウェアインストールセットアップ">1. 前提条件ソフトウェアインストール・セットアップ</a></li><li><a href="#2-papiインストールセットアップ">2. PAPIインストール・セットアップ</a></li><li><a href="#3-サンプルプログラムコンパイル">3. サンプルプログラムコンパイル</a></li><li><a href="#4-サンプルプログラム実行">4. サンプルプログラム実行</a></li><li><a href="#5-プロファイリング結果確認">5. プロファイリング結果確認</a></li></ul>
            </nav>
          </aside>
        
        <hr />
<h1 id="0-概要">0. 概要</h1>

<p><strong><a href="https://icl.utk.edu/papi/">PAPI</a></strong> は、異なるプラットフォーム間を共通のインターフェースで利用できるように設計された性能解析ツールで、プロファイリング対象のアプリケーションからAPIをコールすることで、CPU/GPUをはじめとする様々なハードウェアからプロファイリングに有益な情報を収集します。<br />
この <strong>PAPI</strong> の可搬性は、ハードウェア固有の部分を吸収する下層と、プロファイリングを行うアプリケーション開発者が利用する抽象化された上位層にソフトウェアスタックを分割することで、これを実現しています。これらの関係は、 <strong><a href="https://icl.utk.edu/projects/papi/files/documentation/PAPI_USER_GUIDE_23.htm#ARCHITECTURE">ここ</a></strong> に記載のアーキテクチャ図が参考になります。</p>

<p><strong>PAPI</strong> のAPIは、HPCアプリケーションをプロファイリングするユースケースの場合、 <strong>Low Level API</strong> と <strong><a href="https://github.com/icl-utk-edu/papi/wiki/PAPI-HL">High Level API</a></strong> から選択して利用することが出来ます。<br />
<strong>High Level API</strong> は、内部で <strong>Low Level API</strong> を使用することでより高機能のプロファイリングが可能なAPIを提供し、 <strong>Low Level API</strong> に対して以下の利点があります。</p>

<ul>
  <li>より少ないAPIコールで同じプロファイリング機能を実現（※1）</li>
  <li>コードを修正することなく利用するハードウェアカウンタの変更が可能（※2）</li>
  <li>プロファイリング情報の出力制御が不要（※3）</li>
</ul>

<p>※1）最も少ないケースは、プログラム中の計測範囲の前後に2回APIコールを挟み込むだけです。<br />
※2）アプリケーション実行時の環境変数で利用するハードウェアカウンタを指定します。これに対し <strong>Low Level API</strong> は、APIをコールして利用するハードウェアカウンタを指定するため、これを変更する場合再コンパイルが必要です。<br />
※3）計測範囲の最後を指定するAPIがコールされると、環境変数に指定したディレクトリにプロファイリング情報を格納したファイルが自動的に作成されます。これに対し <strong>Low Level API</strong> は、明示的に取得したハードウェアカウンタの値を出力する必要があります。</p>

<p>以上を踏まえて本プロファイリング関連Tipsでは、 <strong>High Level API</strong> を使用するサンプルプログラムを使って <strong>PAPI</strong> の利用方法を解説します。</p>

<p><strong>PAPI</strong> は、 <strong>Intel Ice Lake</strong> プロセッサを搭載する <strong><a href="https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized">BM.Optimized3.36</a></strong> の場合、以下のようなハードウェアカウンタを利用することが可能です。<br />
利用可能なハードウェアカウンタ一覧は、 <strong><a href="#2-papiインストールセットアップ">2. PAPIインストール・セットアップ</a></strong> のステップ3の手順で取得することが可能です。</p>

<ul>
  <li>浮動小数点演算数</li>
  <li>浮動小数点演算インストラクション数</li>
  <li>L1/L2/L3の各キャッシュヒット/ミス数</li>
  <li>総サイクル数</li>
</ul>

<p>各ソフトウェアは、以下のバージョンを前提とします。</p>

<ul>
  <li>OS ： <strong>Oracle Linux</strong> 8.10ベースのHPC <strong><a href="/ocitutorials/hpc/#5-13-クラスタネットワーキングイメージ">クラスタネットワーキングイメージ</a></strong> （※4）</li>
  <li><strong><a href="https://www.open-mpi.org/">OpenMPI</a></strong> ：5.0.6（※5）</li>
  <li><strong>PAPI</strong> ：7.2.0</li>
</ul>

<p>※4）<strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/">クラスタネットワーキングイメージの選び方</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/#1-クラスタネットワーキングイメージ一覧">1. クラスタネットワーキングイメージ一覧</a></strong> のイメージ <strong>No.12</strong> です。<br />
※5） <strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするUCX通信フレームワークベースのOpenMPI構築方法</a></strong> に従って構築された <strong>OpenMPI</strong> です。</p>

<p>また本プロファイリング関連Tipsで使用するインスタンスは、シェイプを <strong>BM.Optimized3.36</strong> 、OSを <strong>Oracle Linux</strong> 8.10ベースのHPC <strong>クラスタネットワーキングイメージ</strong> として予めデプロイしておきますが、この手順は <strong><a href="https://oracle-japan.github.io/ocitutorials/">OCIチュートリアル</a></strong> の  <strong><a href="https://oracle-japan.github.io/ocitutorials/beginners/creating-compute-instance">その3 - インスタンスを作成する</a></strong> を参照してください。</p>

<p>以降では、以下の順に解説を進めます。</p>

<ul>
  <li>前提条件ソフトウェアインストール・セットアップ</li>
  <li>PAPIインストール・セットアップ</li>
  <li>サンプルプログラムコンパイル</li>
  <li>サンプルプログラム実行</li>
  <li>プロファイリング結果確認</li>
</ul>

<hr />
<h1 id="1-前提条件ソフトウェアインストールセットアップ">1. 前提条件ソフトウェアインストール・セットアップ</h1>

<p>本章は、本プロファイリングTipsで使用する <strong>PAPI</strong> 機能を検証するために必要な前提条件ソフトウェアとして、 <strong>OpenMPI</strong> をインストール・セットアップします。</p>

<p>この方法は、 <strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするUCX通信フレームワークベースのOpenMPI構築方法</a></strong> を参照してください。</p>

<hr />
<h1 id="2-papiインストールセットアップ">2. PAPIインストール・セットアップ</h1>

<p>本章は、 <strong>PAPI</strong> をインストールし、利用に必要な環境設定を行います。<br />
なお <strong>PAPI</strong> は、以降のインストール手順の中で環境に合わせた構築を行うため、 <strong>PAPI</strong> を利用する環境（本プロファイリング関連Tipsでは <strong>BM.Optimized3.36</strong> ）以外で本手順を実施（いわゆるクロスコンパイル）した場合、想定通りに動作しない場合があることに留意します。</p>

<p>以下コマンドをopcユーザで実行し、 <strong>PAPI</strong> をインストールします。<br />
これにより、 <strong>PAPI</strong> のライブラリ群が <strong>/usr/local/lib</strong> に、プロファイリング情報集計ツール等のユーティリティーツール群が <strong>/usr/local/bin</strong> にインストールされます。<br />
なお、makeコマンドの並列数は当該ノードのコア数に合わせて調整します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">mkdir</span> ~/<span class="sb">`</span><span class="nb">hostname</span><span class="sb">`</span> <span class="o">&amp;&amp;</span> <span class="nb">cd</span> ~/<span class="sb">`</span><span class="nb">hostname</span><span class="sb">`</span> <span class="o">&amp;&amp;</span> wget https://icl.utk.edu/projects/papi/downloads/papi-7.2.0.tar.gz
<span class="nv">$ </span><span class="nb">tar</span> <span class="nt">-xvf</span> ./papi-7.2.0.tar.gz
<span class="nv">$ </span><span class="nb">cd </span>papi-7.2.0/src <span class="o">&amp;&amp;</span> ./configure
<span class="nv">$ </span>make <span class="nt">-j</span> 36 <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>make <span class="nb">install</span>
</code></pre></div></div>

<p>次に、以下コマンドをopcユーザで実行し、コマンド出力の最後に <strong>PASSED</strong> が出力されることを以って、 <strong>PAPI</strong> が利用可能であることを確認します。<br />
このコマンドは、利用可能な全てのプリセットイベント（ここでは42個）をテストし、その結果を表示します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo</span> ./ctests/all_events

Trying all pre-defined events:
Adding PAPI_L1_DCM   successful
:
:
:
Adding PAPI_VEC_DP   successful
Adding PAPI_REF_CYC  successful
Successfully added, started and stopped 42 events.
PASSED
<span class="err">$</span>
</code></pre></div></div>

<p>次に、以下コマンドをopcユーザで実行し、利用可能なプリセットイベントを確認します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>papi_avail | <span class="nb">grep</span> ^PAPI_ | <span class="nb">awk</span> <span class="s1">'{if($3=="Yes")print $0}'</span>
PAPI_L1_DCM  0x80000000  Yes   No   Level 1 data cache misses
:
:
:
PAPI_REF_CYC 0x8000006b  Yes   No   Reference clock cycles

<span class="nv">$ </span>
</code></pre></div></div>

<p>次に、以下コマンドを <strong>PAPI</strong> を利用するユーザで実行し、 <strong>PAPI</strong> 実行に必要な環境変数を設定します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"export LD_LIBRARY_PATH=/usr/local/lib:</span><span class="se">\$</span><span class="s2">LD_LIBRARY_PATH"</span> | <span class="nb">tee</span> <span class="nt">-a</span> ~/.bashrc
<span class="nv">$ </span><span class="nb">source</span> ~/.bashrc
</code></pre></div></div>

<hr />
<h1 id="3-サンプルプログラムコンパイル">3. サンプルプログラムコンパイル</h1>

<p>本章は、 <strong>PAPI</strong> を使用してプロファイリング情報を取得する、非並列・OpenMP並列兼用版（<strong>matrix_multiply_thrpa_hl.f90</strong>）とMPI並列版（<strong>matrix_multiply_mpipa_hl.f90</strong> と <strong>para_range.f90</strong>）のサンプルプログラムをそれぞれコンパイルし、実行バイナリを作成します。</p>

<p>このサンプルプログラムは、サイズが3,000x3,000の正方行列を乱数で初期化した後にその行列積を求め、その結果を標準出力に出力します。この際、行列積を求める3重ループ前後で <strong>PAPI</strong> の <strong>High Level API</strong> 関数を呼び出すことで、プロファイリング情報をカーネル部分にフォーカスして取得します。<br />
ここでOpenMP並列版は、 <strong>PAPI</strong> 関数をスレッド生成後に呼び出している（<strong>!$OMP parallel</strong> と <strong>!$OMP end parallel</strong>の間に配置します。）ことに留意します。これは、 <strong>PAPI</strong> がスレッドの生成を検知できない制限から来ており、スレッド生成前に <strong>PAPI</strong> 関数を呼び出すと、その後生成されたスレッドが消費した分のプロファイリング情報を取得することができません。</p>

<p>[matrix_multiply_thrpa_hl.f90]</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>program main
        implicit none
        include <span class="s2">"f90papi.h"</span>
        integer i, j, k, l, m, n
        integer retval
        double precision, allocatable :: a<span class="o">(</span>:,:<span class="o">)</span>, b<span class="o">(</span>:,:<span class="o">)</span>, c<span class="o">(</span>:,:<span class="o">)</span>
        double precision alpha, beta
        
        <span class="nv">l</span><span class="o">=</span>3000<span class="p">;</span> <span class="nv">m</span><span class="o">=</span>l<span class="p">;</span> <span class="nv">n</span><span class="o">=</span>l
        <span class="nv">alpha</span><span class="o">=</span>1.0<span class="p">;</span> <span class="nv">beta</span><span class="o">=</span>0.0
        allocate<span class="o">(</span>a<span class="o">(</span>l,n<span class="o">)</span>, b<span class="o">(</span>n,m<span class="o">)</span>, c<span class="o">(</span>l,m<span class="o">))</span>
        call random_number<span class="o">(</span>a<span class="o">)</span>
        call random_number<span class="o">(</span>b<span class="o">)</span>
        a <span class="o">=</span> a - 0.5d0
        b <span class="o">=</span> b - 0.5d0
        c <span class="o">=</span> 0.0d0

<span class="o">!</span><span class="nv">$OMP</span> parallel
        call papif_hl_region_begin<span class="o">(</span><span class="s2">"computation"</span>, retval<span class="o">)</span>
        <span class="k">if</span> <span class="o">(</span> retval .ne. papi_ok <span class="o">)</span> <span class="k">then
                </span>write <span class="o">(</span><span class="k">*</span>,<span class="k">*</span><span class="o">)</span> <span class="s2">"PAPIf_hl_region_begin failed!"</span>
        end <span class="k">if</span>

<span class="o">!</span><span class="nv">$OMP</span> <span class="k">do
        do </span>i <span class="o">=</span> 1,l
                <span class="k">do </span>j <span class="o">=</span> 1,m
                        c<span class="o">(</span>i,j<span class="o">)</span> <span class="o">=</span> beta <span class="k">*</span> c<span class="o">(</span>i,j<span class="o">)</span>
                        <span class="k">do </span>k <span class="o">=</span> 1,n
                                c<span class="o">(</span>i,j<span class="o">)</span> <span class="o">=</span> c<span class="o">(</span>i,j<span class="o">)</span> + alpha <span class="k">*</span> a<span class="o">(</span>i,k<span class="o">)</span><span class="k">*</span>b<span class="o">(</span>k,j<span class="o">)</span>
                        end <span class="k">do
                </span>end <span class="k">do
        </span>end <span class="k">do</span>
<span class="o">!</span><span class="nv">$OMP</span> end <span class="k">do
        
        </span>call papif_hl_region_end<span class="o">(</span><span class="s2">"computation"</span>, retval<span class="o">)</span>
        <span class="k">if</span> <span class="o">(</span> retval .ne. papi_ok <span class="o">)</span> <span class="k">then
                </span>write <span class="o">(</span><span class="k">*</span>,<span class="k">*</span><span class="o">)</span> <span class="s2">"PAPIf_hl_region_end failed!"</span>
        end <span class="k">if</span>
<span class="o">!</span><span class="nv">$OMP</span> end parallel

        <span class="k">do </span>i <span class="o">=</span> lbound<span class="o">(</span>c,1<span class="o">)</span>, ubound<span class="o">(</span>c,1<span class="o">)</span>
            write<span class="o">(</span><span class="k">*</span>,<span class="k">*</span><span class="o">)</span> <span class="o">(</span>c<span class="o">(</span>i,j<span class="o">)</span>, <span class="nv">j</span><span class="o">=</span>lbound<span class="o">(</span>c,2<span class="o">)</span>, ubound<span class="o">(</span>c,2<span class="o">))</span>
        end <span class="k">do
</span>end program main
</code></pre></div></div>

<p>[matrix_multiply_mpipa_hl.f90]</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>program main
        implicit none
        include <span class="s2">"f90papi.h"</span>
        include <span class="s2">"mpif.h"</span>
        integer i, j, k, l, m, n
        integer nprocs, myrank, ista, iend
        integer retval, ierr
        double precision, allocatable :: a<span class="o">(</span>:,:<span class="o">)</span>, b<span class="o">(</span>:,:<span class="o">)</span>, c<span class="o">(</span>:,:<span class="o">)</span>, d<span class="o">(</span>:,:<span class="o">)</span>
        double precision alpha, beta
        
        <span class="nv">l</span><span class="o">=</span>3000<span class="p">;</span> <span class="nv">m</span><span class="o">=</span>l<span class="p">;</span> <span class="nv">n</span><span class="o">=</span>l
        <span class="nv">alpha</span><span class="o">=</span>1.0<span class="p">;</span> <span class="nv">beta</span><span class="o">=</span>0.0

        call mpi_init<span class="o">(</span>ierr<span class="o">)</span>
        call mpi_comm_size<span class="o">(</span>mpi_comm_world,nprocs,ierr<span class="o">)</span>
        call mpi_comm_rank<span class="o">(</span>mpi_comm_world,myrank,ierr<span class="o">)</span>
        call para_range<span class="o">(</span>1,m,nprocs,myrank,ista,iend<span class="o">)</span>

        allocate<span class="o">(</span>a<span class="o">(</span>l,n<span class="o">)</span>, b<span class="o">(</span>n,m<span class="o">)</span>, c<span class="o">(</span>l,iend-ista+1<span class="o">))</span>
        <span class="k">if</span> <span class="o">(</span><span class="nv">myrank</span><span class="o">==</span>0<span class="o">)</span> <span class="k">then
                </span>allocate<span class="o">(</span>d<span class="o">(</span>l,m<span class="o">))</span>
        endif

        <span class="k">if</span> <span class="o">(</span><span class="nv">myrank</span><span class="o">==</span>0<span class="o">)</span> <span class="k">then
                </span>call random_number<span class="o">(</span>a<span class="o">)</span>
                call random_number<span class="o">(</span>b<span class="o">)</span>
                a <span class="o">=</span> a - 0.5d0
                b <span class="o">=</span> b - 0.5d0
        endif
        c <span class="o">=</span> 0.0d0

        call mpi_bcast<span class="o">(</span>a,l<span class="k">*</span>n,mpi_double_precision,0,mpi_comm_world,ierr<span class="o">)</span>
        call mpi_bcast<span class="o">(</span>b,n<span class="k">*</span>m,mpi_double_precision,0,mpi_comm_world,ierr<span class="o">)</span>
 
        call PAPIf_hl_region_begin<span class="o">(</span><span class="s2">"computation"</span>, retval<span class="o">)</span>
        <span class="k">if</span> <span class="o">(</span> retval .ne. papi_ok <span class="o">)</span> <span class="k">then
                </span>write <span class="o">(</span><span class="k">*</span>,<span class="k">*</span><span class="o">)</span> <span class="s2">"PAPIf_hl_region_begin failed!"</span>
        end <span class="k">if

        do </span>i <span class="o">=</span> 1,l
                <span class="k">do </span>j <span class="o">=</span> ista,iend
                        c<span class="o">(</span>i,j-ista+1<span class="o">)</span> <span class="o">=</span> beta <span class="k">*</span> c<span class="o">(</span>i,j-ista+1<span class="o">)</span>
                        <span class="k">do </span>k <span class="o">=</span> 1,n
                                c<span class="o">(</span>i,j-ista+1<span class="o">)</span> <span class="o">=</span> c<span class="o">(</span>i,j-ista+1<span class="o">)</span> + alpha <span class="k">*</span> a<span class="o">(</span>i,k<span class="o">)</span><span class="k">*</span>b<span class="o">(</span>k,j<span class="o">)</span>
                        end <span class="k">do
                </span>end <span class="k">do
        </span>end <span class="k">do
        
        </span>call PAPIf_hl_region_end<span class="o">(</span><span class="s2">"computation"</span>, retval<span class="o">)</span>
        <span class="k">if</span> <span class="o">(</span> retval .ne. papi_ok <span class="o">)</span> <span class="k">then
                </span>write <span class="o">(</span><span class="k">*</span>,<span class="k">*</span><span class="o">)</span> <span class="s2">"PAPIf_hl_region_end failed!"</span>
        end <span class="k">if

        </span>call mpi_gather<span class="o">(</span>c,l<span class="k">*</span><span class="o">(</span>iend-ista+1<span class="o">)</span>,mpi_double_precision, &amp;
                        d,l<span class="k">*</span><span class="o">(</span>iend-ista+1<span class="o">)</span>,mpi_double_precision,0,mpi_comm_world,ierr<span class="o">)</span>

        <span class="k">if</span> <span class="o">(</span><span class="nv">myrank</span><span class="o">==</span>0<span class="o">)</span> <span class="k">then
                do </span>i <span class="o">=</span> lbound<span class="o">(</span>d,1<span class="o">)</span>, ubound<span class="o">(</span>d,1<span class="o">)</span>
                    write<span class="o">(</span><span class="k">*</span>,<span class="k">*</span><span class="o">)</span> <span class="o">(</span>d<span class="o">(</span>i,j<span class="o">)</span>, <span class="nv">j</span><span class="o">=</span>lbound<span class="o">(</span>d,2<span class="o">)</span>, ubound<span class="o">(</span>d,2<span class="o">))</span>
                end <span class="k">do
        </span>endif
        call mpi_finalize<span class="o">(</span>ierr<span class="o">)</span>
end program main
</code></pre></div></div>

<p>[para_range.f90]</p>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>subroutine para_range<span class="o">(</span>n1, n2, nprocs, irank, ista, iend<span class="o">)</span>
        implicit none
        integer n1, n2, nprocs, irank, ista, iend, iwork1, iwork2
        iwork1 <span class="o">=</span> <span class="o">(</span>n2 - n1 + 1<span class="o">)</span> / nprocs
        iwork2 <span class="o">=</span> mod<span class="o">(</span>n2 - n1 + 1, nprocs<span class="o">)</span>
        ista <span class="o">=</span> irank <span class="k">*</span> iwork1 + n1 + min<span class="o">(</span>irank, iwork2<span class="o">)</span>
        iend <span class="o">=</span> ista + iwork1 - 1
        <span class="k">if</span> <span class="o">(</span>iwork2 <span class="o">&gt;</span> irank<span class="o">)</span> iend <span class="o">=</span> iend + 1
end
</code></pre></div></div>

<p>以下コマンドを <strong>PAPI</strong> を利用するユーザで実行し、非並列・OpenMP並列兼用版サンプルプログラムを非並列プログラムとしてコンパイルします。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>gfortran <span class="nt">-O3</span> <span class="nt">-I</span>/usr/local/include <span class="nt">-lpapi</span> <span class="nt">-o</span> serial ./matrix_multiply_thrpa_hl.f90
</code></pre></div></div>

<p>次に、以下コマンドを <strong>PAPI</strong> を利用するユーザで実行し、非並列・OpenMP並列兼用版サンプルプログラムをOpenMP並列プログラムとしてコンパイルします。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>gfortran <span class="nt">-O3</span> <span class="nt">-fopenmp</span> <span class="nt">-I</span>/usr/local/include <span class="nt">-lpapi</span> <span class="nt">-o</span> openmp ./matrix_multiply_thrpa_hl.f90
</code></pre></div></div>

<p>次に、以下コマンドを <strong>PAPI</strong> を利用するユーザで実行し、MPI並列版サンプルプログラムをコンパイルします。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>mpifort <span class="nt">-O3</span> <span class="nt">-I</span>/usr/local/include <span class="nt">-lpapi</span> <span class="nt">-o</span> mpi ./matrix_multiply_mpipa_hl.f90 ./para_range.f90
</code></pre></div></div>

<hr />
<h1 id="4-サンプルプログラム実行">4. サンプルプログラム実行</h1>

<p>本章は、先に作成した実行バイナリを実行し、 <strong>PAPI</strong> がプロファイリング情報を出力することを確認します。</p>

<p><strong>PAPI</strong> は、環境変数 <strong>PAPI_EVENTS</strong> に指定されているハードウェアカウンタにアクセスし、環境変数 <strong>PAPI_OUTPUT_DIRECTORY</strong> に指定されているディレクトリ以下の <strong>papi_hl_output</strong> ディレクトリにプロファイリング情報を格納します。</p>

<p>ここでは、浮動小数点演算数（<strong>PAPI_FP_OPS</strong>）と浮動小数点演算インストラクション数（<strong>PAPI_FP_INS</strong>）を取得しています。</p>

<p>以下コマンドを <strong>PAPI</strong> を利用するユーザで実行し、非並列版実行バイナリを実行して <strong>PAPI</strong> がプロファイリング情報を出力することを確認します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">export </span><span class="nv">PAPI_EVENTS</span><span class="o">=</span>PAPI_FP_OPS,PAPI_FP_INS
<span class="nv">$ </span><span class="nb">mkdir</span> ./prof_serial ./prof_openmp ./prof_mpi
<span class="nv">$ </span><span class="nb">export </span><span class="nv">PAPI_OUTPUT_DIRECTORY</span><span class="o">=</span>./prof_serial
<span class="nv">$ </span>./serial <span class="o">&gt;</span> /dev/null
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> ./prof_serial/papi_hl_output/
total 4
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 opc opc 638 Mar 28 10:47 rank_894286.json
<span class="err">$</span>
</code></pre></div></div>

<p>次に、以下コマンドを <strong>PAPI</strong> を利用するユーザで実行し、OpenMP並列版実行バイナリを実行して <strong>PAPI</strong> がプロファイリング情報を出力することを確認します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">export </span><span class="nv">PAPI_OUTPUT_DIRECTORY</span><span class="o">=</span>./prof_openmp
<span class="nv">$ </span><span class="nb">export </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>2
<span class="nv">$ </span>./openmp <span class="o">&gt;</span> /dev/null
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> ./prof_openmp/papi_hl_output/
total 4
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 opc opc 925 Mar 28 12:09 rank_586163.json
<span class="err">$</span>
</code></pre></div></div>

<p>次に、以下コマンドを <strong>PAPI</strong> を利用するユーザで実行し、MPI並列版実行バイナリを実行して <strong>PAPI</strong> がプロファイリング情報を出力することを確認します。<br />
この結果より、 <strong>PAPI</strong> はプロセス毎にプロファイリング情報ファイルを出力することがわかります。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">export </span><span class="nv">PAPI_OUTPUT_DIRECTORY</span><span class="o">=</span>./prof_mpi
<span class="nv">$ </span>mpirun <span class="nt">-n</span> 2 ./mpi <span class="o">&gt;</span> /dev/null
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span> ./prof_mpi/papi_hl_output/
total 8
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 opc opc 638 Mar 28 12:38 rank_000000.json
<span class="nt">-rw-r--r--</span><span class="nb">.</span> 1 opc opc 638 Mar 28 12:38 rank_000001.json
<span class="err">$</span>
</code></pre></div></div>

<hr />
<h1 id="5-プロファイリング結果確認">5. プロファイリング結果確認</h1>

<p>本章は、 <strong>PAPI</strong> が提供する <strong>High Level API</strong> 用プロファイリング情報集計ツールを使用し、先に取得したプロファイリング情報から非並列版・OpenMP並列版・MPI並列版の実行時性能をそれぞれ確認します。</p>

<p>以下コマンドを <strong>PAPI</strong> を利用するユーザで実行し、非並列版の性能を確認します。<br />
この出力より、実時間 <strong>32.9秒</strong> で浮動小数点演算を <strong>54.0 G回</strong> 実行し、 <strong>1.6 GFLOPS</strong> の性能であることがわかります。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>papi_hl_output_writer.py <span class="nt">--notation</span><span class="o">=</span>raw <span class="nt">--type</span><span class="o">=</span>summary <span class="nt">--source_dir</span> ./prof_serial/papi_hl_output
<span class="o">{</span>
    <span class="s2">"computation"</span>: <span class="o">{</span>
        <span class="s2">"region_count"</span>: 1,
        <span class="s2">"cycles"</span>: 98335317464,
        <span class="s2">"real_time_nsec"</span>: 32854673374,
        <span class="s2">"PAPI_FP_OPS"</span>: 54009000020,
        <span class="s2">"PAPI_FP_INS"</span>: 40509000020
    <span class="o">}</span>
<span class="o">}</span>
<span class="nv">$ </span>papi_hl_output_writer.py <span class="nt">--notation</span><span class="o">=</span>derived <span class="nt">--type</span><span class="o">=</span>summary <span class="nt">--source_dir</span> ./prof_serial/papi_hl_output
<span class="o">{</span>
    <span class="s2">"computation"</span>: <span class="o">{</span>
        <span class="s2">"Region count"</span>: 1,
        <span class="s2">"Real time in s"</span>: 32.85,
        <span class="s2">"MFLIPS/s"</span>: 1233.15,
        <span class="s2">"MFLOPS/s"</span>: 1644.11
    <span class="o">}</span>
<span class="o">}</span>
<span class="err">$</span>
</code></pre></div></div>

<p>次に、以下コマンドを <strong>PAPI</strong> を利用するユーザで実行し、OpenMP並列版の性能を確認します。<br />
この出力より、実時間 <strong>17.1秒</strong> で浮動小数点演算を <strong>81.0 G回</strong> 実行し、 <strong>4.8 GFLOPS</strong> の性能であることがわかります。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>papi_hl_output_writer.py <span class="nt">--notation</span><span class="o">=</span>raw <span class="nt">--type</span><span class="o">=</span>summary <span class="nt">--source_dir</span> ./prof_openmp/papi_hl_output
<span class="o">{</span>
    <span class="s2">"computation"</span>: <span class="o">{</span>
        <span class="s2">"region_count"</span>: 2,
        <span class="s2">"cycles"</span>: <span class="o">{</span>
            <span class="s2">"total"</span>: 102064244838,
            <span class="s2">"min"</span>: 51031993766,
            <span class="s2">"median"</span>: 51032122419.0,
            <span class="s2">"max"</span>: 51032251072
        <span class="o">}</span>,
        <span class="s2">"real_time_nsec"</span>: <span class="o">{</span>
            <span class="s2">"total"</span>: 34100545311,
            <span class="s2">"min"</span>: 17050231232,
            <span class="s2">"median"</span>: 17050272655.5,
            <span class="s2">"max"</span>: 17050314079
        <span class="o">}</span>,
        <span class="s2">"PAPI_FP_OPS"</span>: <span class="o">{</span>
            <span class="s2">"total"</span>: 81009000040,
            <span class="s2">"min"</span>: 40504500020,
            <span class="s2">"median"</span>: 40504500020.0,
            <span class="s2">"max"</span>: 40504500020
        <span class="o">}</span>,
        <span class="s2">"PAPI_FP_INS"</span>: <span class="o">{</span>
            <span class="s2">"total"</span>: 81009000040,
            <span class="s2">"min"</span>: 40504500020,
            <span class="s2">"median"</span>: 40504500020.0,
            <span class="s2">"max"</span>: 40504500020
        <span class="o">}</span>,
        <span class="s2">"Number of ranks"</span>: 1,
        <span class="s2">"Number of threads per rank"</span>: 2
    <span class="o">}</span>
<span class="o">}</span>
<span class="nv">$ </span>papi_hl_output_writer.py <span class="nt">--notation</span><span class="o">=</span>raw <span class="nt">--type</span><span class="o">=</span>summary <span class="nt">--source_dir</span> ./prof_openmp/papi_hl_output | jq <span class="nt">-r</span> <span class="s1">'[.computation.PAPI_FP_OPS.total, .computation.real_time_nsec.max] | @csv'</span> | <span class="nb">awk</span> <span class="nt">-F</span>, <span class="s1">'{print $1/$2 " GFLOPS"}'</span>
4.75117 GFLOPS
<span class="err">$</span>
</code></pre></div></div>

<p>次に、以下コマンドを <strong>PAPI</strong> を利用するユーザで実行し、MPI並列版の性能を確認します。<br />
この出力より、実時間 <strong>16.4秒</strong> で浮動小数点演算を <strong>54.0 G回</strong> 実行し、 <strong>3.3 GFLOPS</strong> の性能であることがわかります。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>papi_hl_output_writer.py <span class="nt">--notation</span><span class="o">=</span>raw <span class="nt">--type</span><span class="o">=</span>summary <span class="nt">--source_dir</span> ./prof_mpi/papi_hl_output
<span class="o">{</span>
    <span class="s2">"computation"</span>: <span class="o">{</span>
        <span class="s2">"region_count"</span>: 2,
        <span class="s2">"cycles"</span>: <span class="o">{</span>
            <span class="s2">"total"</span>: 98000743834,
            <span class="s2">"min"</span>: 48974201500,
            <span class="s2">"median"</span>: 49000371917.0,
            <span class="s2">"max"</span>: 49026542334
        <span class="o">}</span>,
        <span class="s2">"real_time_nsec"</span>: <span class="o">{</span>
            <span class="s2">"total"</span>: 32742892813,
            <span class="s2">"min"</span>: 16362702292,
            <span class="s2">"median"</span>: 16371446406.5,
            <span class="s2">"max"</span>: 16380190521
        <span class="o">}</span>,
        <span class="s2">"PAPI_FP_OPS"</span>: <span class="o">{</span>
            <span class="s2">"total"</span>: 54009000040,
            <span class="s2">"min"</span>: 27004500020,
            <span class="s2">"median"</span>: 27004500020.0,
            <span class="s2">"max"</span>: 27004500020
        <span class="o">}</span>,
        <span class="s2">"PAPI_FP_INS"</span>: <span class="o">{</span>
            <span class="s2">"total"</span>: 40509000040,
            <span class="s2">"min"</span>: 20254500020,
            <span class="s2">"median"</span>: 20254500020.0,
            <span class="s2">"max"</span>: 20254500020
        <span class="o">}</span>,
        <span class="s2">"Number of ranks"</span>: 2,
        <span class="s2">"Number of threads per rank"</span>: 1
    <span class="o">}</span>
<span class="o">}</span>
<span class="nv">$ </span>papi_hl_output_writer.py <span class="nt">--notation</span><span class="o">=</span>raw <span class="nt">--type</span><span class="o">=</span>summary <span class="nt">--source_dir</span> ./prof_mpi/papi_hl_output | jq <span class="nt">-r</span> <span class="s1">'[.computation.PAPI_FP_OPS.total, .computation.real_time_nsec.max] | @csv'</span> | <span class="nb">awk</span> <span class="nt">-F</span>, <span class="s1">'{print $1/$2 " GFLOPS"}'</span>
3.29721 GFLOPS
<span class="err">$</span>
</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 更新日時:</strong> <time class="dt-published" datetime="2025-07-26T13:05:45+09:00">July 26, 2025</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">共有</h4>

  <a href="https://x.com/intent/tweet?text=PAPI%E3%81%A7HPC%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%82%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AA%E3%83%B3%E3%82%B0%20https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fbenchmark%2Fpapi-profiling%2F" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fbenchmark%2Fpapi-profiling%2F" class="btn btn--facebook" aria-label="Share on Facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Facebook">
    <i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://oracle-japan.github.io/ocitutorials/hpc/benchmark/papi-profiling/" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

  <a href="https://bsky.app/intent/compose?text=PAPI%E3%81%A7HPC%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%82%92%E3%83%97%E3%83%AD%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AA%E3%83%B3%E3%82%B0%20https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fbenchmark%2Fpapi-profiling%2F" class="btn btn--bluesky" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Bluesky">
    <i class="fab fa-fw fa-bluesky" aria-hidden="true"></i><span> Bluesky</span>
  </a>
</section>


      
  <nav class="pagination">
    
      <a href="/ocitutorials/hpc/benchmark/openmpi-perftune-e6/" class="pagination--pager" title="OpenMPIのMPI集合通信チューニング方法（BM.Standard.E6.256編）">前へ</a>
    
    
      <a href="/ocitutorials/hpc/benchmark/scorep-profiling/" class="pagination--pager" title="Score-P・Scalasca・CubeGUIで並列アプリケーションをプロファイリング">次へ</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">関連記事</h2>
  <div class="grid__wrapper">
    
  </div>
</div>

  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="検索キーワードを入力してください..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>フォロー</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/#ocijp" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/oracle-japan" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/ocitutorials/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2025 <a href="https://oracle-japan.github.io">Oracle Cloud Infrastructure チュートリアル</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/ocitutorials/assets/js/main.min.js"></script>




<script src="/ocitutorials/assets/js/lunr/lunr.min.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-store.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6W7FEC5CEH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6W7FEC5CEH', { 'anonymize_ip': false});
</script>







  
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
  
    <script src="/ocitutorials/assets/js/clipboardrouge.js"></script>
  
    <script src="/ocitutorials/assets/js/tabs.js"></script>
  
    <script src="/ocitutorials/assets/js/sidebar.js"></script>
  


  </body>
</html>
