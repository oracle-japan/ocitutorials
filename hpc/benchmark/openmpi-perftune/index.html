<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.2 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="ja-JP" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>OpenMPIのMPI集合通信チューニング方法（BM.Optimized3.36編） | Oracle Cloud Infrastructure チュートリアル</title>
<meta name="description" content="MPI並列アプリケーションは、MPI通信時間がボトルネックになっている場合そのMPI通信をチューニングすることで性能が向上しますが、ボトルネックのMPI通信が集合通信の場合は、使用する通信アルゴリズムやその切り替えメッセージサイズ等の実行時パラメータ、MPIプロセス分割方法やNUMA nodes per socket等のアプリケーション実行環境まで、様々な要因がその性能に影響します。本パフォーマンス関連Tipsは、MPIの実装にOpenMPIを取り上げ、これが採用するModular Component ArchitectureやUCXの実行時パラメーター、MPIプロセス分割方法やNUMA nodes per socketを組合せて、HPCワークロード向けベア・メタル・シェイプBM.Optimized3.36でMPI集合通信をチューニングする方法を解説します。">


  <meta name="author" content="Oracle Japan Solution Engineers">
  
  <meta property="article:author" content="Oracle Japan Solution Engineers">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ja_JP">
<meta property="og:site_name" content="Oracle Cloud Infrastructure チュートリアル">
<meta property="og:title" content="OpenMPIのMPI集合通信チューニング方法（BM.Optimized3.36編）">
<meta property="og:url" content="https://oracle-japan.github.io/ocitutorials/hpc/benchmark/openmpi-perftune/">


  <meta property="og:description" content="MPI並列アプリケーションは、MPI通信時間がボトルネックになっている場合そのMPI通信をチューニングすることで性能が向上しますが、ボトルネックのMPI通信が集合通信の場合は、使用する通信アルゴリズムやその切り替えメッセージサイズ等の実行時パラメータ、MPIプロセス分割方法やNUMA nodes per socket等のアプリケーション実行環境まで、様々な要因がその性能に影響します。本パフォーマンス関連Tipsは、MPIの実装にOpenMPIを取り上げ、これが採用するModular Component ArchitectureやUCXの実行時パラメーター、MPIプロセス分割方法やNUMA nodes per socketを組合せて、HPCワークロード向けベア・メタル・シェイプBM.Optimized3.36でMPI集合通信をチューニングする方法を解説します。">



  <meta property="og:image" content="https://oracle-japan.github.io/ocitutorials/hpc/benchmark/openmpi-perftune/are_08_36_step3.png">





  <meta property="article:published_time" content="2025-07-28T16:55:26+09:00">






<link rel="canonical" href="https://oracle-japan.github.io/ocitutorials/hpc/benchmark/openmpi-perftune/">












<!-- end _includes/seo.html -->



  <link href="/ocitutorials/feed.xml" type="application/atom+xml" rel="alternate" title="Oracle Cloud Infrastructure チュートリアル Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/ocitutorials/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-32.png" sizes="32x32">
<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-128.png" sizes="128x128">
<link rel="icon" href="https://www.oracle.com/asset/web/favicons/favicon-192.png" sizes="192x192">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-120.png" sizes="120x120">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-152.png" sizes="152x152">
<link rel="apple-touch-icon" href="https://www.oracle.com/asset/web/favicons/favicon-180.png" sizes="180x180">
<link rel="shortcut icon" type="image/x-icon" href="/ocitutorials/assets/favicon/favicon.ico">
<link rel="manifest" href="/ocitutorials/assets/favicon/site.webmanifest">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/ocitutorials/"><img src="/ocitutorials/assets/images/social-og-oracle-badge.jpg" alt="OCI チュートリアル"></a>
        
        <a class="site-title" href="/ocitutorials/">
          OCI チュートリアル
          <span class="site-subtitle">Oracle Cloud Infrastructure を使ってみよう</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/ocitutorials/#%E3%83%81%E3%83%A5%E3%83%BC%E3%83%88%E3%83%AA%E3%82%A2%E3%83%AB%E3%82%B3%E3%83%B3%E3%83%86%E3%83%B3%E3%83%84%E4%B8%80%E8%A6%A7"
                
                
              >チュートリアル一覧</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ocitutorials/about/"
                
                
              >このサイトについて</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">メニュー</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style=" background-image: linear-gradient(rgba(34, 66, 55, 0.7), rgba(34, 66, 55, 0.7)), url('/ocitutorials/hpc/benchmark/openmpi-perftune/are_08_36_step3.png');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          OpenMPIのMPI集合通信チューニング方法（BM.Optimized3.36編）

        
      </h1>
      
        <p class="page__lead">MPI並列アプリケーションは、MPI通信時間がボトルネックになっている場合そのMPI通信をチューニングすることで性能が向上しますが、ボトルネックのMPI通信が集合通信の場合は、使用する通信アルゴリズムやその切り替えメッセージサイズ等の実行時パラメータ、MPIプロセス分割方法やNUMA nodes per socket等のアプリケーション実行環境まで、様々な要因がその性能に影響します。本パフォーマンス関連Tipsは、MPIの実装にOpenMPIを取り上げ、これが採用するModular Component ArchitectureやUCXの実行時パラメーター、MPIプロセス分割方法やNUMA nodes per socketを組合せて、HPCワークロード向けベア・メタル・シェイプBM.Optimized3.36でMPI集合通信をチューニングする方法を解説します。
</p>
      
      


      
    </div>
  
  
</div>






  
    



<nav class="breadcrumbs">
  <ol itemscope itemtype="https://schema.org/BreadcrumbList">
    
    
    
      
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/" itemprop="item"><span itemprop="name">ホーム</span></a>

          <meta itemprop="position" content="1" />
        </li>
        <span class="sep">></span>
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/hpc" itemprop="item"><span itemprop="name">Hpc</span></a>
          <meta itemprop="position" content="2" />
        </li>
        <span class="sep">></span>
      
    
      
      
        
        <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
          <a href="/ocitutorials/benchmark" itemprop="item"><span itemprop="name">Benchmark</span></a>
          <meta itemprop="position" content="3" />
        </li>
        <span class="sep">></span>
      
    
      
      
        <li class="current">OpenMPIのMPI集合通信チューニング方法（BM.Optimized3.36編）</li>
      
    
  </ol>
</nav>

  


<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">メニュー</label>
  <ul class="nav__items">
    <li>
      
      <a href=""><span class="nav__sub-title">HPC編</span></a>
      <ul>
        
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-cluster-network/">HPCクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster-withterraform/">HPCクラスタを構築する(基礎インフラ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster/">HPCクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-hpc-cluster-withautoscaling/">HPCクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-ml-instance/">GPUインスタンスで機械学習にトライ</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-ml-instance-cntnd/">GPUインスタンスで分散機械学習環境を構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster/">GPUクラスタを構築する(基礎インフラ手動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withterraform/">GPUクラスタを構築する(基礎インフラ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withstack/">GPUクラスタを構築する(スタティッククラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withautoscaling/">GPUクラスタを構築する(オンデマンドクラスタ自動構築編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-gpu-cluster-withubuntu/">GPUクラスタを構築する(Ubuntu OS編)</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server-fss/">ファイル・ストレージでファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-lustre-server-fswl/">File Storage with Lustreでファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server/">ブロック・ボリュームでファイル共有ストレージを構築する（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server-e6/">ブロック・ボリュームでファイル共有ストレージを構築する（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-nfs-server-nvme/">短期保存データ用高速ファイル共有ストレージを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/spinup-backup-server/">ベア・メタル・インスタンスNFSサーバ向けバックアップサーバを構築する</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-base/">ブロック・ボリュームNFSサーバと基礎インフラ編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/cluster-with-bv-stack/">ブロック・ボリュームNFSサーバと自動構築編HPC/GPUクラスタを組み合わせる</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl/">HPL実行方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl-e5/">HPL実行方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-hpl-e6/">HPL実行方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream/">STREAM実行方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream-e5/">STREAM実行方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-stream-e6/">STREAM実行方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-imb/">Intel MPI Benchmarks実行方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-nccltests/">NCCL Tests実行方法（BM.GPU4.8/BM.GPU.A100-v2.8編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/run-nccltests-h100/">NCCL Tests実行方法（BM.GPU.H100.8編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/bios-setting/">パフォーマンスに関連するベアメタルインスタンスのBIOS設定方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/stop-unused-service/">不要サービス停止によるパフォーマンスチューニング方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/topology-aware-cn-tuning/">クラスタ・ネットワークのトポロジーを考慮したノード間通信最適化方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openfoam-tuning/">CFD解析フローのコストパフォーマンを向上させるOpenFOAM関連Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/">OpenMPIのMPI通信性能に影響するパラメータとその関連Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/cpu-binding/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/cpu-binding-e5/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/cpu-binding-e6/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftune/" class="active">OpenMPIのMPI集合通信チューニング方法（BM.Optimized3.36編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftune-e5/">OpenMPIのMPI集合通信チューニング方法（BM.Standard.E5.192編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/openmpi-perftune-e6/">OpenMPIのMPI集合通信チューニング方法（BM.Standard.E6.256編）</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/papi-profiling/">PAPIでHPCアプリケーションをプロファイリング</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/scorep-profiling/">Score-P・Scalasca・CubeGUIで並列アプリケーションをプロファイリング</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/benchmark/profiling-tuning/">プロファイリング情報に基づく並列アプリケーションチューニング方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-connect-clusternetwork/">クラスタネットワーキングイメージを使ったクラスタ・ネットワーク接続方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/rdma-interface-configure/">クラスタ・ネットワーク接続用ネットワークインターフェース作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/">クラスタネットワーキングイメージの選び方</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-create-cnenabled-osimage/">クラスタ・ネットワーク未対応OSを使ったクラスタ・ネットワーク接続方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/determine-cnrelated-issue/">クラスタ・ネットワークに接続する計算/GPUノード作成時の問題判別方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-get-cnrelated-statistics/">クラスタ・ネットワーク統計情報の取得方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/nvme-filesystem/">ベアメタルインスタンスのNVMe SSDローカルディスク領域ファイルシステム作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-configure-sharedstorage/">HPC/GPUクラスタ向けファイル共有ストレージの最適な構築手法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/bv-sharedstorage-recovery/">ブロック・ボリュームを使用するNFSサーバのインスタンス障害からの復旧方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/boot-volume-extension/">計算/GPUノードのブート・ボリューム動的拡張方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/howto-choose-osbackuptool/">ファイル共有ストレージ向けバックアップ環境の最適な構築手法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-name-resolution/">計算/GPUノードの効果的な名前解決方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-os-customization/">計算/GPUノードデプロイ時の効果的なOSカスタマイズ方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/compute-host-list/">計算/GPUノードのホスト名リスト作成方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/cluster-resize/">計算/GPUノードの追加・削除・入れ替え方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/cluster-with-pdsh/">pdshで効率的にクラスタ管理オペレーションを実行</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/instance-principal-auth/">オンデマンドクラスタ実現のためのインスタンス・プリンシパル認証設定方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/log-monitoring/">OCIロギングとGrafanaを使用したHPC/GPUクラスタのログ監視方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/metric-monitoring/">OCIモニタリングとGrafanaを使用したHPC/GPUクラスタのメトリック監視方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/gpu-with-ubuntu/">UbuntuをOSとする機械学習ワークロード向けGPUノード構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするUCX通信フレームワークベースのOpenMPI構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/setup-slurm-cluster/">Slurmによるリソース管理・ジョブ管理システム構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/install-blas/">線形代数演算ライブラリインストール・利用方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/install-openfoam/">OpenFOAMインストール・利用方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/slurm-tips/">Slurmによるリソース管理・ジョブ管理システム運用Tips</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/build-oraclelinux-hpcenv/">Oracle Linuxプラットフォーム・イメージベースのHPCワークロード実行環境構築方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/kdump-on-baremetal/">ベアメタルインスタンスのカーネルダンプ取得方法</a></li></p>
        
          <p></p><li><a href="/ocitutorials/hpc/tech-knowhow/site-to-site-vpn/">サイト間VPNによるOCIとの拠点間接続方法</a></li></p>
        
      </ul>
    </li>
  </ul>
</nav>
    
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="OpenMPIのMPI集合通信チューニング方法（BM.Optimized3.36編）">
    <meta itemprop="description" content="MPI並列アプリケーションは、MPI通信時間がボトルネックになっている場合そのMPI通信をチューニングすることで性能が向上しますが、ボトルネックのMPI通信が集合通信の場合は、使用する通信アルゴリズムやその切り替えメッセージサイズ等の実行時パラメータ、MPIプロセス分割方法やNUMA nodes per socket等のアプリケーション実行環境まで、様々な要因がその性能に影響します。本パフォーマンス関連Tipsは、MPIの実装にOpenMPIを取り上げ、これが採用するModular Component ArchitectureやUCXの実行時パラメーター、MPIプロセス分割方法やNUMA nodes per socketを組合せて、HPCワークロード向けベア・メタル・シェイプBM.Optimized3.36でMPI集合通信をチューニングする方法を解説します。">
    <meta itemprop="datePublished" content="2025-07-28T16:55:26+09:00">
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> 目次</h4></header>
              <ul class="toc__menu"><li><a href="#0-概要">0. 概要</a></li><li><a href="#1--1ノード">1.  1ノード</a><ul><li><a href="#1-0-概要">1-0. 概要</a></li><li><a href="#1-1-8-mpiプロセス">1-1. 8 MPIプロセス</a><ul><li><a href="#1-1-0-概要">1-1-0. 概要</a></li><li><a href="#1-1-1-alltoall">1-1-1. Alltoall</a></li><li><a href="#1-1-2-allgather">1-1-2. Allgather</a></li><li><a href="#1-1-3-allreduce">1-1-3. Allreduce</a></li></ul></li><li><a href="#1-2--16-mpiプロセス">1-2.  16 MPIプロセス</a><ul><li><a href="#1-2-0-概要">1-2-0. 概要</a></li><li><a href="#1-2-1-alltoall">1-2-1. Alltoall</a></li><li><a href="#1-2-2-allgather">1-2-2. Allgather</a></li><li><a href="#1-2-3-allreduce">1-2-3. Allreduce</a></li></ul></li><li><a href="#1-3--32-mpiプロセス">1-3.  32 MPIプロセス</a><ul><li><a href="#1-3-0-概要">1-3-0. 概要</a></li><li><a href="#1-3-1-alltoall">1-3-1. Alltoall</a></li><li><a href="#1-3-2-allgather">1-3-2. Allgather</a></li><li><a href="#1-3-3-allreduce">1-3-3. Allreduce</a></li></ul></li><li><a href="#1-4--36-mpiプロセス">1-4.  36 MPIプロセス</a><ul><li><a href="#1-4-0-概要">1-4-0. 概要</a></li><li><a href="#1-4-1-alltoall">1-4-1. Alltoall</a></li><li><a href="#1-4-2-allgather">1-4-2. Allgather</a></li><li><a href="#1-4-3-allreduce">1-4-3. Allreduce</a></li></ul></li></ul></li><li><a href="#2--2ノード">2.  2ノード</a><ul><li><a href="#2-0-概要">2-0. 概要</a></li><li><a href="#2-1-ノード当たり8-mpiプロセス">2-1. ノード当たり8 MPIプロセス</a><ul><li><a href="#2-1-0-概要">2-1-0. 概要</a></li><li><a href="#2-1-1-alltoall">2-1-1. Alltoall</a></li><li><a href="#2-1-2-allgather">2-1-2. Allgather</a></li><li><a href="#2-1-3-allreduce">2-1-3. Allreduce</a></li></ul></li><li><a href="#2-2-ノード当たり16-mpiプロセス">2-2. ノード当たり16 MPIプロセス</a><ul><li><a href="#2-2-0-概要">2-2-0. 概要</a></li><li><a href="#2-2-1-alltoall">2-2-1. Alltoall</a></li><li><a href="#2-2-2-allgather">2-2-2. Allgather</a></li><li><a href="#2-2-3-allreduce">2-2-3. Allreduce</a></li></ul></li><li><a href="#2-3-ノード当たり32-mpiプロセス">2-3. ノード当たり32 MPIプロセス</a><ul><li><a href="#2-3-0-概要">2-3-0. 概要</a></li><li><a href="#2-3-1-alltoall">2-3-1. Alltoall</a></li><li><a href="#2-3-2-allgather">2-3-2. Allgather</a></li><li><a href="#2-3-3-allreduce">2-3-3. Allreduce</a></li></ul></li><li><a href="#2-4-ノード当たり36-mpiプロセス">2-4. ノード当たり36 MPIプロセス</a><ul><li><a href="#2-4-0-概要">2-4-0. 概要</a></li><li><a href="#2-4-1-alltoall">2-4-1. Alltoall</a></li><li><a href="#2-4-2-allgather">2-4-2. Allgather</a></li><li><a href="#2-4-3-allreduce">2-4-3. Allreduce</a></li></ul></li></ul></li><li><a href="#3--4ノード">3.  4ノード</a><ul><li><a href="#3-0-概要">3-0. 概要</a></li><li><a href="#3-1-ノード当たり8-mpiプロセス">3-1. ノード当たり8 MPIプロセス</a><ul><li><a href="#3-1-0-概要">3-1-0. 概要</a></li><li><a href="#3-1-1-alltoall">3-1-1. Alltoall</a></li><li><a href="#3-1-2-allgather">3-1-2. Allgather</a></li><li><a href="#3-1-3-allreduce">3-1-3. Allreduce</a></li></ul></li><li><a href="#3-2-ノード当たり16-mpiプロセス">3-2. ノード当たり16 MPIプロセス</a><ul><li><a href="#3-2-0-概要">3-2-0. 概要</a></li><li><a href="#3-2-1-alltoall">3-2-1. Alltoall</a></li><li><a href="#3-2-2-allgather">3-2-2. Allgather</a></li><li><a href="#3-2-3-allreduce">3-2-3. Allreduce</a></li></ul></li><li><a href="#3-3-ノード当たり32-mpiプロセス">3-3. ノード当たり32 MPIプロセス</a><ul><li><a href="#3-3-0-概要">3-3-0. 概要</a></li><li><a href="#3-3-1-alltoall">3-3-1. Alltoall</a></li><li><a href="#3-3-2-allgather">3-3-2. Allgather</a></li><li><a href="#3-3-3-allreduce">3-3-3. Allreduce</a></li></ul></li><li><a href="#3-4-ノード当たり36-mpiプロセス">3-4. ノード当たり36 MPIプロセス</a><ul><li><a href="#3-4-0-概要">3-4-0. 概要</a></li><li><a href="#3-4-1-alltoall">3-4-1. Alltoall</a></li><li><a href="#3-4-2-allgather">3-4-2. Allgather</a></li><li><a href="#3-4-3-allreduce">3-4-3. Allreduce</a></li></ul></li></ul></li><li><a href="#4--8ノード">4.  8ノード</a><ul><li><a href="#4-0-概要">4-0. 概要</a></li><li><a href="#4-1-ノード当たり8-mpiプロセス">4-1. ノード当たり8 MPIプロセス</a><ul><li><a href="#4-1-0-概要">4-1-0. 概要</a></li><li><a href="#4-1-1-alltoall">4-1-1. Alltoall</a></li><li><a href="#4-1-2-allgather">4-1-2. Allgather</a></li><li><a href="#4-1-3-allreduce">4-1-3. Allreduce</a></li></ul></li><li><a href="#4-2-ノード当たり16-mpiプロセス">4-2. ノード当たり16 MPIプロセス</a><ul><li><a href="#4-2-0-概要">4-2-0. 概要</a></li><li><a href="#4-2-1-alltoall">4-2-1. Alltoall</a></li><li><a href="#4-2-2-allgather">4-2-2. Allgather</a></li><li><a href="#4-2-3-allreduce">4-2-3. Allreduce</a></li></ul></li><li><a href="#4-3-ノード当たり32-mpiプロセス">4-3. ノード当たり32 MPIプロセス</a><ul><li><a href="#4-3-0-概要">4-3-0. 概要</a></li><li><a href="#4-3-1-alltoall">4-3-1. Alltoall</a></li><li><a href="#4-3-2-allgather">4-3-2. Allgather</a></li><li><a href="#4-3-3-allreduce">4-3-3. Allreduce</a></li></ul></li><li><a href="#4-4-ノード当たり36-mpiプロセス">4-4. ノード当たり36 MPIプロセス</a><ul><li><a href="#4-4-0-概要">4-4-0. 概要</a></li><li><a href="#4-4-1-alltoall">4-4-1. Alltoall</a></li><li><a href="#4-4-2-allgather">4-4-2. Allgather</a></li><li><a href="#4-4-3-allreduce">4-4-3. Allreduce</a></li></ul></li></ul></li></ul>
            </nav>
          </aside>
        
        <hr />
<h1 id="0-概要">0. 概要</h1>

<p>オープンソースのMPI実装である <strong><a href="https://www.open-mpi.org/">OpenMPI</a></strong> は、  <strong><a href="https://docs.open-mpi.org/en/v5.0.x/mca.html">Modular Component Architecture</a></strong> （以降 <strong>MCA</strong> と呼称します。）を採用することで、ビルド時に組み込むコンポーネントを介して集合通信を含む多彩な機能を提供し、この <strong>MCA</strong> パラメータにはMPI集合通信性能に影響するものがあります。<br />
特にMPI集合通信の高速化を意識して開発されている <strong><a href="https://docs.nvidia.com/networking/display/hpcxv223/hcoll">HCOLL</a></strong> や <strong><a href="https://github.com/openucx/ucc">Unified Collective Communication</a></strong> （以降 <strong>UCC</strong> と呼称します。）は、その特性を理解して適切に利用することで、MPI集合通信性能を大幅に向上させることが可能です。</p>

<p>また <strong>OpenMPI</strong> は、高帯域・低遅延のMPIプロセス間通信を実現するためにその通信フレームワークに <strong><a href="https://openucx.org/">UCX</a></strong> を採用し、この <strong>UCX</strong> のパラメータにもMPI集合通信性能に影響するパラメータが存在します。</p>

<p>またMPI集合通信は、ノード内並列では実質的にメモリコピーとなるため、メモリ性能に影響するMPIプロセスのコア割当てや <strong>NUMA nodes per socket</strong> （以降 <strong>NPS</strong> と呼称します。）もその性能に影響します。</p>

<p>以上を踏まえて本パフォーマンス関連Tipsは、HPCワークロード向けベア・メタル・シェイプ <strong><a href="https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized">BM.Optimized3.36</a></strong> に於ける <strong>OpenMPI</strong> のMPI集合通信性能にフォーカスし、以下の <strong>計測条件</strong> を組合せたテストケース毎に以下の <strong>実行時パラメータ</strong> を変えてその性能を <strong><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-mpi-benchmarks.html">Intel MPI Benchmarks</a></strong> で計測し、最適な <strong>実行時パラメータ</strong> の組み合わせを導きます。</p>

<p>[<strong>計測条件</strong>]</p>

<ul>
  <li>ノード数 ： <strong>1</strong> ・ <strong>2</strong> ・ <strong>4</strong> ・ <strong>8</strong></li>
  <li>ノード当たりMPIプロセス数 ： <strong>8</strong> ・ <strong>16</strong> ・ <strong>32</strong> ・ <strong>36</strong></li>
  <li>MPI集合通信関数 ： <strong>Alltoall</strong> ・ <strong>Allgather</strong> ・ <strong>Allreduce</strong></li>
</ul>

<p>[<strong>実行時パラメータ</strong>]</p>

<ul>
  <li><strong>UCX_TLS</strong> ： <strong>all</strong> ・ <strong>self,sm,rc</strong> ・ <strong>self,sm,ud</strong> ・ <strong>self,sm,dc</strong> （※1）</li>
  <li><strong>UCX_RNDV_THRESH</strong> ： <strong>auto</strong> ・ <strong>4kb</strong> ・ <strong>8kb</strong> ・ <strong>16kb</strong> ・ <strong>32kb</strong> ・ <strong>64kb</strong> ・ <strong>128kb</strong> （※2）</li>
  <li><strong>UCX_ZCOPY_THRESH</strong> ： <strong>auto</strong> ・ <strong>4kb</strong> ・ <strong>8kb</strong> ・ <strong>16kb</strong> ・ <strong>32kb</strong> ・ <strong>64kb</strong> ・ <strong>128kb</strong> （※3）</li>
  <li><strong>coll_hcoll_enable</strong> ： <strong>0</strong> ・ <strong>1</strong> （※4）</li>
  <li><strong>coll_ucc_enable</strong> ： <strong>0</strong> ・ <strong>1</strong> （※5）</li>
  <li>MPIプロセス分割方法 ： ブロック分割・サイクリック分割（※6）</li>
  <li><strong>NPS</strong> ：  <strong>1</strong> （以降 <strong>NPS1</strong> と呼称します。）・ <strong>2</strong> （以降 <strong>NPS2</strong> と呼称します。）（※7）</li>
</ul>

<p>※1） <strong>UCX</strong> のパラメータで、2ノード以上の <strong>計測条件</strong> で使用します。詳細は <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/">OpenMPIのMPI通信性能に影響するパラメータとその関連Tips</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/#3-4-ucx_tls">3-4. UCX_TLS</a></strong> を参照してください。<br />
※2） <strong>UCX</strong> のパラメータで、詳細は <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/">OpenMPIのMPI通信性能に影響するパラメータとその関連Tips</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/#3-6-ucx_rndv_thresh">3-6. UCX_RNDV_THRESH</a></strong> を参照してください。<br />
※3） <strong>UCX</strong> のパラメータで、2ノード以上の <strong>計測条件</strong> で使用します。詳細は <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/">OpenMPIのMPI通信性能に影響するパラメータとその関連Tips</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/#3-7-ucx_zcopy_thresh">3-7. UCX_ZCOPY_THRESH</a></strong> を参照してください。<br />
※4） <strong>MCA</strong> のパラメータで、詳細は <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/">OpenMPIのMPI通信性能に影響するパラメータとその関連Tips</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/#3-1-coll_hcoll_enable">3-1. coll_hcoll_enable</a></strong> を参照してください。<br />
※5） <strong>MCA</strong> のパラメータで、詳細は <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/">OpenMPIのMPI通信性能に影響するパラメータとその関連Tips</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/openmpi-perftips/#3-9-coll_ucc_enable">3-9. coll_ucc_enable</a></strong> を参照してください。<br />
※6）NUMAノードに対するMPIプロセスの分割方法で、詳細は <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/cpu-binding/">パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Optimized3.36編）</a></strong> を参照してください。<br />
※7）<strong>NPS</strong> の設定方法は、 <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/bios-setting/">パフォーマンスに関連するベアメタルインスタンスのBIOS設定方法</a></strong> を参照してください。</p>

<p>また本パフォーマンス関連Tipsの検証は、以下の実行環境で実施しています。</p>

<p>[実行環境]</p>
<ul>
  <li>シェイプ： <strong>BM.Optimized3.36</strong> （  <strong>Simultanious Multi Threading</strong> （以降 <strong>SMT</strong> と呼称します。）無効（※8））</li>
  <li>ノード間接続 ： <strong><a href="/ocitutorials/hpc/#5-1-クラスタネットワーク">クラスタ・ネットワーク</a></strong> （※9）</li>
  <li>OS： <strong>Oracle Linux</strong> 8.10ベースのHPC <strong><a href="/ocitutorials/hpc/#5-13-クラスタネットワーキングイメージ">クラスタネットワーキングイメージ</a></strong> （※10）</li>
  <li><strong>OpenMPI</strong> ： 5.0.6（※11）</li>
  <li><strong>Intel MPI Benchmarks</strong> ： 2021.7（※12）</li>
</ul>

<p>※8）<strong>SMT</strong> の設定方法は、 <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/bios-setting/">パフォーマンスに関連するベアメタルインスタンスのBIOS設定方法</a></strong> を参照してください。<br />
※9）本テクニカルTipsの2ノード以上の計測は、 <strong>クラスタ・ネットワーク</strong> の同一リーフスイッチに接続するインスタンスを使用して行っています。同一リーフスイッチに接続するインスタンス間のノード間接続に於ける効果は、 <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/topology-aware-cn-tuning/">クラスタ・ネットワークのトポロジーを考慮したノード間通信最適化方法</a></strong> を参照してください。<br />
※10）<strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/">クラスタネットワーキングイメージの選び方</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/osimage-for-cluster/#1-クラスタネットワーキングイメージ一覧">1. クラスタネットワーキングイメージ一覧</a></strong> のイメージ <strong>No.12</strong> です。<br />
※11） <strong><a href="/ocitutorials/hpc/#3-oci-hpcテクニカルtips集">OCI HPCテクニカルTips集</a></strong> の <strong><a href="/ocitutorials/hpc/tech-knowhow/build-openmpi/">Slurm環境での利用を前提とするUCX通信フレームワークベースのOpenMPI構築方法</a></strong> に従って構築された <strong>OpenMPI</strong> です。<br />
※12） <strong><a href="/ocitutorials/hpc/#2-oci-hpcパフォーマンス関連情報">OCI HPCパフォーマンス関連情報</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/run-imb/">Intel MPI Benchmarks実行方法</a></strong> の <strong><a href="/ocitutorials/hpc/benchmark/run-imb/#1-openmpiでintel-mpi-benchmarksを実行する場合">1. OpenMPIでIntel MPI Benchmarksを実行する場合</a></strong> に従って構築された <strong>Intel MPI Benchmarks</strong> です。</p>

<p>また <strong>Intel MPI Benchmarks</strong> の計測は、 <strong>numactl</strong> コマンドを介して以下の実行時オプションを指定して起動します。</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>numactl <span class="nt">-l</span> IMB-MPI1 <span class="nt">-msglog</span> 0:xx <span class="nt">-mem</span> 2.3G <span class="nt">-off_cache</span> 39,64 <span class="nt">-npmin</span> num_of_procs alltoall/allgather/allreduce
</code></pre></div></div>

<p>ここで計測するメッセージサイズの上限（ <strong>xx</strong> ）は、MPI集合通信関数とノード数に応じて以下の値を使用します。<br />
この設定値は、計測可能な最大値から決定しています。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">1ノード</th>
      <th style="text-align: center">2ノード</th>
      <th style="text-align: center">4ノード</th>
      <th>8ノード</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">25</td>
      <td style="text-align: center">24</td>
      <td style="text-align: center">23</td>
      <td>22</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">25</td>
      <td style="text-align: center">24</td>
      <td style="text-align: center">23</td>
      <td>22</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">30</td>
      <td style="text-align: center">30</td>
      <td style="text-align: center">30</td>
      <td>30</td>
    </tr>
  </tbody>
</table>

<p>また <strong>Intel MPI Benchmarks</strong> の計測は、テストケース毎に5回実施し、その最大値と最小値を除く3回の算術平均をその結果とします。</p>

<p>以降では、以下 <strong>計測条件</strong> の順に解説します。</p>

<ol>
  <li><strong><a href="#1--1ノード">1ノード</a></strong></li>
  <li><strong><a href="#2--2ノード">2ノード</a></strong></li>
  <li><strong><a href="#3--4ノード">4ノード</a></strong></li>
  <li><strong><a href="#4--8ノード">8ノード</a></strong></li>
</ol>

<hr />
<h1 id="1--1ノード">1.  1ノード</h1>

<h2 id="1-0-概要">1-0. 概要</h2>

<p>本章は、1ノードに8・16・32・36の各MPIプロセスを割当てる場合の各MPI集合通信関数の通信性能について、以下の <strong>実行時パラメータ</strong> の最適な組み合わせを検証します。</p>

<ul>
  <li><strong>UCX_RNDV_THRESH</strong> ： <strong>auto</strong> ・ <strong>16kb</strong> ・ <strong>32kb</strong> ・ <strong>64kb</strong> ・ <strong>128kb</strong></li>
  <li><strong>coll_hcoll_enable</strong> ： <strong>0</strong> ・ <strong>1</strong></li>
  <li><strong>coll_ucc_enable</strong> ： <strong>0</strong> ・ <strong>1</strong></li>
  <li>MPIプロセス分割方法 ： ブロック分割・サイクリック分割</li>
  <li><strong>NPS</strong> ：  <strong>1</strong> ・ <strong>2</strong></li>
</ul>

<h2 id="1-1-8-mpiプロセス">1-1. 8 MPIプロセス</h2>

<h3 id="1-1-0-概要">1-1-0. 概要</h3>

<p>本章は、1ノードに8 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<h3 id="1-1-1-alltoall">1-1-1. Alltoall</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="ata_01_08_n1_bl_no.png" alt="Alltoall 1 node 8 processes NPS1 no coll" /></p>

<p><img src="ata_01_08_n1_bl_uc.png" alt="Alltoall 1 node 8 processes NPS1 UCC" /></p>

<p><img src="ata_01_08_n1_bl_hc.png" alt="Alltoall 1 node 8 processes NPS1 HCOLL" /></p>

<p><img src="ata_01_08_n1_cy_no.png" alt="Alltoall 1 node 8 processes NPS1 no coll" /></p>

<p><img src="ata_01_08_n1_cy_uc.png" alt="Alltoall 1 node 8 processes NPS1 UCC" /></p>

<p><img src="ata_01_08_n1_cy_hc.png" alt="Alltoall 1 node 8 processes NPS1 HCOLL" /></p>

<p><img src="ata_01_08_n2_bl_no.png" alt="Alltoall 1 node 8 processes NPS2 no coll" /></p>

<p><img src="ata_01_08_n2_bl_uc.png" alt="Alltoall 1 node 8 processes NPS2 UCC" /></p>

<p><img src="ata_01_08_n2_bl_hc.png" alt="Alltoall 1 node 8 processes NPS2 HCOLL" /></p>

<p><img src="ata_01_08_n2_cy_no.png" alt="Alltoall 1 node 8 processes NPS2 no coll" /></p>

<p><img src="ata_01_08_n2_cy_uc.png" alt="Alltoall 1 node 8 processes NPS2 UCC" /></p>

<p><img src="ata_01_08_n2_cy_hc.png" alt="Alltoall 1 node 8 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">32KB</td>
      <td style="text-align: right">32KB</td>
      <td style="text-align: right">32KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="ata_01_08_step1_no.png" alt="Alltoall 1 node 8 processes no coll" /></p>

<p><img src="ata_01_08_step1_uc.png" alt="Alltoall 1 node 8 processes UCC" /></p>

<p><img src="ata_01_08_step1_hc.png" alt="Alltoall 1 node 8 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_01_08_step2.png" alt="Alltoall 1 node 8 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は32KB以上で性能が低下しそれ未満で顕著な傾向無し</li>
  <li><strong>HCOLL</strong> は顕著な傾向無し</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して32KB以上で性能が向上しそれ未満で顕著な傾向無し</li>
  <li>チューニング未適用は8KBから128KBの間で大幅に性能が低下</li>
</ul>

<h3 id="1-1-2-allgather">1-1-2. Allgather</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="aga_01_08_n1_bl_no.png" alt="Allgather 1 node 8 processes NPS1 no coll" /></p>

<p><img src="aga_01_08_n1_bl_uc.png" alt="Allgather 1 node 8 processes NPS1 UCC" /></p>

<p><img src="aga_01_08_n1_bl_hc.png" alt="Allgather 1 node 8 processes NPS1 HCOLL" /></p>

<p><img src="aga_01_08_n1_cy_no.png" alt="Allgather 1 node 8 processes NPS1 no coll" /></p>

<p><img src="aga_01_08_n1_cy_uc.png" alt="Allgather 1 node 8 processes NPS1 UCC" /></p>

<p><img src="aga_01_08_n1_cy_hc.png" alt="Allgather 1 node 8 processes NPS1 HCOLL" /></p>

<p><img src="aga_01_08_n2_bl_no.png" alt="Allgather 1 node 8 processes NPS2 no coll" /></p>

<p><img src="aga_01_08_n2_bl_uc.png" alt="Allgather 1 node 8 processes NPS2 UCC" /></p>

<p><img src="aga_01_08_n2_bl_hc.png" alt="Allgather 1 node 8 processes NPS2 HCOLL" /></p>

<p><img src="aga_01_08_n2_cy_no.png" alt="Allgather 1 node 8 processes NPS2 no coll" /></p>

<p><img src="aga_01_08_n2_cy_uc.png" alt="Allgather 1 node 8 processes NPS2 UCC" /></p>

<p><img src="aga_01_08_n2_cy_hc.png" alt="Allgather 1 node 8 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">64KB</td>
      <td style="text-align: right">32KB</td>
      <td style="text-align: right">64KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="aga_01_08_step1_no.png" alt="Allgather 1 node 8 processes no coll" /></p>

<p><img src="aga_01_08_step1_uc.png" alt="Allgather 1 node 8 processes UCC" /></p>

<p><img src="aga_01_08_step1_hc.png" alt="Allgather 1 node 8 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_01_08_step2.png" alt="Allgather 1 node 8 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> はほぼ全域で性能が低下</li>
  <li><strong>HCOLL</strong> はほぼ全域で性能が低下</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して4KBから32KBの間で性能が向上しそれ未満で性能が低下</li>
  <li>チューニング適用による性能の変化無し</li>
</ul>

<h3 id="1-1-3-allreduce">1-1-3. Allreduce</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="are_01_08_n1_bl_no.png" alt="Allreduce 1 node 8 processes NPS1 no coll" /></p>

<p><img src="are_01_08_n1_bl_uc.png" alt="Allreduce 1 node 8 processes NPS1 UCC" /></p>

<p><img src="are_01_08_n1_bl_hc.png" alt="Allreduce 1 node 8 processes NPS1 HCOLL" /></p>

<p><img src="are_01_08_n1_cy_no.png" alt="Allreduce 1 node 8 processes NPS1 no coll" /></p>

<p><img src="are_01_08_n1_cy_uc.png" alt="Allreduce 1 node 8 processes NPS1 UCC" /></p>

<p><img src="are_01_08_n1_cy_hc.png" alt="Allreduce 1 node 8 processes NPS1 HCOLL" /></p>

<p><img src="are_01_08_n2_bl_no.png" alt="Allreduce 1 node 8 processes NPS2 no coll" /></p>

<p><img src="are_01_08_n2_bl_uc.png" alt="Allreduce 1 node 8 processes NPS2 UCC" /></p>

<p><img src="are_01_08_n2_bl_hc.png" alt="Allreduce 1 node 8 processes NPS2 HCOLL" /></p>

<p><img src="are_01_08_n2_cy_no.png" alt="Allreduce 1 node 8 processes NPS2 no coll" /></p>

<p><img src="are_01_08_n2_cy_uc.png" alt="Allreduce 1 node 8 processes NPS2 UCC" /></p>

<p><img src="are_01_08_n2_cy_hc.png" alt="Allreduce 1 node 8 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">64KB</td>
      <td style="text-align: right">128KB</td>
      <td style="text-align: right">128KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="are_01_08_step1_no.png" alt="Allreduce 1 node 8 processes no coll" /></p>

<p><img src="are_01_08_step1_uc.png" alt="Allreduce 1 node 8 processes UCC" /></p>

<p><img src="are_01_08_step1_hc.png" alt="Allreduce 1 node 8 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_01_08_step2.png" alt="Allreduce 1 node 8 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は256KB以上で性能が向上しそれ未満で性能が低下</li>
  <li><strong>HCOLL</strong> はほぼ全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対してほぼ全域で性能が低下</li>
  <li>チューニング未適用は16KBから16MBの間で大幅に性能が低下</li>
</ul>

<h2 id="1-2--16-mpiプロセス">1-2.  16 MPIプロセス</h2>

<h3 id="1-2-0-概要">1-2-0. 概要</h3>

<p>本章は、1ノードに16 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<h3 id="1-2-1-alltoall">1-2-1. Alltoall</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="ata_01_16_n1_bl_no.png" alt="Alltoall 1 node 16 processes NPS1 no coll" /></p>

<p><img src="ata_01_16_n1_bl_uc.png" alt="Alltoall 1 node 16 processes NPS1 UCC" /></p>

<p><img src="ata_01_16_n1_bl_hc.png" alt="Alltoall 1 node 16 processes NPS1 HCOLL" /></p>

<p><img src="ata_01_16_n1_cy_no.png" alt="Alltoall 1 node 16 processes NPS1 no coll" /></p>

<p><img src="ata_01_16_n1_cy_uc.png" alt="Alltoall 1 node 16 processes NPS1 UCC" /></p>

<p><img src="ata_01_16_n1_cy_hc.png" alt="Alltoall 1 node 16 processes NPS1 HCOLL" /></p>

<p><img src="ata_01_16_n2_bl_no.png" alt="Alltoall 1 node 16 processes NPS2 no coll" /></p>

<p><img src="ata_01_16_n2_bl_uc.png" alt="Alltoall 1 node 16 processes NPS2 UCC" /></p>

<p><img src="ata_01_16_n2_bl_hc.png" alt="Alltoall 1 node 16 processes NPS2 HCOLL" /></p>

<p><img src="ata_01_16_n2_cy_no.png" alt="Alltoall 1 node 16 processes NPS2 no coll" /></p>

<p><img src="ata_01_16_n2_cy_uc.png" alt="Alltoall 1 node 16 processes NPS2 UCC" /></p>

<p><img src="ata_01_16_n2_cy_hc.png" alt="Alltoall 1 node 16 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">16KB</td>
      <td style="text-align: right">32KB</td>
      <td style="text-align: right">32KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="ata_01_16_step1_no.png" alt="Alltoall 1 node 16 processes no coll" /></p>

<p><img src="ata_01_16_step1_uc.png" alt="Alltoall 1 node 16 processes UCC" /></p>

<p><img src="ata_01_16_step1_hc.png" alt="Alltoall 1 node 16 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_01_16_step2.png" alt="Alltoall 1 node 16 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は16KB以上で性能が低下し128B未満で性能が向上</li>
  <li><strong>HCOLL</strong> は1KB以上で顕著な傾向無くそれ未満で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して32KB以上で性能が低下しそれ未満で顕著な傾向無し</li>
  <li>チューニング未適用は8KBから128KBの間で大幅に性能が低下</li>
</ul>

<h3 id="1-2-2-allgather">1-2-2. Allgather</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="aga_01_16_n1_bl_no.png" alt="Allgather 1 node 16 processes NPS1 no coll" /></p>

<p><img src="aga_01_16_n1_bl_uc.png" alt="Allgather 1 node 16 processes NPS1 UCC" /></p>

<p><img src="aga_01_16_n1_bl_hc.png" alt="Allgather 1 node 16 processes NPS1 HCOLL" /></p>

<p><img src="aga_01_16_n1_cy_no.png" alt="Allgather 1 node 16 processes NPS1 no coll" /></p>

<p><img src="aga_01_16_n1_cy_uc.png" alt="Allgather 1 node 16 processes NPS1 UCC" /></p>

<p><img src="aga_01_16_n1_cy_hc.png" alt="Allgather 1 node 16 processes NPS1 HCOLL" /></p>

<p><img src="aga_01_16_n2_bl_no.png" alt="Allgather 1 node 16 processes NPS2 no coll" /></p>

<p><img src="aga_01_16_n2_bl_uc.png" alt="Allgather 1 node 16 processes NPS2 UCC" /></p>

<p><img src="aga_01_16_n2_bl_hc.png" alt="Allgather 1 node 16 processes NPS2 HCOLL" /></p>

<p><img src="aga_01_16_n2_cy_no.png" alt="Allgather 1 node 16 processes NPS2 no coll" /></p>

<p><img src="aga_01_16_n2_cy_uc.png" alt="Allgather 1 node 16 processes NPS2 UCC" /></p>

<p><img src="aga_01_16_n2_cy_hc.png" alt="Allgather 1 node 16 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">64KB</td>
      <td style="text-align: right">32KB</td>
      <td style="text-align: right">32KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="aga_01_16_step1_no.png" alt="Allgather 1 node 16 processes no coll" /></p>

<p><img src="aga_01_16_step1_uc.png" alt="Allgather 1 node 16 processes UCC" /></p>

<p><img src="aga_01_16_step1_hc.png" alt="Allgather 1 node 16 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_01_16_step2.png" alt="Allgather 1 node 16 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は16KB以上で顕著な傾向無くそれ未満で性能が低下</li>
  <li><strong>HCOLL</strong> は32KB以上で顕著な傾向無くそれ未満のほぼ全域で性能が低下</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して4KBから16KBの間で性能が向上しそれ未満で性能が低下</li>
  <li>チューニング適用による性能の変化無し</li>
</ul>

<h3 id="1-2-3-allreduce">1-2-3. Allreduce</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="are_01_16_n1_bl_no.png" alt="Allreduce 1 node 16 processes NPS1 no coll" /></p>

<p><img src="are_01_16_n1_bl_uc.png" alt="Allreduce 1 node 16 processes NPS1 UCC" /></p>

<p><img src="are_01_16_n1_bl_hc.png" alt="Allreduce 1 node 16 processes NPS1 HCOLL" /></p>

<p><img src="are_01_16_n1_cy_no.png" alt="Allreduce 1 node 16 processes NPS1 no coll" /></p>

<p><img src="are_01_16_n1_cy_uc.png" alt="Allreduce 1 node 16 processes NPS1 UCC" /></p>

<p><img src="are_01_16_n1_cy_hc.png" alt="Allreduce 1 node 16 processes NPS1 HCOLL" /></p>

<p><img src="are_01_16_n2_bl_no.png" alt="Allreduce 1 node 16 processes NPS2 no coll" /></p>

<p><img src="are_01_16_n2_bl_uc.png" alt="Allreduce 1 node 16 processes NPS2 UCC" /></p>

<p><img src="are_01_16_n2_bl_hc.png" alt="Allreduce 1 node 16 processes NPS2 HCOLL" /></p>

<p><img src="are_01_16_n2_cy_no.png" alt="Allreduce 1 node 16 processes NPS2 no coll" /></p>

<p><img src="are_01_16_n2_cy_uc.png" alt="Allreduce 1 node 16 processes NPS2 UCC" /></p>

<p><img src="are_01_16_n2_cy_hc.png" alt="Allreduce 1 node 16 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">64KB</td>
      <td style="text-align: right">128KB</td>
      <td style="text-align: right">128KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="are_01_16_step1_no.png" alt="Allreduce 1 node 16 processes no coll" /></p>

<p><img src="are_01_16_step1_uc.png" alt="Allreduce 1 node 16 processes UCC" /></p>

<p><img src="are_01_16_step1_hc.png" alt="Allreduce 1 node 16 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_01_16_step2.png" alt="Allreduce 1 node 16 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は32KB以上で性能が向上しそれ未満で性能が低下</li>
  <li><strong>HCOLL</strong> は32MB以上で性能が向上し4KB以下で性能が低下</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して8KB以上で顕著な傾向が無くそれ未満で性能が低下</li>
  <li>チューニング未適用は8KBから16MBの間で大幅に性能が低下</li>
</ul>

<h2 id="1-3--32-mpiプロセス">1-3.  32 MPIプロセス</h2>

<h3 id="1-3-0-概要">1-3-0. 概要</h3>

<p>本章は、1ノードに32 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<h3 id="1-3-1-alltoall">1-3-1. Alltoall</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="ata_01_32_n1_bl_no.png" alt="Alltoall 1 node 32 processes NPS1 no coll" /></p>

<p><img src="ata_01_32_n1_bl_uc.png" alt="Alltoall 1 node 32 processes NPS1 UCC" /></p>

<p><img src="ata_01_32_n1_bl_hc.png" alt="Alltoall 1 node 32 processes NPS1 HCOLL" /></p>

<p><img src="ata_01_32_n1_cy_no.png" alt="Alltoall 1 node 32 processes NPS1 no coll" /></p>

<p><img src="ata_01_32_n1_cy_uc.png" alt="Alltoall 1 node 32 processes NPS1 UCC" /></p>

<p><img src="ata_01_32_n1_cy_hc.png" alt="Alltoall 1 node 32 processes NPS1 HCOLL" /></p>

<p><img src="ata_01_32_n2_bl_no.png" alt="Alltoall 1 node 32 processes NPS2 no coll" /></p>

<p><img src="ata_01_32_n2_bl_uc.png" alt="Alltoall 1 node 32 processes NPS2 UCC" /></p>

<p><img src="ata_01_32_n2_bl_hc.png" alt="Alltoall 1 node 32 processes NPS2 HCOLL" /></p>

<p><img src="ata_01_32_n2_cy_no.png" alt="Alltoall 1 node 32 processes NPS2 no coll" /></p>

<p><img src="ata_01_32_n2_cy_uc.png" alt="Alltoall 1 node 32 processes NPS2 UCC" /></p>

<p><img src="ata_01_32_n2_cy_hc.png" alt="Alltoall 1 node 32 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">16KB</td>
      <td style="text-align: right">16KB</td>
      <td style="text-align: right">16KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="ata_01_32_step1_no.png" alt="Alltoall 1 node 32 processes no coll" /></p>

<p><img src="ata_01_32_step1_uc.png" alt="Alltoall 1 node 32 processes UCC" /></p>

<p><img src="ata_01_32_step1_hc.png" alt="Alltoall 1 node 32 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_01_32_step2.png" alt="Alltoall 1 node 32 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は16KB以上と128B以下で性能が低下</li>
  <li><strong>HCOLL</strong> は2KB以上で顕著な傾向が無くそれ未満で概ね性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して16KB以上で性能が低下しそれ未満で顕著な傾向無し</li>
  <li>チューニング未適用は8KBから512KBの間で大幅に性能が低下</li>
</ul>

<h3 id="1-3-2-allgather">1-3-2. Allgather</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="aga_01_32_n1_bl_no.png" alt="Allgather 1 node 32 processes NPS1 no coll" /></p>

<p><img src="aga_01_32_n1_bl_uc.png" alt="Allgather 1 node 32 processes NPS1 UCC" /></p>

<p><img src="aga_01_32_n1_bl_hc.png" alt="Allgather 1 node 32 processes NPS1 HCOLL" /></p>

<p><img src="aga_01_32_n1_cy_no.png" alt="Allgather 1 node 32 processes NPS1 no coll" /></p>

<p><img src="aga_01_32_n1_cy_uc.png" alt="Allgather 1 node 32 processes NPS1 UCC" /></p>

<p><img src="aga_01_32_n1_cy_hc.png" alt="Allgather 1 node 32 processes NPS1 HCOLL" /></p>

<p><img src="aga_01_32_n2_bl_no.png" alt="Allgather 1 node 32 processes NPS2 no coll" /></p>

<p><img src="aga_01_32_n2_bl_uc.png" alt="Allgather 1 node 32 processes NPS2 UCC" /></p>

<p><img src="aga_01_32_n2_bl_hc.png" alt="Allgather 1 node 32 processes NPS2 HCOLL" /></p>

<p><img src="aga_01_32_n2_cy_no.png" alt="Allgather 1 node 32 processes NPS2 no coll" /></p>

<p><img src="aga_01_32_n2_cy_uc.png" alt="Allgather 1 node 32 processes NPS2 UCC" /></p>

<p><img src="aga_01_32_n2_cy_hc.png" alt="Allgather 1 node 32 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">16KB</td>
      <td style="text-align: right">16KB</td>
      <td style="text-align: right">16KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="aga_01_32_step1_no.png" alt="Allgather 1 node 32 processes no coll" /></p>

<p><img src="aga_01_32_step1_uc.png" alt="Allgather 1 node 32 processes UCC" /></p>

<p><img src="aga_01_32_step1_hc.png" alt="Allgather 1 node 32 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_01_32_step2.png" alt="Allgather 1 node 32 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は128Bから512Bの間で性能が低下しそれ以外で顕著な傾向無し</li>
  <li><strong>HCOLL</strong> は2KBから8KBの間で性能が低下しそれ以外で顕著な傾向無し</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して128Bから512Bの間で性能が低下し2KBから8KBの間で性能が向上</li>
  <li>チューニング適用による性能の変化無し</li>
</ul>

<h3 id="1-3-3-allreduce">1-3-3. Allreduce</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="are_01_32_n1_bl_no.png" alt="Allreduce 1 node 32 processes NPS1 no coll" /></p>

<p><img src="are_01_32_n1_bl_uc.png" alt="Allreduce 1 node 32 processes NPS1 UCC" /></p>

<p><img src="are_01_32_n1_bl_hc.png" alt="Allreduce 1 node 32 processes NPS1 HCOLL" /></p>

<p><img src="are_01_32_n1_cy_no.png" alt="Allreduce 1 node 32 processes NPS1 no coll" /></p>

<p><img src="are_01_32_n1_cy_uc.png" alt="Allreduce 1 node 32 processes NPS1 UCC" /></p>

<p><img src="are_01_32_n1_cy_hc.png" alt="Allreduce 1 node 32 processes NPS1 HCOLL" /></p>

<p><img src="are_01_32_n2_bl_no.png" alt="Allreduce 1 node 32 processes NPS2 no coll" /></p>

<p><img src="are_01_32_n2_bl_uc.png" alt="Allreduce 1 node 32 processes NPS2 UCC" /></p>

<p><img src="are_01_32_n2_bl_hc.png" alt="Allreduce 1 node 32 processes NPS2 HCOLL" /></p>

<p><img src="are_01_32_n2_cy_no.png" alt="Allreduce 1 node 32 processes NPS2 no coll" /></p>

<p><img src="are_01_32_n2_cy_uc.png" alt="Allreduce 1 node 32 processes NPS2 UCC" /></p>

<p><img src="are_01_32_n2_cy_hc.png" alt="Allreduce 1 node 32 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">Auto</td>
      <td style="text-align: right">128KB</td>
      <td style="text-align: right">128KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="are_01_32_step1_no.png" alt="Allreduce 1 node 32 processes no coll" /></p>

<p><img src="are_01_32_step1_uc.png" alt="Allreduce 1 node 32 processes UCC" /></p>

<p><img src="are_01_32_step1_hc.png" alt="Allreduce 1 node 32 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_01_32_step2.png" alt="Allreduce 1 node 32 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は512KB以上で性能が向上しそれ未満で顕著な傾向無し</li>
  <li><strong>HCOLL</strong> は32MB以上と4KB以下で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して4KB以下で性能が低下</li>
  <li>チューニング未適用は64KBから16MBの間で大幅に性能が低下</li>
</ul>

<h2 id="1-4--36-mpiプロセス">1-4.  36 MPIプロセス</h2>

<h3 id="1-4-0-概要">1-4-0. 概要</h3>

<p>本章は、1ノードに36 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<h3 id="1-4-1-alltoall">1-4-1. Alltoall</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="ata_01_36_n1_bl_no.png" alt="Alltoall 1 node 36 processes NPS1 no coll" /></p>

<p><img src="ata_01_36_n1_bl_uc.png" alt="Alltoall 1 node 36 processes NPS1 UCC" /></p>

<p><img src="ata_01_36_n1_bl_hc.png" alt="Alltoall 1 node 36 processes NPS1 HCOLL" /></p>

<p><img src="ata_01_36_n1_cy_no.png" alt="Alltoall 1 node 36 processes NPS1 no coll" /></p>

<p><img src="ata_01_36_n1_cy_uc.png" alt="Alltoall 1 node 36 processes NPS1 UCC" /></p>

<p><img src="ata_01_36_n1_cy_hc.png" alt="Alltoall 1 node 36 processes NPS1 HCOLL" /></p>

<p><img src="ata_01_36_n2_bl_no.png" alt="Alltoall 1 node 36 processes NPS2 no coll" /></p>

<p><img src="ata_01_36_n2_bl_uc.png" alt="Alltoall 1 node 36 processes NPS2 UCC" /></p>

<p><img src="ata_01_36_n2_bl_hc.png" alt="Alltoall 1 node 36 processes NPS2 HCOLL" /></p>

<p><img src="ata_01_36_n2_cy_no.png" alt="Alltoall 1 node 36 processes NPS2 no coll" /></p>

<p><img src="ata_01_36_n2_cy_uc.png" alt="Alltoall 1 node 36 processes NPS2 UCC" /></p>

<p><img src="ata_01_36_n2_cy_hc.png" alt="Alltoall 1 node 36 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">16KB</td>
      <td style="text-align: right">16KB</td>
      <td style="text-align: right">16KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="ata_01_36_step1_no.png" alt="Alltoall 1 node 36 processes no coll" /></p>

<p><img src="ata_01_36_step1_uc.png" alt="Alltoall 1 node 36 processes UCC" /></p>

<p><img src="ata_01_36_step1_hc.png" alt="Alltoall 1 node 36 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_01_36_step2.png" alt="Alltoall 1 node 36 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は16KB以上で性能が低下し128B以下で性能が向上</li>
  <li><strong>HCOLL</strong> は128KB以上で性能が低下し1KB以下で概ね性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して16KBから128KBの間で性能が低下</li>
  <li>チューニング未適用は8KBから256KBの間で大幅に性能が低下</li>
</ul>

<h3 id="1-4-2-allgather">1-4-2. Allgather</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="aga_01_36_n1_bl_no.png" alt="Allgather 1 node 36 processes NPS1 no coll" /></p>

<p><img src="aga_01_36_n1_bl_uc.png" alt="Allgather 1 node 36 processes NPS1 UCC" /></p>

<p><img src="aga_01_36_n1_bl_hc.png" alt="Allgather 1 node 36 processes NPS1 HCOLL" /></p>

<p><img src="aga_01_36_n1_cy_no.png" alt="Allgather 1 node 36 processes NPS1 no coll" /></p>

<p><img src="aga_01_36_n1_cy_uc.png" alt="Allgather 1 node 36 processes NPS1 UCC" /></p>

<p><img src="aga_01_36_n1_cy_hc.png" alt="Allgather 1 node 36 processes NPS1 HCOLL" /></p>

<p><img src="aga_01_36_n2_bl_no.png" alt="Allgather 1 node 36 processes NPS2 no coll" /></p>

<p><img src="aga_01_36_n2_bl_uc.png" alt="Allgather 1 node 36 processes NPS2 UCC" /></p>

<p><img src="aga_01_36_n2_bl_hc.png" alt="Allgather 1 node 36 processes NPS2 HCOLL" /></p>

<p><img src="aga_01_36_n2_cy_no.png" alt="Allgather 1 node 36 processes NPS2 no coll" /></p>

<p><img src="aga_01_36_n2_cy_uc.png" alt="Allgather 1 node 36 processes NPS2 UCC" /></p>

<p><img src="aga_01_36_n2_cy_hc.png" alt="Allgather 1 node 36 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">16KB</td>
      <td style="text-align: right">16KB</td>
      <td style="text-align: right">16KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="aga_01_36_step1_no.png" alt="Allgather 1 node 36 processes no coll" /></p>

<p><img src="aga_01_36_step1_uc.png" alt="Allgather 1 node 36 processes UCC" /></p>

<p><img src="aga_01_36_step1_hc.png" alt="Allgather 1 node 36 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_01_36_step2.png" alt="Allgather 1 node 36 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は64B以下で性能が向上しそれ以外で顕著な傾向無し</li>
  <li><strong>HCOLL</strong> は1KB以下で性能が向上しそれ以外で顕著な傾向無し</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して2KBから8KBの間で性能が向上しそれ未満で性能が低下</li>
  <li>チューニング適用による性能の変化無し</li>
</ul>

<h3 id="1-4-3-allreduce">1-4-3. Allreduce</h3>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>NPS</strong> 、MPIプロセス分割方法、及び集合通信コンポーネントの組合せ毎に示しています。</p>

<p><img src="are_01_36_n1_bl_no.png" alt="Allreduce 1 node 36 processes NPS1 no coll" /></p>

<p><img src="are_01_36_n1_bl_uc.png" alt="Allreduce 1 node 36 processes NPS1 UCC" /></p>

<p><img src="are_01_36_n1_bl_hc.png" alt="Allreduce 1 node 36 processes NPS1 HCOLL" /></p>

<p><img src="are_01_36_n1_cy_no.png" alt="Allreduce 1 node 36 processes NPS1 no coll" /></p>

<p><img src="are_01_36_n1_cy_uc.png" alt="Allreduce 1 node 36 processes NPS1 UCC" /></p>

<p><img src="are_01_36_n1_cy_hc.png" alt="Allreduce 1 node 36 processes NPS1 HCOLL" /></p>

<p><img src="are_01_36_n2_bl_no.png" alt="Allreduce 1 node 36 processes NPS2 no coll" /></p>

<p><img src="are_01_36_n2_bl_uc.png" alt="Allreduce 1 node 36 processes NPS2 UCC" /></p>

<p><img src="are_01_36_n2_bl_hc.png" alt="Allreduce 1 node 36 processes NPS2 HCOLL" /></p>

<p><img src="are_01_36_n2_cy_no.png" alt="Allreduce 1 node 36 processes NPS2 no coll" /></p>

<p><img src="are_01_36_n2_cy_uc.png" alt="Allreduce 1 node 36 processes NPS2 UCC" /></p>

<p><img src="are_01_36_n2_cy_hc.png" alt="Allreduce 1 node 36 processes NPS2 HCOLL" /></p>

<p>以上より、各集合通信コンポーネントの <strong>UCX_RNDV_THRESH</strong> を下表の値とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>UCX_RNDV_THRESH</strong></td>
      <td style="text-align: right">Auto</td>
      <td style="text-align: right">128KB</td>
      <td style="text-align: right">128KB</td>
    </tr>
  </tbody>
</table>

<p><strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものが以下のグラフです。</p>

<p><img src="are_01_36_step1_no.png" alt="Allreduce 1 node 36 processes no coll" /></p>

<p><img src="are_01_36_step1_uc.png" alt="Allreduce 1 node 36 processes UCC" /></p>

<p><img src="are_01_36_step1_hc.png" alt="Allreduce 1 node 36 processes HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_01_36_step2.png" alt="Allreduce 1 node 36 processes" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は128B以上で性能が向上</li>
  <li><strong>HCOLL</strong> は128KB以上と4KB以下で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して8KB以上で顕著な傾向が無くそれ未満で性能が低下</li>
  <li>チューニング未適用は64KBから64MBの間で大幅に性能が低下</li>
</ul>

<hr />
<h1 id="2--2ノード">2.  2ノード</h1>

<h2 id="2-0-概要">2-0. 概要</h2>

<p>本章は、2ノードにノード当たり8・16・32・36の各MPIプロセス、トータルで16・32・64・72の各MPIプロセスを割当てる場合の各MPI集合通信関数の通信性能について、以下の <strong>実行時パラメータ</strong> の最適な組み合わせを検証します。</p>

<ul>
  <li><strong>UCX_TLS</strong> ： <strong>all</strong> ・ <strong>self,sm,rc</strong> ・ <strong>self,sm,ud</strong> ・ <strong>self,sm,dc</strong></li>
  <li><strong>UCX_RNDV_THRESH</strong> ： <strong>auto</strong> ・ <strong>4kb</strong> ・ <strong>8kb</strong> ・ <strong>16kb</strong> ・ <strong>32kb</strong> ・ <strong>64kb</strong> ・ <strong>128kb</strong> （※13）</li>
  <li><strong>UCX_ZCOPY_THRESH</strong> ： <strong>auto</strong> ・ <strong>4kb</strong> ・ <strong>8kb</strong> ・ <strong>16kb</strong> ・ <strong>32kb</strong> ・ <strong>64kb</strong> ・ <strong>128kb</strong></li>
  <li><strong>coll_hcoll_enable</strong> ： <strong>0</strong> ・ <strong>1</strong></li>
  <li><strong>coll_ucc_enable</strong> ： <strong>0</strong> ・ <strong>1</strong></li>
  <li>MPIプロセス分割方法 ： ブロック分割・サイクリック分割</li>
  <li><strong>NPS</strong> ：  <strong>1</strong> ・ <strong>2</strong></li>
</ul>

<p>※13） <strong>UCX_RNDV_THRESH</strong> は、ノード内は <strong>auto</strong> と <strong><a href="#1--1ノード">1. 1ノード</a></strong> で判明した最適値、ノード間はここに記載の7種類を使用し、以下8個の組合せを検証します。</p>

<ul>
  <li><strong>intra:auto,inter:auto</strong></li>
  <li><strong>intra:optimal_value,inter:auto</strong></li>
  <li><strong>intra:optimal_value,inter:4kb</strong></li>
  <li><strong>intra:optimal_value,inter:8kb</strong></li>
  <li><strong>intra:optimal_value,inter:16kb</strong></li>
  <li><strong>intra:optimal_value,inter:32kb</strong></li>
  <li><strong>intra:optimal_value,inter:64kb</strong></li>
  <li><strong>intra:optimal_value,inter:128kb</strong></li>
</ul>

<p>ここで、全ての <strong>実行時パラメータ</strong> の組み合わせを検証することは非現実的なため、組み合わせを減らす目的で以下3ステップに分けて検証を行います。</p>

<ul>
  <li>ステップ1
    <ul>
      <li><strong>UCX_TLS</strong> と <strong>UCX_RNDV_THRESH</strong> を組合せた32個のパターンを検証してこれらの最適値を決定</li>
      <li><strong>coll_hcoll_enable</strong> は <strong>1</strong> に固定（デフォルト）</li>
      <li><strong>coll_ucc_enable</strong> は <strong>0</strong> に固定（デフォルト）</li>
      <li>MPIプロセス分割方法はブロック分割に固定</li>
      <li><strong>NPS</strong> は <strong>NPS1</strong> に固定</li>
    </ul>
  </li>
  <li>ステップ2
    <ul>
      <li><strong>UCX_ZCOPY_THRESH</strong> の7パターンを検証してこの最適値を決定</li>
      <li><strong>UCX_TLS</strong> と <strong>UCX_RNDV_THRESH</strong> はステップ1で決定した最適値を使用</li>
      <li><strong>coll_hcoll_enable</strong> は <strong>1</strong> に固定（デフォルト）</li>
      <li><strong>coll_ucc_enable</strong> は <strong>0</strong> に固定（デフォルト）</li>
      <li>MPIプロセス分割方法はブロック分割に固定</li>
      <li><strong>NPS</strong> は <strong>NPS1</strong> に固定</li>
    </ul>
  </li>
  <li>ステップ3
    <ul>
      <li><strong>coll_hcoll_enable</strong> / <strong>coll_ucc_enable</strong> / MPIプロセス分割方法 / <strong>NPS</strong> を組合せた12パターンを検証してこれらの最適値を決定</li>
      <li><strong>UCX_TLS</strong> と <strong>UCX_RNDV_THRESH</strong> はステップ1で決定した最適値を使用</li>
      <li><strong>UCX_ZCOPY_THRESH</strong> はステップ2で決定した最適値を使用</li>
    </ul>
  </li>
</ul>

<h2 id="2-1-ノード当たり8-mpiプロセス">2-1. ノード当たり8 MPIプロセス</h2>

<h3 id="2-1-0-概要">2-1-0. 概要</h3>

<p>本章は、2ノードにノード当たり8 MPIプロセスでトータル16 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:32kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:64kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="2-1-1-alltoall">2-1-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-1-1-alltoall">1-1-1 Alltoall</a></strong> の <strong>HCOLL</strong> の結果から32kbとしています。</p>

<p><img src="ata_02_08_step1_all.png" alt="Alltoall 2 node 8 ppn all step1" /></p>

<p><img src="ata_02_08_step1_dc.png" alt="Alltoall 2 node 8 ppn dc step1" /></p>

<p><img src="ata_02_08_step1_rc.png" alt="Alltoall 2 node 8 ppn rc step1" /></p>

<p><img src="ata_02_08_step1_ud.png" alt="Alltoall 2 node 8 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:32kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_02_08_step1.png" alt="Alltoall 2 node 8 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_02_08_step2.png" alt="Alltoall 2 node 8 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_02_08_step3_no.png" alt="Alltoall 2 node 8 ppn no coll" /></p>

<p><img src="ata_02_08_step3_uc.png" alt="Alltoall 2 node 8 ppn UCC" /></p>

<p><img src="ata_02_08_step3_hc.png" alt="Alltoall 2 node 8 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_02_08_step3.png" alt="Alltoall 2 node 8 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は4KBから256KBの間で性能が低下し256B以下で概ね性能が向上</li>
  <li><strong>HCOLL</strong> は256B以下で概ね性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して64KB以下で概ね性能が低下</li>
  <li>チューニング未適用は2KBから128KBの間で大幅に性能が低下</li>
</ul>

<h3 id="2-1-2-allgather">2-1-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-1-2-allgather">1-1-2 Allgather</a></strong> の <strong>HCOLL</strong> の結果から64kbとしています。</p>

<p><img src="aga_02_08_step1_all.png" alt="Allgather 2 node 8 ppn all step1" /></p>

<p><img src="aga_02_08_step1_dc.png" alt="Allgather 2 node 8 ppn dc step1" /></p>

<p><img src="aga_02_08_step1_rc.png" alt="Allgather 2 node 8 ppn rc step1" /></p>

<p><img src="aga_02_08_step1_ud.png" alt="Allgather 2 node 8 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:64kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_02_08_step1.png" alt="Allgather 2 node 8 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_02_08_step2.png" alt="Allgather 2 node 8 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_02_08_step3_no.png" alt="Allgather 2 node 8 ppn no coll" /></p>

<p><img src="aga_02_08_step3_uc.png" alt="Allgather 2 node 8 ppn UCC" /></p>

<p><img src="aga_02_08_step3_hc.png" alt="Allgather 2 node 8 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_02_08_step3.png" alt="Allgather 2 node 8 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は全域で性能が向上</li>
  <li><strong>HCOLL</strong> はほぼ全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して16KB以下で概ね性能が向上</li>
  <li>チューニング適用による性能の変化無し</li>
</ul>

<h3 id="2-1-3-allreduce">2-1-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-1-3-allreduce">1-1-3 Allreduce</a></strong> の <strong>HCOLL</strong> の結果から128kbとしています。</p>

<p><img src="are_02_08_step1_all.png" alt="Allreduce 2 node 8 ppn all step1" /></p>

<p><img src="are_02_08_step1_dc.png" alt="Allreduce 2 node 8 ppn dc step1" /></p>

<p><img src="are_02_08_step1_rc.png" alt="Allreduce 2 node 8 ppn rc step1" /></p>

<p><img src="are_02_08_step1_ud.png" alt="Allreduce 2 node 8 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_02_08_step1.png" alt="Allreduce 2 node 8 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_02_08_step2.png" alt="Allreduce 2 node 8 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_02_08_step3_no.png" alt="Allreduce 2 node 8 ppn no coll" /></p>

<p><img src="are_02_08_step3_uc.png" alt="Allreduce 2 node 8 ppn UCC" /></p>

<p><img src="are_02_08_step3_hc.png" alt="Allreduce 2 node 8 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_02_08_step3.png" alt="Allreduce 2 node 8 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して128Bから512KBで性能が低下しそれ以外で顕著な傾向無し</li>
  <li>チューニング未適用は16KBから16MBの間で大幅に性能が低下</li>
</ul>

<h2 id="2-2-ノード当たり16-mpiプロセス">2-2. ノード当たり16 MPIプロセス</h2>

<h3 id="2-2-0-概要">2-2-0. 概要</h3>

<p>本章は、2ノードにノード当たり16 MPIプロセスでトータル32 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:32kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:32kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="2-2-1-alltoall">2-2-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-2-1-alltoall">1-2-1 Alltoall</a></strong> の <strong>HCOLL</strong> の結果から32kbとしています。</p>

<p><img src="ata_02_16_step1_all.png" alt="Alltoall 2 node 16 ppn all step1" /></p>

<p><img src="ata_02_16_step1_dc.png" alt="Alltoall 2 node 16 ppn dc step1" /></p>

<p><img src="ata_02_16_step1_rc.png" alt="Alltoall 2 node 16 ppn rc step1" /></p>

<p><img src="ata_02_16_step1_ud.png" alt="Alltoall 2 node 16 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:32kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_02_16_step1.png" alt="Alltoall 2 node 16 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_02_16_step2.png" alt="Alltoall 2 node 16 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_02_16_step3_no.png" alt="Alltoall 2 node 16 ppn no coll" /></p>

<p><img src="ata_02_16_step3_uc.png" alt="Alltoall 2 node 16 ppn UCC" /></p>

<p><img src="ata_02_16_step3_hc.png" alt="Alltoall 2 node 16 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_02_16_step3.png" alt="Alltoall 2 node 16 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は512B未満で性能が向上しそれ以上で顕著な傾向無し</li>
  <li><strong>HCOLL</strong> は512B未満で性能が向上しそれ以上で顕著な傾向無し</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して64KB以下で概ね性能が低下</li>
  <li>チューニング未適用は2KBから32KBの間で大幅に性能が低下</li>
</ul>

<h3 id="2-2-2-allgather">2-2-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-2-2-allgather">1-2-2 Allgather</a></strong> の <strong>HCOLL</strong> の結果から32kbとしています。</p>

<p><img src="aga_02_16_step1_all.png" alt="Allgather 2 node 16 ppn all step1" /></p>

<p><img src="aga_02_16_step1_dc.png" alt="Allgather 2 node 16 ppn dc step1" /></p>

<p><img src="aga_02_16_step1_rc.png" alt="Allgather 2 node 16 ppn rc step1" /></p>

<p><img src="aga_02_16_step1_ud.png" alt="Allgather 2 node 16 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:32kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_02_16_step1.png" alt="Allgather 2 node 16 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_02_16_step2.png" alt="Allgather 2 node 16 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_02_16_step3_no.png" alt="Allgather 2 node 16 ppn no coll" /></p>

<p><img src="aga_02_16_step3_uc.png" alt="Allgather 2 node 16 ppn UCC" /></p>

<p><img src="aga_02_16_step3_hc.png" alt="Allgather 2 node 16 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_02_16_step3.png" alt="Allgather 2 node 16 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は全域で性能が向上</li>
  <li><strong>HCOLL</strong> はほぼ全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して顕著な傾向無し</li>
  <li>チューニング適用による性能の変化無し</li>
</ul>

<h3 id="2-2-3-allreduce">2-2-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-2-3-allreduce">1-2-3 Allreduce</a></strong> の <strong>HCOLL</strong> の結果から128kbとしています。</p>

<p><img src="are_02_16_step1_all.png" alt="Allreduce 2 node 16 ppn all step1" /></p>

<p><img src="are_02_16_step1_dc.png" alt="Allreduce 2 node 16 ppn dc step1" /></p>

<p><img src="are_02_16_step1_rc.png" alt="Allreduce 2 node 16 ppn rc step1" /></p>

<p><img src="are_02_16_step1_ud.png" alt="Allreduce 2 node 16 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_02_16_step1.png" alt="Allreduce 2 node 16 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_02_16_step2.png" alt="Allreduce 2 node 16 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_02_16_step3_no.png" alt="Allreduce 2 node 16 ppn no coll" /></p>

<p><img src="are_02_16_step3_uc.png" alt="Allreduce 2 node 16 ppn UCC" /></p>

<p><img src="are_02_16_step3_hc.png" alt="Allreduce 2 node 16 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_02_16_step3.png" alt="Allreduce 2 node 16 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して1MBから16MBで性能が低下し4KB以下で性能が向上</li>
  <li>チューニング未適用は32KBから16MBの間で大幅に性能が低下</li>
</ul>

<h2 id="2-3-ノード当たり32-mpiプロセス">2-3. ノード当たり32 MPIプロセス</h2>

<h3 id="2-3-0-概要">2-3-0. 概要</h3>

<p>本章は、2ノードにノード当たり32 MPIプロセスでトータル64 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="2-3-1-alltoall">2-3-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-3-1-alltoall">1-3-1 Alltoall</a></strong> の <strong>HCOLL</strong> の結果から16kbとしています。</p>

<p><img src="ata_02_32_step1_all.png" alt="Alltoall 2 node 32 ppn all step1" /></p>

<p><img src="ata_02_32_step1_dc.png" alt="Alltoall 2 node 32 ppn dc step1" /></p>

<p><img src="ata_02_32_step1_rc.png" alt="Alltoall 2 node 32 ppn rc step1" /></p>

<p><img src="ata_02_32_step1_ud.png" alt="Alltoall 2 node 32 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_02_32_step1.png" alt="Alltoall 2 node 32 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_02_32_step2.png" alt="Alltoall 2 node 32 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_02_32_step3_no.png" alt="Alltoall 2 node 32 ppn no coll" /></p>

<p><img src="ata_02_32_step3_uc.png" alt="Alltoall 2 node 32 ppn UCC" /></p>

<p><img src="ata_02_32_step3_hc.png" alt="Alltoall 2 node 32 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_02_32_step3.png" alt="Alltoall 2 node 32 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は1KB未満で性能が向上しそれ以上で顕著な傾向無し</li>
  <li><strong>HCOLL</strong> は1KB未満で性能が向上しそれ以上で顕著な傾向無し</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して256B以下で概ね性能が低下</li>
  <li>チューニング未適用は8KB以下で性能が低下</li>
</ul>

<h3 id="2-3-2-allgather">2-3-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-3-2-allgather">1-3-2 Allgather</a></strong> の <strong>HCOLL</strong> の結果から16kbとしています。</p>

<p><img src="aga_02_32_step1_all.png" alt="Allgather 2 node 32 ppn all step1" /></p>

<p><img src="aga_02_32_step1_dc.png" alt="Allgather 2 node 32 ppn dc step1" /></p>

<p><img src="aga_02_32_step1_rc.png" alt="Allgather 2 node 32 ppn rc step1" /></p>

<p><img src="aga_02_32_step1_ud.png" alt="Allgather 2 node 32 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_02_32_step1.png" alt="Allgather 2 node 32 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_02_32_step2.png" alt="Allgather 2 node 32 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_02_32_step3_no.png" alt="Allgather 2 node 32 ppn no coll" /></p>

<p><img src="aga_02_32_step3_uc.png" alt="Allgather 2 node 32 ppn UCC" /></p>

<p><img src="aga_02_32_step3_hc.png" alt="Allgather 2 node 32 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_02_32_step3.png" alt="Allgather 2 node 32 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> はほぼ全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して1KBから4KBの間と16B以下で性能が向上し64Bから512Bの間で性能が低下</li>
  <li>チューニング未適用は32KB以下で性能が低下</li>
</ul>

<h3 id="2-3-3-allreduce">2-3-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-3-3-allreduce">1-3-3 Allreduce</a></strong> の <strong>HCOLL</strong> の結果から128kbとしています。</p>

<p><img src="are_02_32_step1_all.png" alt="Allreduce 2 node 32 ppn all step1" /></p>

<p><img src="are_02_32_step1_dc.png" alt="Allreduce 2 node 32 ppn dc step1" /></p>

<p><img src="are_02_32_step1_rc.png" alt="Allreduce 2 node 32 ppn rc step1" /></p>

<p><img src="are_02_32_step1_ud.png" alt="Allreduce 2 node 32 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_02_32_step1.png" alt="Allreduce 2 node 32 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_02_32_step2.png" alt="Allreduce 2 node 32 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_02_32_step3_no.png" alt="Allreduce 2 node 32 ppn no coll" /></p>

<p><img src="are_02_32_step3_uc.png" alt="Allreduce 2 node 32 ppn UCC" /></p>

<p><img src="are_02_32_step3_hc.png" alt="Allreduce 2 node 32 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_02_32_step3.png" alt="Allreduce 2 node 32 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> はほぼ全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して1MBから16MBまで性能が向上しそれ以上とそれ以下で概ね性能が低下</li>
  <li>チューニング未適用は16MB以下で大幅に性能が低下</li>
</ul>

<h2 id="2-4-ノード当たり36-mpiプロセス">2-4. ノード当たり36 MPIプロセス</h2>

<h3 id="2-4-0-概要">2-4-0. 概要</h3>

<p>本章は、2ノードにノード当たり36 MPIプロセスでトータル72 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="2-4-1-alltoall">2-4-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-4-1-alltoall">1-4-1 Alltoall</a></strong> の <strong>HCOLL</strong> の結果から16kbとしています。</p>

<p><img src="ata_02_36_step1_all.png" alt="Alltoall 2 node 36 ppn all step1" /></p>

<p><img src="ata_02_36_step1_dc.png" alt="Alltoall 2 node 36 ppn dc step1" /></p>

<p><img src="ata_02_36_step1_rc.png" alt="Alltoall 2 node 36 ppn rc step1" /></p>

<p><img src="ata_02_36_step1_ud.png" alt="Alltoall 2 node 36 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_02_36_step1.png" alt="Alltoall 2 node 36 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_02_36_step2.png" alt="Alltoall 2 node 36 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_02_36_step3_no.png" alt="Alltoall 2 node 36 ppn no coll" /></p>

<p><img src="ata_02_36_step3_uc.png" alt="Alltoall 2 node 36 ppn UCC" /></p>

<p><img src="ata_02_36_step3_hc.png" alt="Alltoall 2 node 36 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_02_36_step3.png" alt="Alltoall 2 node 36 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は512B以下で性能が向上</li>
  <li><strong>HCOLL</strong> は512B以下で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して512B以下で概ね性能が低下</li>
  <li>チューニング未適用は256KB以下で性能が低下</li>
</ul>

<h3 id="2-4-2-allgather">2-4-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-4-2-allgather">1-4-2 Allgather</a></strong> の <strong>HCOLL</strong> の結果から16kbとしています。</p>

<p><img src="aga_02_36_step1_all.png" alt="Allgather 2 node 36 ppn all step1" /></p>

<p><img src="aga_02_36_step1_dc.png" alt="Allgather 2 node 36 ppn dc step1" /></p>

<p><img src="aga_02_36_step1_rc.png" alt="Allgather 2 node 36 ppn rc step1" /></p>

<p><img src="aga_02_36_step1_ud.png" alt="Allgather 2 node 36 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_02_36_step1.png" alt="Allgather 2 node 36 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_02_36_step2.png" alt="Allgather 2 node 36 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_02_36_step3_no.png" alt="Allgather 2 node 36 ppn no coll" /></p>

<p><img src="aga_02_36_step3_uc.png" alt="Allgather 2 node 36 ppn UCC" /></p>

<p><img src="aga_02_36_step3_hc.png" alt="Allgather 2 node 36 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_02_36_step3.png" alt="Allgather 2 node 36 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> はほぼ全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して1KBから4KBの間で性能が向上し64Bから512Bの間で性能が低下</li>
  <li>チューニング未適用は16KB以下で性能が低下</li>
</ul>

<h3 id="2-4-3-allreduce">2-4-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-4-3-allreduce">1-4-3 Allreduce</a></strong> の <strong>HCOLL</strong> の結果から128kbとしています。</p>

<p><img src="are_02_36_step1_all.png" alt="Allreduce 2 node 36 ppn all step1" /></p>

<p><img src="are_02_36_step1_dc.png" alt="Allreduce 2 node 36 ppn dc step1" /></p>

<p><img src="are_02_36_step1_rc.png" alt="Allreduce 2 node 36 ppn rc step1" /></p>

<p><img src="are_02_36_step1_ud.png" alt="Allreduce 2 node 36 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_02_36_step1.png" alt="Allreduce 2 node 36 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_02_36_step2.png" alt="Allreduce 2 node 36 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_02_36_step3_no.png" alt="Allreduce 2 node 36 ppn no coll" /></p>

<p><img src="are_02_36_step3_uc.png" alt="Allreduce 2 node 36 ppn UCC" /></p>

<p><img src="are_02_36_step3_hc.png" alt="Allreduce 2 node 36 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_02_36_step3.png" alt="Allreduce 2 node 36 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して8KB以上で概ね性能が向上しそれ未満で性能が低下</li>
  <li>チューニング未適用は64MB以下で大幅に性能が低下</li>
</ul>

<hr />
<h1 id="3--4ノード">3.  4ノード</h1>

<h2 id="3-0-概要">3-0. 概要</h2>

<p>本章は、4ノードにノード当たり8・16・32・36の各MPIプロセス、トータルで32・64・128・144の各MPIプロセスを割当てる場合の各MPI集合通信関数の通信性能について、以下の <strong>実行時パラメータ</strong> の最適な組み合わせを検証します。</p>

<ul>
  <li><strong>UCX_TLS</strong> ： <strong>all</strong> ・ <strong>self,sm,rc</strong> ・ <strong>self,sm,ud</strong> ・ <strong>self,sm,dc</strong></li>
  <li><strong>UCX_RNDV_THRESH</strong> ： <strong>auto</strong> ・ <strong>4kb</strong> ・ <strong>8kb</strong> ・ <strong>16kb</strong> ・ <strong>32kb</strong> ・ <strong>64kb</strong> ・ <strong>128kb</strong> （※14）</li>
  <li><strong>UCX_ZCOPY_THRESH</strong> ： <strong>auto</strong> ・ <strong>4kb</strong> ・ <strong>8kb</strong> ・ <strong>16kb</strong> ・ <strong>32kb</strong> ・ <strong>64kb</strong> ・ <strong>128kb</strong></li>
  <li><strong>coll_hcoll_enable</strong> ： <strong>0</strong> ・ <strong>1</strong></li>
  <li><strong>coll_ucc_enable</strong> ： <strong>0</strong> ・ <strong>1</strong></li>
  <li>MPIプロセス分割方法 ： ブロック分割・サイクリック分割</li>
  <li><strong>NPS</strong> ：  <strong>1</strong> ・ <strong>2</strong></li>
</ul>

<p>※14） <strong>UCX_RNDV_THRESH</strong> は、ノード内は <strong>auto</strong> と <strong><a href="#1--1ノード">1. 1ノード</a></strong> で判明した最適値、ノード間はここに記載の7種類を使用し、以下8個の組合せを検証します。</p>

<ul>
  <li><strong>intra:auto,inter:auto</strong></li>
  <li><strong>intra:optimal_value,inter:auto</strong></li>
  <li><strong>intra:optimal_value,inter:4kb</strong></li>
  <li><strong>intra:optimal_value,inter:8kb</strong></li>
  <li><strong>intra:optimal_value,inter:16kb</strong></li>
  <li><strong>intra:optimal_value,inter:32kb</strong></li>
  <li><strong>intra:optimal_value,inter:64kb</strong></li>
  <li><strong>intra:optimal_value,inter:128kb</strong></li>
</ul>

<p>ここで、全ての <strong>実行時パラメータ</strong> の組み合わせを検証することは非現実的なため、組み合わせを減らす目的で以下3ステップに分けて検証を行います。</p>

<ul>
  <li>ステップ1
    <ul>
      <li><strong>UCX_TLS</strong> と <strong>UCX_RNDV_THRESH</strong> を組合せた32個のパターンを検証してこれらの最適値を決定</li>
      <li><strong>coll_hcoll_enable</strong> は <strong>1</strong> に固定（デフォルト）</li>
      <li><strong>coll_ucc_enable</strong> は <strong>0</strong> に固定（デフォルト）</li>
      <li>MPIプロセス分割方法はブロック分割に固定</li>
      <li><strong>NPS</strong> は <strong>NPS1</strong> に固定</li>
    </ul>
  </li>
  <li>ステップ2
    <ul>
      <li><strong>UCX_ZCOPY_THRESH</strong> の7パターンを検証してこの最適値を決定</li>
      <li><strong>UCX_TLS</strong> と <strong>UCX_RNDV_THRESH</strong> はステップ1で決定した最適値を使用</li>
      <li><strong>coll_hcoll_enable</strong> は <strong>1</strong> に固定（デフォルト）</li>
      <li><strong>coll_ucc_enable</strong> は <strong>0</strong> に固定（デフォルト）</li>
      <li>MPIプロセス分割方法はブロック分割に固定</li>
      <li><strong>NPS</strong> は <strong>NPS1</strong> に固定</li>
    </ul>
  </li>
  <li>ステップ3
    <ul>
      <li><strong>coll_hcoll_enable</strong> / <strong>coll_ucc_enable</strong> / MPIプロセス分割方法 / <strong>NPS</strong> を組合せた12パターンを検証してこれらの最適値を決定</li>
      <li><strong>UCX_TLS</strong> と <strong>UCX_RNDV_THRESH</strong> はステップ1で決定した最適値を使用</li>
      <li><strong>UCX_ZCOPY_THRESH</strong> はステップ2で決定した最適値を使用</li>
    </ul>
  </li>
</ul>

<h2 id="3-1-ノード当たり8-mpiプロセス">3-1. ノード当たり8 MPIプロセス</h2>

<h3 id="3-1-0-概要">3-1-0. 概要</h3>

<p>本章は、4ノードにノード当たり8 MPIプロセスでトータル32 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,ud</td>
      <td style="text-align: center">intra:32kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:64kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="3-1-1-alltoall">3-1-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-1-1-alltoall">1-1-1 Alltoall</a></strong> の <strong>HCOLL</strong> の結果から32kbとしています。</p>

<p><img src="ata_04_08_step1_all.png" alt="Alltoall 4 node 8 ppn all step1" /></p>

<p><img src="ata_04_08_step1_dc.png" alt="Alltoall 4 node 8 ppn dc step1" /></p>

<p><img src="ata_04_08_step1_rc.png" alt="Alltoall 4 node 8 ppn rc step1" /></p>

<p><img src="ata_04_08_step1_ud.png" alt="Alltoall 4 node 8 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:32kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_04_08_step1.png" alt="Alltoall 4 node 8 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,ud</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_04_08_step2.png" alt="Alltoall 4 node 8 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_04_08_step3_no.png" alt="Alltoall 4 node 8 ppn no coll" /></p>

<p><img src="ata_04_08_step3_uc.png" alt="Alltoall 4 node 8 ppn UCC" /></p>

<p><img src="ata_04_08_step3_hc.png" alt="Alltoall 4 node 8 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_04_08_step3.png" alt="Alltoall 4 node 8 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は2KBから64KBの間で性能が低下し256B以下で性能が向上</li>
  <li><strong>HCOLL</strong> は512Bから32KBの間で性能が低下しそれ未満で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して128B以下で概ね性能が低下</li>
  <li>チューニング未適用は2KB以上で性能が低下</li>
</ul>

<h3 id="3-1-2-allgather">3-1-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-1-2-allgather">1-1-2 Allgather</a></strong> の <strong>HCOLL</strong> の結果から64kbとしています。</p>

<p><img src="aga_04_08_step1_all.png" alt="Allgather 4 node 8 ppn all step1" /></p>

<p><img src="aga_04_08_step1_dc.png" alt="Allgather 4 node 8 ppn dc step1" /></p>

<p><img src="aga_04_08_step1_rc.png" alt="Allgather 4 node 8 ppn rc step1" /></p>

<p><img src="aga_04_08_step1_ud.png" alt="Allgather 4 node 8 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:64kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_04_08_step1.png" alt="Allgather 4 node 8 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_04_08_step2.png" alt="Allgather 4 node 8 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_04_08_step3_no.png" alt="Allgather 4 node 8 ppn no coll" /></p>

<p><img src="aga_04_08_step3_uc.png" alt="Allgather 4 node 8 ppn UCC" /></p>

<p><img src="aga_04_08_step3_hc.png" alt="Allgather 4 node 8 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_04_08_step3.png" alt="Allgather 4 node 8 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> はほぼ全域で性能が向上</li>
  <li><strong>HCOLL</strong> はほぼ全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して128Bから1KBの間で性能が低下し2KBと8KBの間と64B以下で性能が向上</li>
  <li>チューニング適用による性能の変化無し</li>
</ul>

<h3 id="3-1-3-allreduce">3-1-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-1-3-allreduce">1-1-3 Allreduce</a></strong> の <strong>HCOLL</strong> の結果から128kbとしています。</p>

<p><img src="are_04_08_step1_all.png" alt="Allreduce 4 node 8 ppn all step1" /></p>

<p><img src="are_04_08_step1_dc.png" alt="Allreduce 4 node 8 ppn dc step1" /></p>

<p><img src="are_04_08_step1_rc.png" alt="Allreduce 4 node 8 ppn rc step1" /></p>

<p><img src="are_04_08_step1_ud.png" alt="Allreduce 4 node 8 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_04_08_step1.png" alt="Allreduce 4 node 8 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_04_08_step2.png" alt="Allreduce 4 node 8 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_04_08_step3_no.png" alt="Allreduce 4 node 8 ppn no coll" /></p>

<p><img src="are_04_08_step3_uc.png" alt="Allreduce 4 node 8 ppn UCC" /></p>

<p><img src="are_04_08_step3_hc.png" alt="Allreduce 4 node 8 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_04_08_step3.png" alt="Allreduce 4 node 8 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して128Bから4KBで性能が低下</li>
  <li>チューニング未適用は32KBから16MBの間で大幅に性能が低下</li>
</ul>

<h2 id="3-2-ノード当たり16-mpiプロセス">3-2. ノード当たり16 MPIプロセス</h2>

<h3 id="3-2-0-概要">3-2-0. 概要</h3>

<p>本章は、4ノードにノード当たり16 MPIプロセスでトータル64 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,ud</td>
      <td style="text-align: center">intra:32kb,inter:32kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:32kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="3-2-1-alltoall">3-2-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-2-1-alltoall">1-2-1 Alltoall</a></strong> の <strong>HCOLL</strong> の結果から32kbとしています。</p>

<p><img src="ata_04_16_step1_all.png" alt="Alltoall 4 node 16 ppn all step1" /></p>

<p><img src="ata_04_16_step1_dc.png" alt="Alltoall 4 node 16 ppn dc step1" /></p>

<p><img src="ata_04_16_step1_rc.png" alt="Alltoall 4 node 16 ppn rc step1" /></p>

<p><img src="ata_04_16_step1_ud.png" alt="Alltoall 4 node 16 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:32kb,inter:32kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_04_16_step1.png" alt="Alltoall 4 node 16 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,ud</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_04_16_step2.png" alt="Alltoall 4 node 16 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_04_16_step3_no.png" alt="Alltoall 4 node 16 ppn no coll" /></p>

<p><img src="ata_04_16_step3_uc.png" alt="Alltoall 4 node 16 ppn UCC" /></p>

<p><img src="ata_04_16_step3_hc.png" alt="Alltoall 4 node 16 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_04_16_step3.png" alt="Alltoall 4 node 16 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は1KB以下で性能が向上</li>
  <li><strong>HCOLL</strong> は1KB以下で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して256B以下で概ね性能が低下</li>
  <li>チューニング未適用は64KB以下で概ね性能が低下</li>
</ul>

<h3 id="3-2-2-allgather">3-2-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-2-2-allgather">1-2-2 Allgather</a></strong> の <strong>HCOLL</strong> の結果から32kbとしています。</p>

<p><img src="aga_04_16_step1_all.png" alt="Allgather 4 node 16 ppn all step1" /></p>

<p><img src="aga_04_16_step1_dc.png" alt="Allgather 4 node 16 ppn dc step1" /></p>

<p><img src="aga_04_16_step1_rc.png" alt="Allgather 4 node 16 ppn rc step1" /></p>

<p><img src="aga_04_16_step1_ud.png" alt="Allgather 4 node 16 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:32kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_04_16_step1.png" alt="Allgather 4 node 16 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_04_16_step2.png" alt="Allgather 4 node 16 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_04_16_step3_no.png" alt="Allgather 4 node 16 ppn no coll" /></p>

<p><img src="aga_04_16_step3_uc.png" alt="Allgather 4 node 16 ppn UCC" /></p>

<p><img src="aga_04_16_step3_hc.png" alt="Allgather 4 node 16 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_04_16_step3.png" alt="Allgather 4 node 16 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> はほぼ全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して64Bから512Bの間で性能が低下し16B以下で性能が向上</li>
  <li>チューニング未適用は128KB以下で性能が低下</li>
</ul>

<h3 id="3-2-3-allreduce">3-2-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-2-3-allreduce">1-2-3 Allreduce</a></strong> の <strong>HCOLL</strong> の結果から128kbとしています。</p>

<p><img src="are_04_16_step1_all.png" alt="Allreduce 4 node 16 ppn all step1" /></p>

<p><img src="are_04_16_step1_dc.png" alt="Allreduce 4 node 16 ppn dc step1" /></p>

<p><img src="are_04_16_step1_rc.png" alt="Allreduce 4 node 16 ppn rc step1" /></p>

<p><img src="are_04_16_step1_ud.png" alt="Allreduce 4 node 16 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_04_16_step1.png" alt="Allreduce 4 node 16 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_04_16_step2.png" alt="Allreduce 4 node 16 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_04_16_step3_no.png" alt="Allreduce 4 node 16 ppn no coll" /></p>

<p><img src="are_04_16_step3_uc.png" alt="Allreduce 4 node 16 ppn UCC" /></p>

<p><img src="are_04_16_step3_hc.png" alt="Allreduce 4 node 16 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_04_16_step3.png" alt="Allreduce 4 node 16 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して2MBから16MBの間で性能が向上し128Bから4KBの間で性能が低下</li>
  <li>チューニング未適用は16MB以下で大幅に性能が低下</li>
</ul>

<h2 id="3-3-ノード当たり32-mpiプロセス">3-3. ノード当たり32 MPIプロセス</h2>

<h3 id="3-3-0-概要">3-3-0. 概要</h3>

<p>本章は、4ノードにノード当たり32 MPIプロセスでトータル128 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,ud</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="3-3-1-alltoall">3-3-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-3-1-alltoall">1-3-1 Alltoall</a></strong> の結果から16kbとしています。</p>

<p><img src="ata_04_32_step1_all.png" alt="Alltoall 4 node 32 ppn all step1" /></p>

<p><img src="ata_04_32_step1_dc.png" alt="Alltoall 4 node 32 ppn dc step1" /></p>

<p><img src="ata_04_32_step1_rc.png" alt="Alltoall 4 node 32 ppn rc step1" /></p>

<p><img src="ata_04_32_step1_ud.png" alt="Alltoall 4 node 32 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_04_32_step1.png" alt="Alltoall 4 node 32 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,ud</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_04_32_step2.png" alt="Alltoall 4 node 32 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_04_32_step3_no.png" alt="Alltoall 4 node 32 ppn no coll" /></p>

<p><img src="ata_04_32_step3_uc.png" alt="Alltoall 4 node 32 ppn UCC" /></p>

<p><img src="ata_04_32_step3_hc.png" alt="Alltoall 4 node 32 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_04_32_step3.png" alt="Alltoall 4 node 32 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は512B以下で性能が向上</li>
  <li><strong>HCOLL</strong> は256B以下で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して顕著な傾向無し</li>
  <li>チューニング未適用は8KB以下で性能が大幅に低下</li>
</ul>

<h3 id="3-3-2-allgather">3-3-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-3-2-allgather">1-3-2 Allgather</a></strong> の結果から16kbとしています。</p>

<p><img src="aga_04_32_step1_all.png" alt="Allgather 4 node 32 ppn all step1" /></p>

<p><img src="aga_04_32_step1_dc.png" alt="Allgather 4 node 32 ppn dc step1" /></p>

<p><img src="aga_04_32_step1_rc.png" alt="Allgather 4 node 32 ppn rc step1" /></p>

<p><img src="aga_04_32_step1_ud.png" alt="Allgather 4 node 32 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_04_32_step1.png" alt="Allgather 4 node 32 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_04_32_step2.png" alt="Allgather 4 node 32 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_04_32_step3_no.png" alt="Allgather 4 node 32 ppn no coll" /></p>

<p><img src="aga_04_32_step3_uc.png" alt="Allgather 4 node 32 ppn UCC" /></p>

<p><img src="aga_04_32_step3_hc.png" alt="Allgather 4 node 32 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_04_32_step3.png" alt="Allgather 4 node 32 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> はほぼ全域で性能が向上</li>
  <li><strong>HCOLL</strong> はほぼ全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して32Bから512Bの間で性能が低下</li>
  <li>チューニング未適用は16KB以下で性能が低下</li>
</ul>

<h3 id="3-3-3-allreduce">3-3-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-3-3-allreduce">1-3-3 Allreduce</a></strong> の結果から128kbとしています。</p>

<p><img src="are_04_32_step1_all.png" alt="Allreduce 4 node 32 ppn all step1" /></p>

<p><img src="are_04_32_step1_dc.png" alt="Allreduce 4 node 32 ppn dc step1" /></p>

<p><img src="are_04_32_step1_rc.png" alt="Allreduce 4 node 32 ppn rc step1" /></p>

<p><img src="are_04_32_step1_ud.png" alt="Allreduce 4 node 32 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_04_32_step1.png" alt="Allreduce 4 node 32 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_04_32_step2.png" alt="Allreduce 4 node 32 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_04_32_step3_no.png" alt="Allreduce 4 node 32 ppn no coll" /></p>

<p><img src="are_04_32_step3_uc.png" alt="Allreduce 4 node 32 ppn UCC" /></p>

<p><img src="are_04_32_step3_hc.png" alt="Allreduce 4 node 32 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_04_32_step3.png" alt="Allreduce 4 node 32 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して512KBから16MBまで性能が向上し4KB以下で性能が低下</li>
  <li>チューニング未適用は16MB以下で大幅に性能が低下</li>
</ul>

<h2 id="3-4-ノード当たり36-mpiプロセス">3-4. ノード当たり36 MPIプロセス</h2>

<h3 id="3-4-0-概要">3-4-0. 概要</h3>

<p>本章は、4ノードにノード当たり36 MPIプロセスでトータル144 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,ud</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="3-4-1-alltoall">3-4-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-4-1-alltoall">1-4-1 Alltoall</a></strong> の結果から16kbとしています。</p>

<p><img src="ata_04_36_step1_all.png" alt="Alltoall 4 node 36 ppn all step1" /></p>

<p><img src="ata_04_36_step1_dc.png" alt="Alltoall 4 node 36 ppn dc step1" /></p>

<p><img src="ata_04_36_step1_rc.png" alt="Alltoall 4 node 36 ppn rc step1" /></p>

<p><img src="ata_04_36_step1_ud.png" alt="Alltoall 4 node 36 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_04_36_step1.png" alt="Alltoall 4 node 36 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,ud</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_04_36_step2.png" alt="Alltoall 4 node 36 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_04_36_step3_no.png" alt="Alltoall 4 node 36 ppn no coll" /></p>

<p><img src="ata_04_36_step3_uc.png" alt="Alltoall 4 node 36 ppn UCC" /></p>

<p><img src="ata_04_36_step3_hc.png" alt="Alltoall 4 node 36 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_04_36_step3.png" alt="Alltoall 4 node 36 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は512B以下で概ね性能が向上</li>
  <li><strong>HCOLL</strong> は512B以下で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して256B以下で概ね性能が低下</li>
  <li>チューニング未適用は全域で性能が低下</li>
</ul>

<h3 id="3-4-2-allgather">3-4-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-4-2-allgather">1-4-2 Allgather</a></strong> の結果から16kbとしています。</p>

<p><img src="aga_04_36_step1_all.png" alt="Allgather 4 node 36 ppn all step1" /></p>

<p><img src="aga_04_36_step1_dc.png" alt="Allgather 4 node 36 ppn dc step1" /></p>

<p><img src="aga_04_36_step1_rc.png" alt="Allgather 4 node 36 ppn rc step1" /></p>

<p><img src="aga_04_36_step1_ud.png" alt="Allgather 4 node 36 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_04_36_step1.png" alt="Allgather 4 node 36 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_04_36_step2.png" alt="Allgather 4 node 36 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_04_36_step3_no.png" alt="Allgather 4 node 36 ppn no coll" /></p>

<p><img src="aga_04_36_step3_uc.png" alt="Allgather 4 node 36 ppn UCC" /></p>

<p><img src="aga_04_36_step3_hc.png" alt="Allgather 4 node 36 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_04_36_step3.png" alt="Allgather 4 node 36 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> はほぼ全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して顕著な傾向無し</li>
  <li>チューニング未適用は16KB以下で性能が低下</li>
</ul>

<h3 id="3-4-3-allreduce">3-4-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-4-3-allreduce">1-4-3 Allreduce</a></strong> の結果から128kbとしています。</p>

<p><img src="are_04_36_step1_all.png" alt="Allreduce 4 node 36 ppn all step1" /></p>

<p><img src="are_04_36_step1_dc.png" alt="Allreduce 4 node 36 ppn dc step1" /></p>

<p><img src="are_04_36_step1_rc.png" alt="Allreduce 4 node 36 ppn rc step1" /></p>

<p><img src="are_04_36_step1_ud.png" alt="Allreduce 4 node 36 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_04_36_step1.png" alt="Allreduce 4 node 36 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_04_36_step2.png" alt="Allreduce 4 node 36 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_04_36_step3_no.png" alt="Allreduce 4 node 36 ppn no coll" /></p>

<p><img src="are_04_36_step3_uc.png" alt="Allreduce 4 node 36 ppn UCC" /></p>

<p><img src="are_04_36_step3_hc.png" alt="Allreduce 4 node 36 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_04_36_step3.png" alt="Allreduce 4 node 36 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> はほぼ全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して8KB以上で概ね性能が向上しそれ未満で性能が低下</li>
  <li>チューニング未適用は32MB以下で大幅に性能が低下</li>
</ul>

<hr />
<h1 id="4--8ノード">4.  8ノード</h1>

<h2 id="4-0-概要">4-0. 概要</h2>

<p>本章は、8ノードにノード当たり8・16・32・36の各MPIプロセス、トータルで64・128・256・288の各MPIプロセスを割当てる場合の各MPI集合通信関数の通信性能について、以下の <strong>実行時パラメータ</strong> の最適な組み合わせを検証します。</p>

<ul>
  <li><strong>UCX_TLS</strong> ： <strong>all</strong> ・ <strong>self,sm,rc</strong> ・ <strong>self,sm,ud</strong> ・ <strong>self,sm,dc</strong></li>
  <li><strong>UCX_RNDV_THRESH</strong> ： <strong>auto</strong> ・ <strong>4kb</strong> ・ <strong>8kb</strong> ・ <strong>16kb</strong> ・ <strong>32kb</strong> ・ <strong>64kb</strong> ・ <strong>128kb</strong> （※15）</li>
  <li><strong>UCX_ZCOPY_THRESH</strong> ： <strong>auto</strong> ・ <strong>4kb</strong> ・ <strong>8kb</strong> ・ <strong>16kb</strong> ・ <strong>32kb</strong> ・ <strong>64kb</strong> ・ <strong>128kb</strong></li>
  <li><strong>coll_hcoll_enable</strong> ： <strong>0</strong> ・ <strong>1</strong></li>
  <li><strong>coll_ucc_enable</strong> ： <strong>0</strong> ・ <strong>1</strong></li>
  <li>MPIプロセス分割方法 ： ブロック分割・サイクリック分割</li>
  <li><strong>NPS</strong> ：  <strong>1</strong> ・ <strong>2</strong></li>
</ul>

<p>※15） <strong>UCX_RNDV_THRESH</strong> は、ノード内は <strong>auto</strong> と <strong><a href="#1--1ノード">1. 1ノード</a></strong> で判明した最適値、ノード間はここに記載の7種類を使用し、以下8個の組合せを検証します。</p>

<ul>
  <li><strong>intra:auto,inter:auto</strong></li>
  <li><strong>intra:optimal_value,inter:auto</strong></li>
  <li><strong>intra:optimal_value,inter:4kb</strong></li>
  <li><strong>intra:optimal_value,inter:8kb</strong></li>
  <li><strong>intra:optimal_value,inter:16kb</strong></li>
  <li><strong>intra:optimal_value,inter:32kb</strong></li>
  <li><strong>intra:optimal_value,inter:64kb</strong></li>
  <li><strong>intra:optimal_value,inter:128kb</strong></li>
</ul>

<p>ここで、全ての <strong>実行時パラメータ</strong> の組み合わせを検証することは非現実的なため、組み合わせを減らす目的で以下3ステップに分けて検証を行います。</p>

<ul>
  <li>ステップ1
    <ul>
      <li><strong>UCX_TLS</strong> と <strong>UCX_RNDV_THRESH</strong> を組合せた32個のパターンを検証してこれらの最適値を決定</li>
      <li><strong>coll_hcoll_enable</strong> は <strong>1</strong> に固定（デフォルト）</li>
      <li><strong>coll_ucc_enable</strong> は <strong>0</strong> に固定（デフォルト）</li>
      <li>MPIプロセス分割方法はブロック分割に固定</li>
      <li><strong>NPS</strong> は <strong>NPS1</strong> に固定</li>
    </ul>
  </li>
  <li>ステップ2
    <ul>
      <li><strong>UCX_ZCOPY_THRESH</strong> の7パターンを検証してこの最適値を決定</li>
      <li><strong>UCX_TLS</strong> と <strong>UCX_RNDV_THRESH</strong> はステップ1で決定した最適値を使用</li>
      <li><strong>coll_hcoll_enable</strong> は <strong>1</strong> に固定（デフォルト）</li>
      <li><strong>coll_ucc_enable</strong> は <strong>0</strong> に固定（デフォルト）</li>
      <li>MPIプロセス分割方法はブロック分割に固定</li>
      <li><strong>NPS</strong> は <strong>NPS1</strong> に固定</li>
    </ul>
  </li>
  <li>ステップ3
    <ul>
      <li><strong>coll_hcoll_enable</strong> / <strong>coll_ucc_enable</strong> / MPIプロセス分割方法 / <strong>NPS</strong> を組合せた12パターンを検証してこれらの最適値を決定</li>
      <li><strong>UCX_TLS</strong> と <strong>UCX_RNDV_THRESH</strong> はステップ1で決定した最適値を使用</li>
      <li><strong>UCX_ZCOPY_THRESH</strong> はステップ2で決定した最適値を使用</li>
    </ul>
  </li>
</ul>

<h2 id="4-1-ノード当たり8-mpiプロセス">4-1. ノード当たり8 MPIプロセス</h2>

<h3 id="4-1-0-概要">4-1-0. 概要</h3>

<p>本章は、8ノードにノード当たり8 MPIプロセスでトータル64 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:32kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:64kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="4-1-1-alltoall">4-1-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-1-1-alltoall">1-1-1 Alltoall</a></strong> の結果から32kbとしています。</p>

<p><img src="ata_08_08_step1_all.png" alt="Alltoall 8 node 8 ppn all step1" /></p>

<p><img src="ata_08_08_step1_dc.png" alt="Alltoall 8 node 8 ppn dc step1" /></p>

<p><img src="ata_08_08_step1_rc.png" alt="Alltoall 8 node 8 ppn rc step1" /></p>

<p><img src="ata_08_08_step1_ud.png" alt="Alltoall 8 node 8 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:32kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_08_08_step1.png" alt="Alltoall 8 node 8 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_08_08_step2.png" alt="Alltoall 8 node 8 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_08_08_step3_no.png" alt="Alltoall 8 node 8 ppn no coll" /></p>

<p><img src="ata_08_08_step3_uc.png" alt="Alltoall 8 node 8 ppn UCC" /></p>

<p><img src="ata_08_08_step3_hc.png" alt="Alltoall 8 node 8 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_08_08_step3.png" alt="Alltoall 8 node 8 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は1KB以下で性能が向上</li>
  <li><strong>HCOLL</strong> は1KB以下で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して128B以下で概ね性能が低下</li>
  <li>チューニング未適用は概ね性能が低下</li>
</ul>

<h3 id="4-1-2-allgather">4-1-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-1-2-allgather">1-1-2 Allgather</a></strong> の結果から64kbとしています。</p>

<p><img src="aga_08_08_step1_all.png" alt="Allgather 8 node 8 ppn all step1" /></p>

<p><img src="aga_08_08_step1_dc.png" alt="Allgather 8 node 8 ppn dc step1" /></p>

<p><img src="aga_08_08_step1_rc.png" alt="Allgather 8 node 8 ppn rc step1" /></p>

<p><img src="aga_08_08_step1_ud.png" alt="Allgather 8 node 8 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:64kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_08_08_step1.png" alt="Allgather 8 node 8 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_08_08_step2.png" alt="Allgather 8 node 8 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_08_08_step3_no.png" alt="Allgather 8 node 8 ppn no coll" /></p>

<p><img src="aga_08_08_step3_uc.png" alt="Allgather 8 node 8 ppn UCC" /></p>

<p><img src="aga_08_08_step3_hc.png" alt="Allgather 8 node 8 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_08_08_step3.png" alt="Allgather 8 node 8 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は64KB以上と32B以下で性能が向上し64Bから512Bの間で性能が低下</li>
  <li><strong>HCOLL</strong> は64KB以上で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して64Bから512Bの間で性能が低下し32B以下で性能が向上</li>
  <li>チューニング未適用は64KB以下で性能が低下</li>
</ul>

<h3 id="4-1-3-allreduce">4-1-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。 <strong>coll_hcoll_enable</strong> ・MPIプロセス分割方法・ <strong>NPS</strong> は、ここではそれぞれ <strong>1</strong> ・ブロック分割・ <strong>NPS1</strong> としています。<br />
なお、ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-1-3-allreduce">1-1-3 Allreduce</a></strong> の結果から128kbとしています。</p>

<p><img src="are_08_08_step1_all.png" alt="Allreduce 8 node 8 ppn all step1" /></p>

<p><img src="are_08_08_step1_dc.png" alt="Allreduce 8 node 8 ppn dc step1" /></p>

<p><img src="are_08_08_step1_rc.png" alt="Allreduce 8 node 8 ppn rc step1" /></p>

<p><img src="are_08_08_step1_ud.png" alt="Allreduce 8 node 8 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_08_08_step1.png" alt="Allreduce 8 node 8 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_08_08_step2.png" alt="Allreduce 8 node 8 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_08_08_step3_no.png" alt="Allreduce 8 node 8 ppn no coll" /></p>

<p><img src="are_08_08_step3_uc.png" alt="Allreduce 8 node 8 ppn UCC" /></p>

<p><img src="are_08_08_step3_hc.png" alt="Allreduce 8 node 8 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_08_08_step3.png" alt="Allreduce 8 node 8 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して64MB以上で性能が向上</li>
  <li>チューニング未適用は16MB以下で大幅に性能が低下</li>
</ul>

<h2 id="4-2-ノード当たり16-mpiプロセス">4-2. ノード当たり16 MPIプロセス</h2>

<h3 id="4-2-0-概要">4-2-0. 概要</h3>

<p>本章は、8ノードにノード当たり16 MPIプロセスでトータル128 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:32kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:32kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="4-2-1-alltoall">4-2-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-2-1-alltoall">1-2-1 Alltoall</a></strong> の結果から32kbとしています。</p>

<p><img src="ata_08_16_step1_all.png" alt="Alltoall 8 node 16 ppn all step1" /></p>

<p><img src="ata_08_16_step1_dc.png" alt="Alltoall 8 node 16 ppn dc step1" /></p>

<p><img src="ata_08_16_step1_rc.png" alt="Alltoall 8 node 16 ppn rc step1" /></p>

<p><img src="ata_08_16_step1_ud.png" alt="Alltoall 8 node 16 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:32kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_08_16_step1.png" alt="Alltoall 8 node 16 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_08_16_step2.png" alt="Alltoall 8 node 16 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_08_16_step3_no.png" alt="Alltoall 8 node 16 ppn no coll" /></p>

<p><img src="ata_08_16_step3_uc.png" alt="Alltoall 8 node 16 ppn UCC" /></p>

<p><img src="ata_08_16_step3_hc.png" alt="Alltoall 8 node 16 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_08_16_step3.png" alt="Alltoall 8 node 16 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は512B以下で性能が向上</li>
  <li><strong>HCOLL</strong> は512B以下で概ね性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して2KB以下で概ね性能が低下</li>
  <li>チューニング未適用は4KB以下で概ね性能が低下</li>
</ul>

<h3 id="4-2-2-allgather">4-2-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-2-2-allgather">1-2-2 Allgather</a></strong> の結果から32kbとしています。</p>

<p><img src="aga_08_16_step1_all.png" alt="Allgather 8 node 16 ppn all step1" /></p>

<p><img src="aga_08_16_step1_dc.png" alt="Allgather 8 node 16 ppn dc step1" /></p>

<p><img src="aga_08_16_step1_rc.png" alt="Allgather 8 node 16 ppn rc step1" /></p>

<p><img src="aga_08_16_step1_ud.png" alt="Allgather 8 node 16 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:32kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_08_16_step1.png" alt="Allgather 8 node 16 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_08_16_step2.png" alt="Allgather 8 node 16 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_08_16_step3_no.png" alt="Allgather 8 node 16 ppn no coll" /></p>

<p><img src="aga_08_16_step3_uc.png" alt="Allgather 8 node 16 ppn UCC" /></p>

<p><img src="aga_08_16_step3_hc.png" alt="Allgather 8 node 16 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_08_16_step3.png" alt="Allgather 8 node 16 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> はほぼ全域で性能が向上</li>
  <li><strong>HCOLL</strong> はほぼ全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して32Bから512Bの間で性能が低下し16B以下で性能が向上</li>
  <li>チューニング未適用は128KB以下で性能が低下</li>
</ul>

<h3 id="4-2-3-allreduce">4-2-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-2-3-allreduce">1-2-3 Allreduce</a></strong> の結果から128kbとしています。</p>

<p><img src="are_08_16_step1_all.png" alt="Allreduce 8 node 16 ppn all step1" /></p>

<p><img src="are_08_16_step1_dc.png" alt="Allreduce 8 node 16 ppn dc step1" /></p>

<p><img src="are_08_16_step1_rc.png" alt="Allreduce 8 node 16 ppn rc step1" /></p>

<p><img src="are_08_16_step1_ud.png" alt="Allreduce 8 node 16 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_08_16_step1.png" alt="Allreduce 8 node 16 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_08_16_step2.png" alt="Allreduce 8 node 16 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_08_16_step3_no.png" alt="Allreduce 8 node 16 ppn no coll" /></p>

<p><img src="are_08_16_step3_uc.png" alt="Allreduce 8 node 16 ppn UCC" /></p>

<p><img src="are_08_16_step3_hc.png" alt="Allreduce 8 node 16 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_08_16_step3.png" alt="Allreduce 8 node 16 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して1MB以上で性能が向上し128Bから32KBの間で性能が低下</li>
  <li>チューニング未適用は8MB以下で大幅に性能が低下</li>
</ul>

<h2 id="4-3-ノード当たり32-mpiプロセス">4-3. ノード当たり32 MPIプロセス</h2>

<h3 id="4-3-0-概要">4-3-0. 概要</h3>

<p>本章は、8ノードにノード当たり32 MPIプロセスでトータル256 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,ud</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="4-3-1-alltoall">4-3-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-3-1-alltoall">1-3-1 Alltoall</a></strong> の結果から16kbとしています。</p>

<p><img src="ata_08_32_step1_all.png" alt="Alltoall 8 node 32 ppn all step1" /></p>

<p><img src="ata_08_32_step1_dc.png" alt="Alltoall 8 node 32 ppn dc step1" /></p>

<p><img src="ata_08_32_step1_rc.png" alt="Alltoall 8 node 32 ppn rc step1" /></p>

<p><img src="ata_08_32_step1_ud.png" alt="Alltoall 8 node 32 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_08_32_step1.png" alt="Alltoall 8 node 32 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,ud</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_08_32_step2.png" alt="Alltoall 8 node 32 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_08_32_step3_no.png" alt="Alltoall 8 node 32 ppn no coll" /></p>

<p><img src="ata_08_32_step3_uc.png" alt="Alltoall 8 node 32 ppn UCC" /></p>

<p><img src="ata_08_32_step3_hc.png" alt="Alltoall 8 node 32 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_08_32_step3.png" alt="Alltoall 8 node 32 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は顕著な傾向無し</li>
  <li><strong>HCOLL</strong> は2KBから64KBの間で性能が低下し512B以下で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して256Bから16KBの間で性能が向上し128B以下で概ね性能が低下</li>
  <li>チューニング未適用は概ね性能が低下</li>
</ul>

<h3 id="4-3-2-allgather">4-3-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-3-2-allgather">1-3-2 Allgather</a></strong> の結果から16kbとしています。</p>

<p><img src="aga_08_32_step1_all.png" alt="Allgather 8 node 32 ppn all step1" /></p>

<p><img src="aga_08_32_step1_dc.png" alt="Allgather 8 node 32 ppn dc step1" /></p>

<p><img src="aga_08_32_step1_rc.png" alt="Allgather 8 node 32 ppn rc step1" /></p>

<p><img src="aga_08_32_step1_ud.png" alt="Allgather 8 node 32 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_08_32_step1.png" alt="Allgather 8 node 32 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_08_32_step2.png" alt="Allgather 8 node 32 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_08_32_step3_no.png" alt="Allgather 8 node 32 ppn no coll" /></p>

<p><img src="aga_08_32_step3_uc.png" alt="Allgather 8 node 32 ppn UCC" /></p>

<p><img src="aga_08_32_step3_hc.png" alt="Allgather 8 node 32 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_08_32_step3.png" alt="Allgather 8 node 32 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は4KB以上と8B以下で性能が向上し16Bから1KBの間で性能が低下</li>
  <li><strong>HCOLL</strong> はほぼ全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して8B以下で性能が向上し16Bから512KBの間で性能が低下</li>
  <li>チューニング未適用は16KB以下で性能が低下</li>
</ul>

<h3 id="4-3-3-allreduce">4-3-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-3-3-allreduce">1-3-3 Allreduce</a></strong> の結果から128kbとしています。</p>

<p><img src="are_08_32_step1_all.png" alt="Allreduce 8 node 32 ppn all step1" /></p>

<p><img src="are_08_32_step1_dc.png" alt="Allreduce 8 node 32 ppn dc step1" /></p>

<p><img src="are_08_32_step1_rc.png" alt="Allreduce 8 node 32 ppn rc step1" /></p>

<p><img src="are_08_32_step1_ud.png" alt="Allreduce 8 node 32 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_08_32_step1.png" alt="Allreduce 8 node 32 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_08_32_step2.png" alt="Allreduce 8 node 32 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_08_32_step3_no.png" alt="Allreduce 8 node 32 ppn no coll" /></p>

<p><img src="are_08_32_step3_uc.png" alt="Allreduce 8 node 32 ppn UCC" /></p>

<p><img src="are_08_32_step3_hc.png" alt="Allreduce 8 node 32 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_08_32_step3.png" alt="Allreduce 8 node 32 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して512KB以上で性能が向上し64KB以下で概ね性能が低下</li>
  <li>チューニング未適用は16MB以下で大幅に性能が低下</li>
</ul>

<h2 id="4-4-ノード当たり36-mpiプロセス">4-4. ノード当たり36 MPIプロセス</h2>

<h3 id="4-4-0-概要">4-4-0. 概要</h3>

<p>本章は、8ノードにノード当たり36 MPIプロセスでトータル288 MPIプロセスを割当てる場合の最適な <strong>実行時パラメータ</strong> の組み合わせをMPI集合通信関数毎に検証し、その結果を考察します。</p>

<p>下表は、各MPI集合通信関数の最適な <strong>UCX_TLS</strong> 、 <strong>UCX_RNDV_THRESH</strong> 、及び <strong>UCX_ZCOPY_THRESH</strong> を示しており、この設定値を使用することでデフォルト値に対して性能が向上します。</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">MPI集合通信関数</th>
      <th style="text-align: center">UCX_TLS</th>
      <th style="text-align: center">UCX_RNDV_THRESH</th>
      <th style="text-align: center">UCX_ZCOPY_THRESH</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Alltoall</strong></td>
      <td style="text-align: center">self,sm,ud</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allgather</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:16kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Allreduce</strong></td>
      <td style="text-align: center">self,sm,rc</td>
      <td style="text-align: center">intra:128kb,inter:128kb</td>
      <td style="text-align: center">128kb</td>
    </tr>
  </tbody>
</table>

<p><strong>HCOLL</strong> / <strong>UCC</strong> / MPIプロセス分割方法 / <strong>NPS</strong> に関する傾向は、各MPI集合関数のセクションを参照ください。</p>

<h3 id="4-4-1-alltoall">4-4-1. Alltoall</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-4-1-alltoall">1-4-1 Alltoall</a></strong> の結果から16kbとしています。</p>

<p><img src="ata_08_36_step1_all.png" alt="Alltoall 8 node 36 ppn all step1" /></p>

<p><img src="ata_08_36_step1_dc.png" alt="Alltoall 8 node 36 ppn dc step1" /></p>

<p><img src="ata_08_36_step1_rc.png" alt="Alltoall 8 node 36 ppn rc step1" /></p>

<p><img src="ata_08_36_step1_ud.png" alt="Alltoall 8 node 36 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="ata_08_36_step1.png" alt="Alltoall 8 node 36 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,ud</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Alltoall</strong> の結果です。</p>

<p><img src="ata_08_36_step2.png" alt="Alltoall 8 node 36 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="ata_08_36_step3_no.png" alt="Alltoall 8 node 36 ppn no coll" /></p>

<p><img src="ata_08_36_step3_uc.png" alt="Alltoall 8 node 36 ppn UCC" /></p>

<p><img src="ata_08_36_step3_hc.png" alt="Alltoall 8 node 36 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">サイクリック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="ata_08_36_step3.png" alt="Alltoall 8 node 36 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は顕著な傾向無し</li>
  <li><strong>HCOLL</strong> は1KB以上で概ね性能が低下しそれ未満で性能が低下</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して256B以下で概ね性能が低下</li>
  <li>チューニング未適用は全域で概ね性能が低下</li>
</ul>

<h3 id="4-4-2-allgather">4-4-2. Allgather</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-4-2-allgather">1-4-2 Allgather</a></strong> の結果から16kbとしています。</p>

<p><img src="aga_08_36_step1_all.png" alt="Allgather 8 node 36 ppn all step1" /></p>

<p><img src="aga_08_36_step1_dc.png" alt="Allgather 8 node 36 ppn dc step1" /></p>

<p><img src="aga_08_36_step1_rc.png" alt="Allgather 8 node 36 ppn rc step1" /></p>

<p><img src="aga_08_36_step1_ud.png" alt="Allgather 8 node 36 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:16kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="aga_08_36_step1.png" alt="Allgather 8 node 36 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allgather</strong> の結果です。</p>

<p><img src="aga_08_36_step2.png" alt="Allgather 8 node 36 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="aga_08_36_step3_no.png" alt="Allgather 8 node 36 ppn no coll" /></p>

<p><img src="aga_08_36_step3_uc.png" alt="Allgather 8 node 36 ppn UCC" /></p>

<p><img src="aga_08_36_step3_hc.png" alt="Allgather 8 node 36 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">2</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="aga_08_36_step3.png" alt="Allgather 8 node 36 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は16Bから256Bの間で性能が低下しそれ以外で性能が向上</li>
  <li><strong>HCOLL</strong> はほぼ全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して16Bから256Bの間で性能が低下</li>
  <li>チューニング未適用は8KB以下で大幅に性能が低下</li>
</ul>

<h3 id="4-4-3-allreduce">4-4-3. Allreduce</h3>

<p>[ステップ1]</p>

<p>以下のグラフは、 <strong>UCX_RNDV_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果を、 <strong>UCX_TLS</strong> の設定値毎に示しています。<br />
ノード内の <strong>UCX_RNDV_THRESH</strong> の最適値は、 <strong><a href="#1-4-3-allreduce">1-4-3 Allreduce</a></strong> の結果から128kbとしています。</p>

<p><img src="are_08_36_step1_all.png" alt="Allreduce 8 node 36 ppn all step1" /></p>

<p><img src="are_08_36_step1_dc.png" alt="Allreduce 8 node 36 ppn dc step1" /></p>

<p><img src="are_08_36_step1_rc.png" alt="Allreduce 8 node 36 ppn rc step1" /></p>

<p><img src="are_08_36_step1_ud.png" alt="Allreduce 8 node 36 ppn ud step1" /></p>

<p>以上より、 <strong>UCX_RNDV_THRESH</strong> を <strong>intra:128kb,inter:128kb</strong> とした場合が最も性能が良いと判断してこれを固定し、 <strong>UCX_TLS</strong> の各設定値を比較したものが以下のグラフです。</p>

<p><img src="are_08_36_step1.png" alt="Allreduce 8 node 36 ppn step1" /></p>

<p>以上より、 <strong>UCX_TLS</strong> を <strong>self,sm,rc</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ2]</p>

<p>以下のグラフは、 <strong>UCX_ZCOPY_THRESH</strong> を変化させたときの <strong>Allreduce</strong> の結果です。</p>

<p><img src="are_08_36_step2.png" alt="Allreduce 8 node 36 ppn step2" /></p>

<p>以上より、 <strong>UCX_ZCOPY_THRESH</strong> を <strong>128kb</strong> とした場合が最も性能が良いと判断してこれを固定します。</p>

<p>[ステップ3]</p>

<p>以下のグラフは、<strong>NPS</strong> とMPIプロセス分割方法の各組合せを集合通信コンポーネント毎に比較したものです。</p>

<p><img src="are_08_36_step3_no.png" alt="Allreduce 8 node 36 ppn no coll" /></p>

<p><img src="are_08_36_step3_uc.png" alt="Allreduce 8 node 36 ppn UCC" /></p>

<p><img src="are_08_36_step3_hc.png" alt="Allreduce 8 node 36 ppn HCOLL" /></p>

<p>以上より、 <strong>NPS</strong> とMPIプロセス分割方法を下表の設定とした場合が最も性能が良いと判断してこれを固定、</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: right">No COLL</th>
      <th style="text-align: right">UCC</th>
      <th style="text-align: right">HCOLL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>NPS</strong></td>
      <td style="text-align: right">2</td>
      <td style="text-align: right">1</td>
      <td style="text-align: right">1</td>
    </tr>
    <tr>
      <td style="text-align: center">MPIプロセス分割方法</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
      <td style="text-align: right">ブロック分割</td>
    </tr>
  </tbody>
</table>

<p>集合通信コンポーネントを比較したものが以下のグラフです。<br />
ここでは、チューニングを全く適用しなかった場合と比較するため、全パラメータがデフォルトの組合せ（ <strong>UCX_RNDV_THRESH=auto</strong> ・ <strong>UCX_ZCOPY_THRESH=auto</strong> ・ <strong>coll_hcoll_enable=1</strong> ・ <strong>coll_ucc_enable=0</strong> ）を含めています。</p>

<p><img src="are_08_36_step3.png" alt="Allreduce 8 node 36 ppn" /></p>

<p>以上の結果は、以下のように考察することが出来ます。</p>

<ul>
  <li><strong>UCC</strong> は概ね全域で性能が向上</li>
  <li><strong>HCOLL</strong> は全域で性能が向上</li>
  <li><strong>UCC</strong> は <strong>HCOLL</strong> に対して8KB以下で性能が低下</li>
  <li>チューニング未適用は64MB以下で大幅に性能が低下</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 更新日時:</strong> <time class="dt-published" datetime="2025-07-28T16:55:26+09:00">July 28, 2025</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">共有</h4>

  <a href="https://x.com/intent/tweet?text=OpenMPI%E3%81%AEMPI%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E6%96%B9%E6%B3%95%EF%BC%88BM.Optimized3.36%E7%B7%A8%EF%BC%89%20https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fbenchmark%2Fopenmpi-perftune%2F" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fbenchmark%2Fopenmpi-perftune%2F" class="btn btn--facebook" aria-label="Share on Facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Facebook">
    <i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://oracle-japan.github.io/ocitutorials/hpc/benchmark/openmpi-perftune/" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

  <a href="https://bsky.app/intent/compose?text=OpenMPI%E3%81%AEMPI%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E6%96%B9%E6%B3%95%EF%BC%88BM.Optimized3.36%E7%B7%A8%EF%BC%89%20https%3A%2F%2Foracle-japan.github.io%2Focitutorials%2Fhpc%2Fbenchmark%2Fopenmpi-perftune%2F" class="btn btn--bluesky" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="共有 Bluesky">
    <i class="fab fa-fw fa-bluesky" aria-hidden="true"></i><span> Bluesky</span>
  </a>
</section>


      
  <nav class="pagination">
    
      <a href="/ocitutorials/hpc/benchmark/cpu-binding-e6/" class="pagination--pager" title="パフォーマンスを考慮したプロセス・スレッドのコア割当て指定方法（BM.Standard.E6.256編）">前へ</a>
    
    
      <a href="/ocitutorials/hpc/benchmark/openmpi-perftune-e5/" class="pagination--pager" title="OpenMPIのMPI集合通信チューニング方法（BM.Standard.E5.192編）">次へ</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">関連記事</h2>
  <div class="grid__wrapper">
    
  </div>
</div>

  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="検索キーワードを入力してください..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>フォロー</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/#ocijp" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/oracle-japan" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/ocitutorials/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2025 <a href="https://oracle-japan.github.io">Oracle Cloud Infrastructure チュートリアル</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/ocitutorials/assets/js/main.min.js"></script>




<script src="/ocitutorials/assets/js/lunr/lunr.min.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-store.js"></script>
<script src="/ocitutorials/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6W7FEC5CEH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-6W7FEC5CEH', { 'anonymize_ip': false});
</script>







  
    <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
  
    <script src="/ocitutorials/assets/js/clipboardrouge.js"></script>
  
    <script src="/ocitutorials/assets/js/tabs.js"></script>
  
    <script src="/ocitutorials/assets/js/sidebar.js"></script>
  


  </body>
</html>
