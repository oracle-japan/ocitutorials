---
title: "HPC/GPUクラスタ向けファイル共有ストレージの最適な構築手法"
excerpt: "HPC/GPUクラスタを運用する際必須となるファイル共有ストレージは、その選択肢にマネージドサービスを使用する方法と、高帯域のネットワークポートを持つベア・メタル・インスタンスとストレージサービスで構築する方法があります。本テクニカルTipsは、コストパフォーマンス、可用性、構築・運用のしやすさ、最大性能等を考慮し、自身の要件に沿った最適なファイル共有ストレージ構築手法を選定する方法を解説します。"
order: "322"
layout: single
header:
  teaser: "/hpc/tech-knowhow/howto-configure-sharedstorage/architecture_diagram.png"
  overlay_image: "/hpc/tech-knowhow/howto-configure-sharedstorage/architecture_diagram.png"
  overlay_filter: rgba(34, 66, 55, 0.7)
#link: https://community.oracle.com/tech/welcome/discussion/4474261/
---
<style>
table, th, td {
    font-size: 80%;
}
</style>

***
# 0. 概要

HPC/GPUクラスタと共に利用するファイル共有ストレージは、その構築手法を決定する際に以下の評価項目を考慮する必要があります。

1. 計算/GPUノードの複数ノード同時アクセス時性能
2. ランニングコスト
3. ストレージに格納するデータの可用性
4. ファイル共有ストレージサービスの可用性
5. システム構築・運用のしやすさ
6. 構成可能な最大スループット

本テクニカルTipsは、HPC/GPUクラスタ向けファイル共有ストレージの構築を念頭に、いくつかのファイル共有ストレージ構築手法を紹介し、上記評価基準を元にどの手法を採用すればよいか、その考慮点を解説します。

ファイル共有ストレージを構築する際の最初の考慮点は、ファイルシステムにLustreとNFSのどちらを採用するかです。  
Lustreを採用する場合は、マネージドLustreサービスの **[File Storage with Lustre](https://www.oracle.com/jp/cloud/storage/file-storage-with-lustre/)** を使用します。  
NFSを採用する場合は、マネージドNFSサービスの **[ファイル・ストレージ](https://www.oracle.com/jp/cloud/storage/file-storage/)** を使用する方法と、 **ベア・メタル・インスタンス** とストレージサービスで構築する方法（以降"ベア・メタル・インスタンスNFSサーバ"と呼称）の、どちらを採用するかです。

**File Storage with Lustre** を採用する場合は、性能要件に合わせて用意されている4タイプの **パフォーマンス層**（※1）からどれを採用するかを考慮します。

※1）**パフォーマンス層** は、125 MB/s/TB / 250 MB/s/TB / 500 MB/s/TB / 1,000 MB/s/TBが存在しますが、本テクニカルTipsでは125 MB/s/TBを前提に解説します。 **パフォーマンス層** に関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/lustre/overview.htm#concepts)** を参照してください。

**ファイル・ストレージ** を採用する場合は、性能要件に合わせて **マウント・ターゲット** に標準タイプ（以降"標準FSS"と呼称）とHigh Performanceタイプ（以降"高性能FSS"と呼称）（※2）のどちらを採用するかを考慮します。

※2）High Performanceタイプの **マウント・ターゲット** は、その性能違いで3種類存在しますが、本テクニカルTipsでは読み取りスループット設定が80 Gbpsで性能が最も高い **HPMT-80** を前提に解説します。High Performanceタイプの **マウント・ターゲット** 詳細は、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/File/Tasks/managingmounttargets.htm#performance)** を参照して下さい。

ベア・メタル・インスタンスNFSサーバを採用する場合は、以下2種類のどちらにするかを考慮します。

1. ブロック・ボリュームNFSサーバ  
ストレージに **ブロック・ボリューム** を使用し、これをベア・メタル・インスタンスにアタッチする方法です。
2. DenceIO NFSサーバ  
**ベア・メタル・インスタンス** にNVMe SSDドライブを搭載するDenceIOシェイプを使用する方法です。

またブロック・ボリュームNFSサーバを採用する場合は、性能要件に合わせてヘッドノードの異なる標準ブロック・ボリュームNFSサーバと高性能ブロック・ボリュームNFSサーバのどちらを採用するかを考慮します。

以上より、本テクニカルTipsで解説するファイル共有ストレージは、以下6種類です。

| 構築手法                                        | 使用するサービス                                 | ファイルサーバ                                                                                                                  | ストレージ                | 保存対象<br>データ<br>（※3） | ターゲット<br>スループット<br>レンジ |
| :-----------------------------------------: | :--------------------------------------: | :----------------------------------------------------------------------------------------------------------------------: | :------------------: | :-----------------: | :--------------------: |
| 標準FSS<br>(NFS)                              | **ファイル・ストレージ**                           | **標準<br>マウント・ターゲット**                                                                                                     | **ファイル・システム**        | 長期保存<br>データ         | 120 MiB/s<br>（※4）      |
| 高性能FSS<br>(NFS)                             | **ファイル・ストレージ**                           | **High Performance<br>マウント・ターゲット**                                                                                       | **ファイル・システム**        | 長期保存<br>データ         | 10 GiB/s<br>（※4）       |
| 標準<br>ブロック・ボリューム<br>NFSサーバ<br>(NFS)         | **ベア・メタル・インスタンス**<br>+<br>**ブロック・ボリューム** | **[BM.Optimized3.36](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized)** | **ブロック・ボリューム**       | 中期保存<br>データ         | 5 GiB/s<br>（※5）        |
| 高性能<br>ブロック・ボリューム<br>NFSサーバ<br>(NFS)        | **ベア・メタル・インスタンス**<br>+<br>**ブロック・ボリューム** | **[BM.Standard.E6.256](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-standard)**    | **ブロック・ボリューム**       | 中期保存<br>データ         | 10 GiB/s<br>（※5）        |
| DenceIO<br>NFSサーバ<br>(NFS)                  | DenceIOタイプ<br>**ベア・メタル・インスタンス**          | **[BM.DenseIO.E5.128](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-dense)**        | NVMe SSD<br>ローカルディスク | 短期保存<br>データ         | 10 GiB/s<br>（※5）       |
| **File Storage<br>with Lustre**<br>(Lustre) | **File Storage<br>with Lustre**          | -                                                                                                                        | -                    | 長期保存<br>データ         | 3.9 - 25 GB/s<br>（※6）  |

※3）対象の構築手法が提供するデータ可用性レベルに応じて分類しています。（データ可用性が低いほど短期保存データ用ファイル共有ストレージに分類）  
※4）**マウント・ターゲット** の最大スループット値を元に記載しています。  
※5）**IOR** の測定値を元に記載しています。  
※6）使用する **パフォーマンス層** （MB/s/TB）と **容量** （TB）を掛け合わせて計算した値です。

以降では、これら6種類のファイル共有ストレージ構築手法について、まず初めに3種類の **ベア・メタル・インスタンス** NFSサーバ（標準/高性能ブロック・ボリュームNFSサーバとDenceIO NFSサーバ）の構成を解説し、次に前述の評価項目をもとに6種類の構築手法を比較します。

なお各構築手法を使用したファイル共有ストレージは、 **[OCI HPCチュートリアル集](/ocitutorials/hpc/#1-oci-hpcチュートリアル集)** の **[1-3. ファイル共有ストレージ](/ocitutorials/hpc/#1-3-ファイル共有ストレージ)** カテゴリの以下チュートリアルを参照して構築することが出来ます。

| 構築手法                         | チュートリアル                                                                                         |
| :--------------------------: | :---------------------------------------------------------------------------------------------: |
| 標準FSS                        | **[ファイル・ストレージでファイル共有ストレージを構築する](/ocitutorials/hpc/spinup-nfs-server-fss/)**                     |
| 高性能FSS                       | **[ファイル・ストレージでファイル共有ストレージを構築する](/ocitutorials/hpc/spinup-nfs-server-fss/)**                     |
| 標準ブロック・ボリュームNFSサーバ           | **[ブロック・ボリュームでファイル共有ストレージを構築する（BM.Optimized3.36編）](/ocitutorials/hpc/spinup-nfs-server/)**      |
| 高性能ブロック・ボリュームNFSサーバ          | **[ブロック・ボリュームでファイル共有ストレージを構築する（BM.Standard.E6.256編）](/ocitutorials/hpc/spinup-nfs-server-e6/)** |
| DenceIO NFSサーバ               | **[短期保存データ用高速ファイル共有ストレージを構築する](/ocitutorials/hpc/spinup-nfs-server-nvme/)**                     |
| **File Storage with Lustre** | **[File Storage with Lustreでファイル共有ストレージを構築する](/ocitutorials/hpc/spinup-lustre-server-fswl/)**   |
 
***
# 1. ベア・メタル・インスタンスNFSサーバ構成

## 1-0. 概要

本章は、本テクニカルTipsが前提とする3種類のベア・メタル・インスタンスNFSサーバ（標準/高性能ブロック・ボリュームNFSサーバ・DenceIO NFSサーバ）の構成を解説します。

## 1-1. 標準ブロック・ボリュームNFSサーバ構成

NFSサーバのインスタンスは、 **[BM.Optimized3.36](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized)** を使用します。  
この理由は、このシェイプが50 GbpsのTCP/IP接続用ポートを2個搭載し、それぞれをiSCSI接続の **ブロック・ボリューム** アクセス用途とNFSクライアントへのNFSサービス用途に割当ててこれらを同時に使用することで、NFSサービスに50 Gbpsの帯域をフルに使用することが可能となるためです。

またストレージに使用する **ブロック・ボリューム** は、 **パフォーマンス・レベル** を **バランス** で作成します。  
**パフォーマンス・レベル** が **バランス** の **ブロック・ボリューム** は、ボリューム・サイズの増加とともにその性能が向上し1 TBで上限に達するため、1 TB以上のボリュームを基本単位として複数ボリュームをLinuxの論理ボリューム機能で各ボリュームにストライピングを掛けつつ1ファイルシステムに構成し、コストパフォーマンスを最大化します。  
この際、NFSファイルシステム性能としては15ボリューム程度で性能が頭打ちとなることを考慮し、ボリューム15個をストライプ数15でファイルシステムを作成します。

![システム構成図](architecture_diagram.png)

![論理ボリュームディスク構成図](lv_configuration.png)

総容量が15 TBより大きなストレージが必要な場合は、単一 **ブロック・ボリューム** サイズを1 TBより大きくすることで、性能を維持したままその総容量を増やすことが可能です。  
例えば100 TBの総容量が必要な場合は、ボリューム・サイズを7 TBとすることで、7 TB x 15 = 105 TBの総容量を実現することが出来ます。

## 1-2. 高性能ブロック・ボリュームNFSサーバ構成

NFSサーバのインスタンスは、 **[BM.Standard.E6.256](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-standard)** を使用します。  
この理由は、このシェイプが200 GbpsのTCP/IP接続用ポートを1個搭載し、全二重通信の2系統のラインをそれぞれiSCSI接続の **ブロック・ボリューム** アクセス用途とNFSクライアントへのNFSサービス用途にフル活用することで、10GB/sを超えるスループットを実現することが可能となるためです。

またストレージに使用する **ブロック・ボリューム** は、 **パフォーマンス・レベル** を **バランス** で作成します。  
**パフォーマンス・レベル** が **バランス** の **ブロック・ボリューム** は、ボリューム・サイズの増加とともにその性能が向上し1 TBで上限に達するため、1 TB以上のボリュームを基本単位として複数ボリュームをLinuxの論理ボリューム機能で各ボリュームにストライピングを掛けつつ1ファイルシステムに構成し、コストパフォーマンスを最大化します。  
この際、1インスタンスにアタッチできる最大ボリューム数が32であることから、ボリューム32個をストライプ数32でファイルシステムを作成します。

![システム構成図](architecture_diagram_e6.png)

![論理ボリュームディスク構成図](lv_configuration_e6.png)

総容量が32 TBより大きなストレージが必要な場合は、単一 **ブロック・ボリューム** サイズを1 TBより大きくすることで、性能を維持したままその総容量を増やすことが可能です。  
例えば300 TBの総容量が必要な場合は、ボリューム・サイズを10 TBとすることで、10 TB x 32 = 320 TBの総容量を実現することが出来ます。

## 1-3. DenceIO NFSサーバ構成

NFSサーバのインスタンスは、 **[BM.DenseIO.E5.128](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-dense)** を使用します。  
この理由は、このシェイプが100 GbpsのTCP/IP接続用ポートを1個と12台の6.8 TB NVMe SSDローカルディスクを搭載するため、このローカルディスクをストレージに使用することで、100 Gbpsのネットワーク帯域全てをNFSのサービスに使用することが可能となるためです。  
これにより、 **[1-1. ブロック・ボリュームNFSサーバ構成](#1-1-ブロックボリュームnfsサーバ構成)** で解説するブロック・ボリュームNFSサーバに対して、NFSのサービスに使用できるネットワーク帯域が2倍となり、NVMe SSDローカルディスクの持つ高いI/O性能とあいまって、計算/GPUノードからの多数同時アクセス時に高いスループットとメタデータ性能を提供するファイル共有ストレージを構築することが可能になります。

![システム構成図](architecture_diagram_nvme.png)

また、ストレージに使用する12台のNVMe SSDローカルディスクは、データ保全性とパフォーマンスを両立するため、 **Linux software RAID** によるRAID10構成とします。

![RAID構成図](raid_configuration.png)

***
# 2. 各構築手法の比較

## 2.0 概要

本章は、5種類の構築手法を **[0. 概要](#0-概要)** に記載の6個の評価項目を以下4グループに分類して比較します。

- コストパフォーマンス  
  １．計算/GPUノードの複数ノード同時アクセス時性能  
  ２．ランニングコスト
- 可用性  
  ３．ストレージに格納するデータの可用性  
  ４．ファイル共有ストレージサービスの可用性
- 運用性  
  ５．システム構築・運用のしやすさ
- 最大性能  
  ６. 構成可能な最大スループット

詳細は以降の章で解説しますが、コストパフォーマンスはベア・メタル・インスタンスNFSサーバを使用する構築手法が有利で、可用性と運用性は **ファイル・ストレージ** を使用する構築手法が有利となります。

## 2-1. コストパフォーマンスによる比較

本章は、コストパフォーマンスとして以下2項目の評価指標を採用します。

1. 計算/GPUノードの複数ノード同時アクセス時性能
2. ランニングコスト

ここで **評価指標 1.** は、 **[IO500](https://io500.org/)** でも採用されているファイルシステムベンチマークツールである **IOR** と **mdtest** を、それぞれスループットとメタデータ性能の評価指標として採用します。

以下の表は、6種類の構築手法を価格と性能の観点で比較しています。

| 構築手法  | 構成                                                              | 月額定価<br>（2025年4月時点）               | IOR               | mdtest    |
| :---: | :-------------------------------------------------------------: | --------------------------------: | :---------------: | :-------: |
| 標準FSS | **マウント・ターゲット** (**Standard**) <br>**ファイルシステム** (15 TB)<br>Total | 0 円<br>697,500 円<br>**697,500** 円 | 125 MiB/s<br>（※7） | -<br>（※8） |
 高性能FSS| **マウント・ターゲット** (**HPMT-80**)<br>**ファイルシステム** (80 TB)<br>Total |3,720,000 円<br>0 円<br>**3,720,000 円**|10 GiB/s<br>（※7）|-<br>（※8）|
| 標準<br>ブロック・ボリューム<br>NFSサーバ | **ブロック・ボリューム** (Balanced 15 TB)<br>**BM.Optimized3.36**<br>Total | 98,813 円<br>312,748 円<br>**411,561** 円 | 4,793 MiB/s (Write)<br>5,383 MiB/s (Read)<br>（※9） | 30,478 (create)<br>208,984 (stat)<br>28,926 (delete)<br>（※9） | 
| 高性能<br>ブロック・ボリューム<br>NFSサーバ | **ブロック・ボリューム** (Balanced 80 TB)<br>**BM.Standard.E6.256**<br>Total | 527,000 円<br>1,594,184 円<br>**2,121,184** 円 | 10,607 MiB/s (Write)<br>11,882 MiB/s (Read)<br>（※9） | 29,544 (create)<br>282,502 (stat)<br>29,063 (delete)<br>（※9） | 
| DenceIO<br>NFSサーバ|**BM.DenseIO.E5.128**<br>(6.8 TB NVMe SSD x 12)|**1,372,991** 円|8,967 MiB/s (Write)<br>7,526 MiB/s (Read)<br>（※10）|83,406 (create)<br>255,682 (stat)<br>70,499 (delete)<br>（※10）|
| **File Storage<br>with Lustre**| **パフォーマンス層**（125 MB/s/TB）<br>**容量**（80 TB）|**1,841,400** 円|10 GB/s<br>（※11）|-<br>（※8）|

※7）**IOR** 測定値が無いため、 **マウント・ターゲット** の最大スループット値を記載しています。  
※8）**mdtest** 測定値が無いため、記載していません。  
※9）この測定値は、 **[OCI HPCチュートリアル集](/ocitutorials/hpc/#1-oci-hpcチュートリアル集)** の **[ブロック・ボリュームでファイル共有ストレージを構築する（BM.Optimized3.36編）](/ocitutorials/hpc/spinup-nfs-server/)** / **[ブロック・ボリュームでファイル共有ストレージを構築する（BM.Standard.E6.256編）](/ocitutorials/hpc/spinup-nfs-server-e6/)** に記載の手順に従って3回計測した平均値です。  
※10）この測定値は、 **[OCI HPCチュートリアル集](/ocitutorials/hpc/#1-oci-hpcチュートリアル集)** の **[短期保存データ用高速ファイル共有ストレージを構築する](/ocitutorials/hpc/spinup-nfs-server-nvme/)** に従って構築したDenceIO NFSサーバに於いて、8ノードの **VM.Standard2.24** からノード当たりMPIプロセス数16・総MPIプロセス数128の条件で測定し、3回計測した平均値です。  
※11）**IOR** 測定値が無いため、使用する **パフォーマンス層** （MB/s/TB）と **容量** （TB）を掛け合わせて計算した値です。

この表から、以下のことがわかります。

- **IOR** スループットレンジが同程度の高性能FSS、高性能ブロック・ボリュームNFSサーバ、DenceIO NFSサーバ、及び **File Storage with Lustre** を比較すると、
  - DenceIO NFSサーバのランニングコストが最も安価（※12）
  - （他のサービスの測定値が無い状況の中）DenceIO NFSサーバは圧倒的なメタデータ性能を示す
  - 高性能ブロック・ボリュームNFSサーバのランニングコストは高性能FSSの6割程度（※13）
  - **File Storage with Lustre** のランニングコストは高性能FSSの約半分（※13）
- パフォーマンスがエントリーレベルの標準FSSと標準ブロック・ボリュームNFSサーバを比較すると、
  - 標準ブロック・ボリュームNFSサーバのランニングコストは標準FSSの6割程度（※14）
  - 標準ブロック・ボリュームNFSサーバのスループットは標準FSSの40倍程度

※12）この比較は、高性能FSS、高性能ブロック・ボリュームNFSサーバ、及び **File Storage with Lustre** の容量が80 TBに対してDenceIO NFSサーバが40.8 TBと、異なる総容量で比較しているため条件が異なります。  
※13）総容量80TBの高性能ブロック・ボリュームNFSサーバは、その価格の大部分を **BM.Standard.E6.256** が占めるため、総容量がこれより大きくなると、下表のように高性能FSSや **File Storage with Lustre** に対する価格優位性が拡大します。

| 総容量（TB） | 高性能FSS<br>月額定価<br>（2025年5月時点） | File Storage with Lustre<br>月額定価<br>（2025年5月時点） | 高性能ブロック・ボリュームNFSサーバ<br>月額定価<br>（2025年5月時点） |
| ------: | ----------------------------: | ----------------------------------------------: | ----------------------------------------: |
| 80      | **3,720,000** 円                 | **1,841,400** 円                                   | **2,121,184** 円                             |
| 500     | **23,250,000** 円              | **11,508,750** 円                                   | **4,887,934** 円                           |
| 1,000   | **46,500,000** 円              | **23,017,500** 円                                   | **8,181,684** 円                           |

※14）総容量15TBの標準ブロック・ボリュームNFSサーバは、その価格の大部分を **BM.Optimized3.36** が占めるため、総容量がこれより大きくなると、下表のように標準FSSに対する価格優位性が拡大します。

| 総容量（TB） | 標準FSS<br>月額定価<br>（2025年5月時点） | 標準ブロック・ボリュームNFSサーバ<br>月額定価<br>（2025年5月時点） |
| ------: | ---------------------------: | ----------------------------------------: |
| 15      | **697,500** 円                | **411,560** 円                             |
| 100     | **4,650,000** 円              | **971,498** 円                             |
| 500     | **23,250,000** 円             | **3,606,498** 円                           |
| 1,000   | **46,500,000** 円             | **6,900,248** 円                           |

以上より、性能レンジが高性能・エントリーレベル何れの場合も、コストパフォーマンスを評価指標に各構築手法を比較すると、<u>ベア・メタル・インスタンスNFSサーバを使用する構築手法が<strong>ファイル・ストレージ</strong>を使用する構築手法より圧倒的にコストパフォーマンスが良く</u>、マネージドサービス同士では<u><strong>File Storage with Lustre</strong>を使用する構築手法が<strong>ファイル・ストレージ</strong>を使用する構築手法より圧倒的にコストパフォーマンスが良い</u>ことがわかります。

## 2-2 可用性による比較

本章は、可用性として以下2項目の評価指標を採用します。

3. ストレージに格納するデータの可用性  
4. ファイル共有ストレージサービスの可用性

ここで、 **ファイル・ストレージ** を使用する標準/高性能FSSと **ブロック・ボリューム** を使用する標準/高性能ブロック・ボリュームNFSサーバは、可用性を評価指標とした場合同じ評価となるため、これらをまとめて4種類の構築手法を比較します。

以下の表は、これら4種類の構築手法の可用性観点での利点・欠点を比較しています。

| 構築手法                            | 利点                                                                                                                                                                       | 欠点                                                                                                                                                    |
| :-----------------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| 標準/高性能<br>FSS                   | ・HA化された **マウント・ターゲット** による高い<br>サービス可能性<br>・**ファイル・システム** が **可用性ドメイン** 内に複数<br>レプリカを持つことによる高いデータ可用性<br>・ **ファイル・ストレージ** が用意する豊富なバック<br>アップ機能を使用することによる高いデータ可用性<br>（※15） | -                                                                                                                                                     |
| 標準/高性能<br>ブロック・ボリューム<br>NFSサーバ  | ・**ブロック・ボリューム** が **可用性ドメイン** 内に<br>複数レプリカを持つことによる高いデータ可用性<br>（※16）                                                                                                      | ・NFSサーバがサービス可用性に対する<br>単一障害点<br>・データ可用性を高めるために独自の<br>バックアップ環境構築が必要（※17）（※18）                                                                          |
| DenceIO<br>NFSサーバ               | ・ **Linux software RAID** のRAID10によるデータ<br>可用性                                                                                                                           | ・NFSサーバがサービス可用性に対する<br>単一障害点<br> ・NFSサーバがデータ可用性に対する<br>単一障害点（※19）<br> ・NVMe SSDドライブのRAID10による脆弱なデータ<br>可用性<br>・データ可用性を高めるために独自の<br>バックアップ環境構築が必要（※17） |
| **File Storage<br>with Lustre** | ・HA化されたファイルサーバー群による高い<br>サービス可能性<br>・格納するファイルが **可用性ドメイン** 内に複数<br>レプリカを持つことによる高いデータ可用性                                                                                  | ・データ可用性を高めるために独自の<br>バックアップ環境構築が必要（※20）                                                                                                               |

※15）スナップショット、クローン、及びレプリケーションが用意されています。これらサービスの詳細は、OCI公式ドキュメントの **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/File/home.htm)** を参照ください。  
※16）複数の **ブロック・ボリューム** をNFSサーバにアタッチして使用するため、NFSサーバ障害でインスタンスを入れ替える必要が生じた場合、 **ブロック・ボリューム** に格納するファイルを復旧するためには特別な手順が必要です。  
この詳細は、 **[OCI HPCテクニカルTips集](/ocitutorials/hpc/#3-oci-hpcテクニカルtips集)** の **[ブロック・ボリュームを使用するNFSサーバのインスタンス障害からの復旧方法](/ocitutorials/hpc/tech-knowhow/bv-sharedstorage-recovery/)** を参照してください。  
※17）このバックアップ環境は、 **[OCI HPCチュートリアル集](/ocitutorials/hpc/#1-oci-hpcチュートリアル集)** の **[ベア・メタル・インスタンスNFSサーバ向けバックアップサーバを構築する](/ocitutorials/hpc/spinup-backup-server)** に従って構築することが出来ます。  
※18）**ブロック・ボリューム** に格納するファイルのバックアップは、NFSサーバ上のファイルシステムとしてバックアップする必要があり、 **ブロック・ボリューム** が用意するブロックデバイスレベルのバックアップ機能を利用することが出来ません。  
※19）障害等でNFSサーバインスタンスを再構築する必要が発生すると、NVMe SSDローカルディスクに格納するファイルが消失します。  
※20）**オブジェクト・ストレージ** へのバックアップ機能が今後リリースされる予定です。（2025年4月現在）  

以上より、可用性を評価指標に各構築手法を比較すると、マネージドサービスである<u><strong>ファイル・ストレージ</strong>と<strong>File Storage with Lustre</strong>を使用する構築手法がベア・メタル・インスタンスNFSサーバを使用する構築手法より圧倒的にデータ可用性とサービス可用性が高い</u>ことがわかります。

## 2.3 運用性による比較

本章は、運用性としてシステム構築・運用のしやすさを評価指標に採用します。

ここで、 **ファイル・ストレージ** を使用する標準/高性能FSSと **ブロック・ボリューム** を使用する標準/高性能ブロック・ボリュームNFSサーバは、運用性を評価指標とした場合同じ評価となるため、これらをまとめて4種類の構築手法を比較します。

以下の表は、これら4種類の構築手法の運用性観点での利点・欠点を比較しています。

| 構築手法                            | 利点                                                                                                                                                                                  | 欠点                                                                       |
| :-----------------------------: | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| 標準/高性能<br>FSS                   | ・サービス開始までの構築手順が簡単<br>・ファイルシステムの容量拡張作業が不要<br>・メンテナンスフリーの **マウント・ターゲット**<br>・**マウント・ターゲット** のアップグレードによる<br>動的な性能向上が可能<br>・NFSクライアント用ソフトウェアが多くの<br>イメージに標準装備<br>・幅広いNFSクライアント用イメージの選択肢 | -                                                                        |
| 標準/高性能<br>ブロック・ボリューム<br>NFSサーバ  | ・ボリューム・パフォーマンスとシェイプの選択<br>によるNFSサーバ性能のVertical Scalingが可能<br>・NFSクライアント用ソフトウェアが多くの<br>イメージに標準装備<br>・幅広いNFSクライアント用イメージの選択肢                                                           | ・サービス開始までの構築手順が煩雑（※21）<br>・ファイルシステムの容量拡張作業が煩雑<br>・NFSサーバのソフトウェアメンテナンスが必要 |
| DenceIO<br>NFSサーバ               | ・NFSクライアント用ソフトウェアが多くの<br>イメージに標準装備<br>・幅広いNFSクライアント用イメージの選択肢                                                                                                                        | ・サービス開始までの構築手順が煩雑<br>・ファイルシステムの容量拡張が不可能<br>・NFSサーバのソフトウェアメンテナンスが必要       |
| **File Storage<br>with Lustre** | ・サービス開始までの構築手順が簡単<br>・ファイルシステムの容量拡張作業が簡単<br>・メンテナンスフリーのファイルサーバ群<br>・ **パフォーマンス層**のアップグレードによる<br>動的な性能向上が可能                                                                          | ・Lustreクライアントにソフトウェアを<br>インストールする必要有<br>・Lustreクライアントのイメージに制約有（※22）      |

※21）この構築作業を自動化する **[スタック](/ocitutorials/hpc/#5-3-スタック)** が **GitHub** から無料で提供されています。この詳細は、 **[OCI HPCチュートリアル集](/ocitutorials/hpc/#1-oci-hpcチュートリアル集)** の **[ブロック・ボリュームでファイル共有ストレージを構築する（BM.Optimized3.36編）](/ocitutorials/hpc/spinup-nfs-server/)** / **[ブロック・ボリュームでファイル共有ストレージを構築する（BM.Standard.E6.256編）](/ocitutorials/hpc/spinup-nfs-server-e6/)** を参照してください。  
※22）サポートされるLustreクライアントのイメージに関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/lustre/file-system-connect.htm#clients)** を参照してください。

以上より、運用性を評価指標に各構築手法を比較すると、マネージドサービスである<u><strong>ファイル・ストレージ</strong>と<strong>File Storage with Lustre</strong>を使用する構築手法がベア・メタル・インスタンスNFSサーバを使用する構築手法より圧倒的にシステム構築やシステム運用が容易</u>であることがわかります。

またマネージドサービス同士を比較すると、<u>クライアントに手間がかからずOSの制約がない<strong>ファイル・ストレージ</strong>が<strong>File Storage with Lustre</strong>に対して有利</u>であることがわかります。


## 2.4 最大性能による比較

本章は、最大性能として各構築手法で構成することが可能な範囲に於ける最大スループットを評価指標に採用します。  
この評価指標は、近年大規模化の一途を辿っている機械学習ワークロードを実行するGPUクラスタに於いて、特に計算途中のデータのチェックポイントを取得するケースで、バースト的なスループットがファイル共有ストレージに求められる際に有効です。

以下の表は、各構築手法の構成可能な最大スループットレンジを比較しています。

| 構築手法                         | 最大スループットレンジ      |
| :--------------------------: | ---------------: |
| 標準FSS                        | 125 MB/s         |
| 高性能FSS                       | 10 GB/s          |
| 標準ブロック・ボリュームNFSサーバ           | 5 GB/s           |
| 高性能ブロック・ボリュームNFSサーバ          | 10 GB/s          |
| DenceIO NFSサーバ               | 10 GB/s          |
| **File Storage with Lustre** | 25 GB/s<br>（※23） |

※23）**テナンシ** 当たり・ **可用性ドメイン** 当たりの **サービス制限** で決定されています。この **サービス制限** に関連する **OCI** 公式ドキュメントは、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/General/Concepts/servicelimits.htm#File_Storage_w_Lustre_Limits)** を参照してください。

以上より、最大性能を評価指標に各構築手法を比較すると、<u><strong>File Storage with Lustre</strong>を使用する構築手法が他を圧倒している</u>ことがわかります。