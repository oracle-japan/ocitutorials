---
title: "HPC/GPUクラスタ向けファイル共有ストレージの最適な構築手法"
excerpt: "HPC/GPUクラスタを運用する際必須となるファイル共有ストレージは、NFSでこれを構築することが一般的ですが、この際の選択肢は、NFSのマネージドサービスであるファイル・ストレージを使用する方法と、高帯域のネットワークポートを持つベア・メタル・インスタンスとストレージサービスを組合せてNFSサーバを自身で構築する方法があります。本テクニカルTipsは、コストパフォーマンス、可用性、構築・運用のしやすさ等を考慮し、最適なファイル共有ストレージ構築手法を解説します。"
order: "322"
layout: single
header:
  teaser: "/hpc/benchmark/howto-configure-sharedstorage/architecture_diagram.png"
  overlay_image: "/hpc/benchmark/howto-configure-sharedstorage/architecture_diagram.png"
  overlay_filter: rgba(34, 66, 55, 0.7)
#link: https://community.oracle.com/tech/welcome/discussion/4474261/
---
<style>
table, th, td {
    font-size: 80%;
}
</style>

***
# 0. 概要

HPC/GPUクラスタと共に利用するファイル共有ストレージは、その構築方法を決定する際に以下の評価項目を考慮する必要があります。

1. 計算/GPUノードの複数ノード同時アクセス時性能
2. ランニングコスト
3. ストレージ領域に格納するデータの可用性
4. ファイル共有ストレージサービスの可用性
5. システム構築・運用のしやすさ

本テクニカルTipsは、HPC/GPUクラスタ向けにNFSでサービスするファイル共有ストレージを構築することを念頭に、いくつかのファイル共有ストレージ構築手法を紹介し、上記評価基準を元にどの手法を採用すればよいか、その考慮点を解説します。

NFSでサービスするファイル共有ストレージを構築する際の最初の考慮点は、マネージドのNFSサービスである **ファイル・ストレージ** を使用する方法と、自身で **ベア・メタル・インスタンス** からNFSサーバを構築する（以降"ベア・メタル・インスタンスNFSサーバ"と呼称）方法の、どちらを採用するかです。

ここで **ファイル・ストレージ** を採用する場合は、性能要件に合わせて **マウント・ターゲット** に標準タイプ（以降"標準FSS"と呼称）とHigh Performanceタイプ（以降"高性能FSS"と呼称）（※1）のどちらを使用するかを考慮します。

※1）High Performanceタイプの **マウント・ターゲット** は、その性能違いで3種類存在しますが、本テクニカルTipsでは読み取りスループット設定が80 Gbpsで性能が最も高い **HPMT-80** を前提に解説します。  
High Performanceタイプの **マウント・ターゲット** 詳細は、 **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/File/Tasks/managingmounttargets.htm#performance)** を参照くして下さい。

またベア・メタル・インスタンスNFSサーバを採用する場合は、以下2種類のどちらにするかを考慮します。

1. ストレージ領域に **ブロック・ボリューム** を使用しこれをベア・メタル・インスタンスにアタッチする方法（以降"ブロック・ボリュームNFSサーバ”と呼称）
2. **ベア・メタル・インスタンス** にNVMe SSDドライブを搭載するDenceIOシェイプを使用する方法（以降”DenceIO NFSサーバ”と呼称）

以上より、本テクニカルTipsで解説するファイル共有ストレージは、以下4種類です。

| 呼称               | 使用するサービス                                 | ヘッドノード                         | ストレージ領域          | ターゲットスループット |
| :--------------: | :--------------------------------------: | :----------------------------: | :--------------: | :---------: |
| 標準FSS            | **ファイル・ストレージ**                           | **標準マウント・ターゲット**               | **ファイル・システム**    | 100 MB/s    | 
| 高性能FSS           | **ファイル・ストレージ**                           | **High Performanceマウント・ターゲット** | **ファイル・システム**    | 10 GB/s     | 
| ブロック・ボリュームNFSサーバ | **ベア・メタル・インスタンス**<br>+<br>**ブロック・ボリューム** | **ベア・メタル・インスタンス**              | **ブロック・ボリューム**   | 5 GB/s      |
| DenceIO NFSサーバ   | DenceIOシェイプ<br>**ベア・メタル・インスタンス**     | **ベア・メタル・インスタンス**              | NVMe SSDローカルディスク | 10 GB/s     | 

以降では、これら4種類のファイル共有ストレージ構築手法について、まず初めに2種類の **ベア・メタル・インスタンス** NFSサーバ（ブロック・ボリュームNFSサーバとDenceIO NFSサーバ）の構成を解説し、次に前述の評価項目をもとに4種類のファイル共有ストレージ構築手法を比較します。

なお、各ファイル共有ストレージ構築手法に沿ったファイル共有ストレージは、 **[OCI HPCチュートリアル集](/ocitutorials/hpc/#1-oci-hpcチュートリアル集)** の **[1-3. ファイル共有ストレージ](/ocitutorials/hpc/#1-3-ファイル共有ストレージ)** カテゴリの以下チュートリアルを参照して構築することが出来ます。

| 呼称               | チュートリアル                                                                     |
| :--------------: | :-------------------------------------------------------------------------: |
| 標準FSS            | **[ファイル・ストレージでファイル共有ストレージを構築する](/ocitutorials/hpc/spinup-nfs-server-fss/)** |
| 高性能FSS           | **[ファイル・ストレージでファイル共有ストレージを構築する](/ocitutorials/hpc/spinup-nfs-server-fss/)** |
| ブロック・ボリュームNFSサーバ | **[ブロック・ボリュームでNFSサーバを構築する](/ocitutorials/hpc/spinup-nfs-server/)**          |
| DenceIO NFSサーバ   | **[短期保存データ用高速ファイル共有ストレージを構築する](/ocitutorials/hpc/spinup-nfs-server-nvme/)** |
 
***
# 1. ベア・メタル・インスタンスNFSサーバ構成

## 1-0. 概要

本章は、2種類のベア・メタル・インスタンスNFSサーバ（ブロック・ボリュームNFSサーバとDenceIO NFSサーバ）の構成を解説します。

## 1-1. ブロック・ボリュームNFSサーバ構成

NFSサーバとなるベアメタルインスタンスは、 **[BM.Optimized3.36](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-hpc-optimized)**  を使用します。  
この理由は、このシェイプが50 GbpsのTCP/IP接続用ポートを2個搭載し、それぞれをiSCSI接続の **ブロック・ボリューム** アクセス用途とNFSクライアントへのNFSサービス用途に割当ててこれらをNFSサービス時に同時に使用することで、NFSサービスに50 Gbpsの帯域をフルに使用することが可能となるためです。

またデータ格納領域に使用する **ブロック・ボリューム** は、ボリューム・パフォーマンスにBalancedを使用します。  
HPC/GPUクラスタで使用するファイル共有ストレージは、アプリケーションの入出力データのようなサイズの大きなファイルのみならず、ソースプログラムや設定ファイル等のサイズの小さなファイルを多数扱う必要があり、スループットとIOPSの両指標にバランスの取れた **ブロック・ボリューム** とする必要があるためです。  
このBalancedの **ブロック・ボリューム** は、ボリューム・サイズの増加とともにその性能が向上し1 TBで上限に達するため、1 TBボリュームを基本単位として複数ボリュームをLinuxの論理ボリューム機能で1ファイルシステムに構成し、コストパフォーマンスを最大化します。この際、NFSファイルシステム性能としては15ボリューム程度で性能が頭打ちとなることを考慮して、1 TBのボリュームを15個使用した総容量15 TBの **ブロック・ボリューム** 構成とします。  
より大きな総容量が必要な場合は、単一 **ブロック・ボリューム** サイズを1 TBより大きくすることで、性能を維持したままその総容量を増やすことが可能です。  
例えば100 TBを超える総容量が必要な場合は、ボリューム・サイズを7 TBとすることで、7 TB x 15 = 105 TBの総容量を実現することが出来ます。

![システム構成図](architecture_diagram.png)

## 1-2. DenceIO NFSサーバ構成

NFSサーバとなるベアメタルインスタンスは、 **[BM.DenseIO.E5.128](https://docs.oracle.com/ja-jp/iaas/Content/Compute/References/computeshapes.htm#bm-dense)** を使用します。  
この理由は、このシェイプが100 GbpsのTCP/IP接続用ポートを1個搭載し、12台の6.8 TB NVMe SSDローカルディスクに構築するストレージ領域をNFSでサービスする際、100 Gbpsの帯域全てをNFSクライアントへのサービスに使用することが可能となるためです。  
これにより、 **[1-1. ブロック・ボリュームNFSサーバ構成](#1-1-ブロックボリュームnfsサーバ構成)** で解説するブロック・ボリュームNFSサーバに対して、NFSクライアントへのサービスに使用できるネットワーク帯域が2倍となり、NVMe SSDローカルディスクの持つ高いI/O性能とあいまって、計算/GPUノードからの多数同時アクセス時に高いスループットとメタデータ性能を提供するファイル共有ストレージを構築することが可能になります。

![システム構成図](architecture_diagram_nvme.png)

またデータ格納領域に使用する12台のNVMe SSDドライブは、 **Linux software RAID** を使用し、データ保全性とパフォーマンスを両立するRAID10構成とします。

![RAID構成図](raid_configuration.png)

***
# 2. 各ファイル共有ストレージ構築方法の比較

## 2.0 概要

本章は、4種類のファイル共有ストレージ構築手法を、 **[概要](#0-概要)** に記載の5個の評価項目を以下3グループに分類して比較します。

- コストパフォーマンス  
  １．計算/GPUノードの複数ノード同時アクセス時性能  
  ２．ランニングコスト
- 可用性  
  ３．ストレージ領域に格納するデータの可用性  
  ４．ファイル共有ストレージサービスの可用性
- 運用性  
  ５．システム構築・運用のしやすさ

詳細は以降の章で解説しますが、コストパフォーマンスはベア・メタル・インスタンスNFSサーバを使用する構築手法が有利で、可用性と運用性は **ファイル・ストレージ** を使用する構築手法が有利となります。

## 2-1. コストパフォーマンスによる比較

本章は、コストパフォーマンスとして以下2項目の評価指標を採用します。

1. 計算/GPUノードの複数ノード同時アクセス時性能
2. ランニングコスト

ここで **評価指標 1.** は、 **[IO500](https://io500.org/)** でも採用されているファイルシステムベンチマークツールである **IOR** と **mdtest** を、それぞれスループットとメタデータ性能の評価指標として採用します。

以下の表は、 **ファイル・ストレージ** と前述の構成の **ブロック・ボリューム** NFSサーバを、価格と性能の観点で比較しています。

| 呼称    | 構成                                                       | 月あたりランニングコスト<br>（2024年9月時点定価）       | IOR<br>write      | IOR<br>read     | mdtest<br>create | mdtest<br>stat | mdtest<br>delete |
| :---: | :------------------------------------------------------: | -----------------------: | :---------------: | :-------------: | :--------------: | :------------: | :--------------: |
| 標準FSS | **マウント・ターゲット** (Standard) <br>**ファイルシステム** (15 TB)<br>Total | 0 円<br>697,500 円<br>**697,500** 円| 125 MiB/s<br>（※1） | 125 MiB/s<br>（※1） |       -<br>（※2）           |     -<br>（※2）           |           -<br>（※2）       |
 高性能FSS| **マウント・ターゲット** (HPMT-80)<br>**ファイルシステム** (80 TB)<br>Total |3,360,000 円<br>**マウント・ターゲット** 込み<br>**3,360,000 円**|10 GiB/s<br>（※1）|10 GiB/s<br>（※1）|-<br>（※2）|-<br>（※2）|-<br>（※2）|
| ブロック・ボリューム<br>NFSサーバ | **ブロック・ボリューム** (Balanced 15 TB)<br>**BM.Optimized3.36**<br>Total | 98,813 円<br>302,659 円<br>**401,472** 円 | 3,720 MiB/s<br>（※3） | 4,938 MiB/s<br>（※3） | 3,796<br>（※3） | 23,860<br>（※3） | 3,818<br>（※3） |
 DenceIO<br>NFSサーバ|**BM.DenseIO.E5.128**<br>NVMe SSD 40.8 TB (RAID10)|**1,328,701** 円|8,967 MiB/s<br>（※3）|7,526 MiB/s<br>（※3）|83,406<br>（※3）|255,682<br>（※3）|70,499<br>（※3）|

※1）**IOR** 測定値が無いため、 **マウント・ターゲット** の最大スループット値を記載しています。  
※2）**mdtest** 測定値が無いため、記載していません。  
※3）3回計測した平均値です。

この表から、以下のことがわかります。

- **IOR** スループットレンジが同程度の高性能FSSとDenceIO NFSサーバを比較すると、
  - DenceIO NFSサーバのランニングコストは高性能FSSの約3分の1（※4）
  - （高性能FSSの測定値が無い状況の中）DenceIO NFSサーバは圧倒的なメタデータ性能を示す
- パフォーマンスがエントリーレベルの標準FSSとブロック・ボリュームNFSサーバを比較すると、
  - ブロック・ボリュームNFSサーバのランニングコストは標準FSSの6割程度（※5）（※6）
  - ブロック・ボリュームNFSサーバのスループットは標準FSSの30倍以上

※4）この比較は、高性能FSSの80 TBとDenceIO NFSサーバの40.8 TBの異なる総容量で比較しているため条件が異なりますが、高性能FSSの80 TBは使用するにあたっての最低容量で、DenceIO NFSサーバの40.8 TBは当該ベア・メタル・インスタンスに搭載するNVMe SSDドライブ使用から来る固定容量です。  
※5）総容量15TBのブロック・ボリュームNFSサーバの構成は、価格の大部分を **BM.Optimized3.36** が占めるため、総容量がこれより大きくなると、下表のように標準FSSに対する価格優位性が拡大します。

| 総容量（TB） | 標準FSS<br>月額定価    | **ブロック・ボリューム** NFSサーバ<br>月額定価 |
| ------: | ---------------: | ----------------------------: |
| 15      | **697,500** 円    | **401,472** 円                 |
| 100     | **4,650,000** 円  | **961,409** 円                 |
| 500     | **23,250,000** 円 | **3,596,409** 円               |
| 1,000   | **46,500,000** 円 | **6,587,500** 円               |

※6）標準FSSは、従量制課金のため使用量が15 TBを下回るとそれに応じて金額が安くなりますが、 **ブロック・ボリューム** NFSサーバはその時点の使用量に関わらず定額課金されます。

以上より、性能レンジが高性能・エントリーレベル何れの場合も、コストパフォーマンスを評価指標にファイル共有ストレージを比較すると、ベア・メタル・インスタンスNFSサーバを使用する構築手法が **ファイル・ストレージ** を使用する構築手法より圧倒的に有利であることがわかります。

## 2.2 可用性による比較

本章は、可用性として以下2項目の評価指標を採用します。

3. ストレージ領域に格納するデータの可用性  
4. ファイル共有ストレージサービスの可用性

ここで **ファイル・ストレージ** を使用する標準FSSと高性能FSSは、可用性を評価指標とした場合同じ評価となるため、これらをまとめて3種類の構築手法を比較します。

以下の表は、これら3種類の構築手法の可用性観点での利点・欠点を比較しています。

|                           | 利点                                                                                                                                                 | 欠点                                                                                                                          |
| :-----------------------: | -------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 標準FSS<br>高性能FSS           | ・HA化されたマウント・ターゲットによる高いサービス可能性<br>・ **ファイル・ストレージ** が用意する豊富なバックアップ機能を<br>使用することで高いデータ保全性を実現可能（※7） | -                                                                                             |
| ブロック・ボリューム<br>NFSサーバ | ・**ブロック・ボリューム** が **可用性ドメイン** 内に複数レプリカを<br>持つことによる高いデータ可用性                                       | ・NFSサーバインスタンスがサービス可用性に対する単一障害点<br>・データ保全性を高めるためのバックアップに環境構築が必要（※8） |
| DenceIO<br>NFSサーバ | ・ **Linux software RAID** のRAID10によるデータ可用性                                                                               |  ・NFSサーバインスタンスがサービス可用性に対する単一障害点<br> ・NFSサーバインスタンスがデータ可用性に対する単一障害点<br> ・NVMe SSDドライブのRAID10による脆弱なデータ可用性<br>・データ保全性を高めるためのバックアップに環境構築が必要(\*9) |

※7）スナップショット、クローン、及びレプリケーションが用意されています。これらサービスの詳細は、OCI公式ドキュメントの **[ここ](https://docs.oracle.com/ja-jp/iaas/Content/File/home.htm)** を参照ください。

※8）**ブロック・ボリューム** に格納されるファイルのバックアップは、 **ブロック・ボリューム** が用意するバックアップ機能を利用することが出来ず、NFSサーバ上のファイルシステムとしてアクセスしバックアップする必要があります。

この表から、 **ファイル・ストレージ** は **ブロック・ボリューム** NFSサーバに対して、圧倒的にシステム構築やシステム運用が容易であることがわかります。

以上より、可用性を評価指標にファイル共有ストレージを比較すると、 **ファイル・ストレージ** を使用する構築手法がベア・メタル・インスタンスNFSサーバを使用する構築手法より圧倒的にデータ可用性とサービス可用性が高いことがわかります。

## 2.3 運用性による比較

本章は、運用性としてシステム構築・運用のしやすさを評価指標に採用します。

ここで **ファイル・ストレージ** を使用する標準FSSと高性能FSSは、運用性を評価指標とした場合同じ評価となるため、これらをまとめて3種類の構築手法を比較します。

以下の表は、これら3種類の構築手法の運用性観点での利点・欠点を比較しています。

|                           | 利点                                                                                                                                                 | 欠点                                                                                                                          |
| :-----------------------: | -------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| 標準FSS<br>高性能FSS           | ・サービス開始までの構築手順が簡単<br>・ファイルシステムの容量拡張作業が不要<br>・メンテナンスフリーのマウント・ターゲット<br>・マウント・ターゲットのアップグレードによる動的な<br>NFSサーバの性能向上が可能 |                                                                                             |
| ブロック・ボリューム<br>NFSサーバ | ・ボリューム・パフォーマンスとシェイプの選択<br>　によるNFSサーバ性能のVertical Scaling<br>　が可能                                                                                    | ・サービス開始までの構築手順が煩雑（※9）<br>・ファイルシステムの容量拡張作業が煩雑<br>・ベア・メタル・インスタンスのソフトウェアメンテナンスが必要 |
| DenceIO<br>NFSサーバ | -                                                                                    | ・サービス開始までの構築手順が煩雑<br>・ファイルシステムの容量拡張が不可能<br>・ベア・メタル・インスタンスのソフトウェアメンテナンスが必要 |

※9）この構築作業を自動化する **[スタック](/ocitutorials/hpc/#5-3-スタック)** が **[マーケットプレース](/ocitutorials/hpc/#5-5-マーケットプレイス)** から無料で提供されています。この詳細は、 **[OCI HPCチュートリアル集](/ocitutorials/hpc/#1-oci-hpcチュートリアル集)** の **[ブロック・ボリュームでNFSサーバを構築する](/ocitutorials/hpc/spinup-nfs-server/)** を参照ください。

以上より、運用性を評価指標にファイル共有ストレージを比較すると、 **ファイル・ストレージ** を使用する構築手法がベア・メタル・インスタンスNFSサーバを使用する構築手法より圧倒的にシステム構築やシステム運用が容易であることがわかります。